{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0lyPYBs5Jkq",
        "outputId": "a0783438-d77d-42d1-81e5-4c8d2b6d284b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchaudio\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "import soundfile as sf\n",
        "import glob\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import random\n",
        "from IPython.display import Audio, display\n",
        "from google.colab import drive\n",
        "\n",
        "# Tutti i file spostati sul drive\n",
        "# Monta Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Aggiungi il percorso della cartella DeepLearning_StyleTransfer al sys.path\n",
        "DRIVE_DIR = \"/content/drive/MyDrive/DeepLearning_StyleTransfer\"\n",
        "if DRIVE_DIR not in sys.path:\n",
        "    sys.path.append(DRIVE_DIR)\n",
        "\n",
        "# Se i file content_encoder.py, decoder.py e utilityFunctions.py sono nella sottocartella 'models'\n",
        "models_path = os.path.join(DRIVE_DIR, \"models\")\n",
        "if models_path not in sys.path:\n",
        "    sys.path.append(models_path)\n",
        "\n",
        "\n",
        "from content_encoder import ContentEncoder\n",
        "from decoder import Decoder\n",
        "from style_encoder import StyleEncoder, initialize_weights\n",
        "from utilityFunctions import get_STFT, get_CQT, inverse_STFT, get_overlap_windows, sections2spectrogram, concat_stft_cqt, load_audio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parametri\n",
        "SAMPLE_RATE = 22050\n",
        "CUT_TIME_SECONDS = 10\n",
        "BATCH_SIZE = 16  # Metà piano, metà violino, quindi 8+8\n",
        "WINDOW_SIZE = 287\n",
        "OVERLAP_FRAMES = int(WINDOW_SIZE * 0.3)\n",
        "N_FFT = 1024\n",
        "HOP_LENGTH = 256\n",
        "N_BINS = 84\n",
        "TRANSFORMER_DIM = 256  # Deve corrispondere a d_encoder del modello\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Percorsi\n",
        "DATASET_PATH = os.path.join(DRIVE_DIR, 'test_dataset')\n",
        "PIANO_PATH = os.path.join(DATASET_PATH, 'piano')\n",
        "VIOLIN_PATH = os.path.join(DATASET_PATH, 'violin')\n",
        "MODEL_WEIGHTS_PATH = os.path.join(DRIVE_DIR, 'models')\n",
        "OUTPUT_PATH = os.path.join(DRIVE_DIR, 'class_embeddings.pth')\n"
      ],
      "metadata": {
        "id": "_WbRTvRO-Mzo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parametri (modificato MAX_SECTIONS)\n",
        "MAX_SECTIONS = 4  # Numero fisso di sezioni per campione (S=4)\n",
        "\n",
        "# Dataset personalizzato (aggiornato per usare MAX_SECTIONS=4)\n",
        "class AudioDataset(Dataset):\n",
        "    def __init__(self, piano_dir, violin_dir, sample_rate=22050, cut_time_seconds=10, max_sections=MAX_SECTIONS):\n",
        "        self.piano_files = [os.path.join(piano_dir, f) for f in os.listdir(piano_dir) if f.endswith('.mp3')]\n",
        "        self.violin_files = [os.path.join(violin_dir, f) for f in os.listdir(violin_dir) if f.endswith('.mp3')]\n",
        "        self.sample_rate = sample_rate\n",
        "        self.cut_time_seconds = cut_time_seconds\n",
        "        self.max_sections = max_sections\n",
        "        self.file_list = self.piano_files + self.violin_files\n",
        "        self.labels = [0] * len(self.piano_files) + [1] * len(self.violin_files)  # 0: piano, 1: violin\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_path = self.file_list[idx]\n",
        "        label = self.labels[idx]\n",
        "        waveform, sr = load_audio(file_path, self.sample_rate, self.cut_time_seconds)\n",
        "\n",
        "        # Calcola STFT e CQT\n",
        "        stft = get_STFT(waveform, n_fft=N_FFT, hop_length=HOP_LENGTH)  # (2, T, F1)\n",
        "        cqt = get_CQT(waveform, sample_rate=sr, n_bins=N_BINS, hop_length=HOP_LENGTH)  # (2, T, F2)\n",
        "\n",
        "        # Concatena STFT e CQT\n",
        "        spectrogram = concat_stft_cqt(stft, cqt)  # (2, T, F1+F2)\n",
        "\n",
        "        # Crea sezioni sovrapposte\n",
        "        sections = get_overlap_windows(spectrogram, window_size=WINDOW_SIZE, overlap_frames=OVERLAP_FRAMES)  # (S, 2, T, F)\n",
        "\n",
        "        # Tronca o applica padding per avere esattamente max_sections=4\n",
        "        S = sections.shape[0]\n",
        "        if S > self.max_sections:\n",
        "            sections = sections[:self.max_sections]  # Tronca a 4 sezioni\n",
        "        elif S < self.max_sections:\n",
        "            # Padding con zeri\n",
        "            pad_shape = (self.max_sections - S, 2, WINDOW_SIZE, sections.shape[3])\n",
        "            pad_tensor = torch.zeros(pad_shape, device=sections.device)\n",
        "            sections = torch.cat([sections, pad_tensor], dim=0)  # (4, 2, T, F)\n",
        "\n",
        "        return sections, label\n",
        "\n",
        "# Inizializza il dataset (aggiornato per passare MAX_SECTIONS=4)\n",
        "dataset = AudioDataset(PIANO_PATH, VIOLIN_PATH, SAMPLE_RATE, CUT_TIME_SECONDS, MAX_SECTIONS)\n",
        "# Funzione per creare batch bilanciati\n",
        "def create_balanced_batch(dataset, batch_size):\n",
        "    piano_indices = [i for i, label in enumerate(dataset.labels) if label == 0]\n",
        "    violin_indices = [i for i, label in enumerate(dataset.labels) if label == 1]\n",
        "\n",
        "    # Assicurati che ci siano abbastanza campioni\n",
        "    if len(piano_indices) < batch_size // 2 or len(violin_indices) < batch_size // 2:\n",
        "        raise ValueError(\"Not enough piano or violin samples for a balanced batch\")\n",
        "\n",
        "    # Seleziona casualmente metà piano e metà violino\n",
        "    selected_piano = random.sample(piano_indices, batch_size // 2)\n",
        "    selected_violin = random.sample(violin_indices, batch_size // 2)\n",
        "    selected_indices = selected_piano + selected_violin\n",
        "    random.shuffle(selected_indices)  # Mescola per evitare ordine fisso\n",
        "\n",
        "    return selected_indices\n"
      ],
      "metadata": {
        "id": "PucCoUPr-MjC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inizializzazione del modello\n",
        "model = StyleEncoder(\n",
        "    in_channels=2,\n",
        "    cnn_out_dim=256,\n",
        "    transformer_dim=TRANSFORMER_DIM,\n",
        "    num_heads=4,\n",
        "    num_layers=4,\n",
        "    use_cls=True\n",
        ").to(DEVICE)\n",
        "\n",
        "# Carica i pesi pre-addestrati\n",
        "checkpoint_path = os.path.join(models_path, \"checkpoint_epoch_100.pth\")\n",
        "checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
        "model.load_state_dict(checkpoint['style_encoder'])\n",
        "model.eval()\n",
        "\n",
        "# Inizializza il dataset\n",
        "dataset = AudioDataset(PIANO_PATH, VIOLIN_PATH, SAMPLE_RATE, CUT_TIME_SECONDS)\n",
        "\n",
        "# Crea un batch bilanciato\n",
        "batch_indices = create_balanced_batch(dataset, BATCH_SIZE)\n",
        "batch_data = [dataset[i] for i in batch_indices]\n",
        "sections_batch, labels_batch = zip(*batch_data)\n",
        "\n",
        "# Converti in tensori\n",
        "sections_batch = torch.stack(sections_batch).to(DEVICE)  # (B, S, 2, T, F)\n",
        "labels_batch = torch.tensor(labels_batch, dtype=torch.long).to(DEVICE)  # (B,)\n",
        "\n",
        "# Passa i dati attraverso il modello\n",
        "with torch.no_grad():\n",
        "    style_emb, class_emb = model(sections_batch, labels_batch)  # class_emb: (2, transformer_dim)\n",
        "\n",
        "# Salva il tensore class_emb\n",
        "torch.save(class_emb, OUTPUT_PATH)\n",
        "print(f\"Class embeddings salvati in {OUTPUT_PATH}\")\n",
        "\n",
        "# Verifica opzionale\n",
        "class_emb_np = class_emb.cpu().numpy()\n",
        "print(f\"Shape del tensore salvato: {class_emb_np.shape}\")\n",
        "print(f\"Embedding piano (classe 0): {class_emb_np[0][:5]}...\")  # Prime 5 dimensioni\n",
        "print(f\"Embedding violino (classe 1): {class_emb_np[1][:5]}...\")  # Prime 5 dimensioni"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCtJQfZq-MJ5",
        "outputId": "4ce71f18-51f4-4c14-ddbc-85ec9bdb86a4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class embeddings salvati in /content/drive/MyDrive/DeepLearning_StyleTransfer/class_embeddings.pth\n",
            "Shape del tensore salvato: (2, 256)\n",
            "Embedding piano (classe 0): [-1.3264339   0.23438288 -0.49311617  0.6152496   0.7076507 ]...\n",
            "Embedding violino (classe 1): [-1.3810045   0.33650976 -0.4032237   0.44803926  0.69075215]...\n"
          ]
        }
      ]
    }
  ]
}