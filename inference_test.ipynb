{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0A4XXR1D3Oi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import librosa\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "import os\n",
        "import glob\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import random\n",
        "from IPython.display import Audio, display\n",
        "from google.colab import drive\n",
        "\n",
        "# Tutti i file spostati sul drive\n",
        "# Monta Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Aggiungi il percorso della cartella DeepLearning_StyleTransfer al sys.path\n",
        "DRIVE_DIR = \"/content/drive/MyDrive/DeepLearning_StyleTransfer\"\n",
        "if DRIVE_DIR not in sys.path:\n",
        "    sys.path.append(DRIVE_DIR)\n",
        "\n",
        "# Se i file content_encoder.py, decoder.py e utilityFunctions.py sono nella sottocartella 'models'\n",
        "models_path = os.path.join(DRIVE_DIR, \"models\")\n",
        "if models_path not in sys.path:\n",
        "    sys.path.append(models_path)\n",
        "\n",
        "\n",
        "### utilityFunctions da problemi se non messo direttamente nell'ambiente di colab\n",
        "from content_encoder import ContentEncoder\n",
        "from decoder import Decoder\n",
        "from utilityFunctions import get_STFT, get_CQT, inverse_STFT, get_overlap_windows, sections2spectrogram, concat_stft_cqt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sovrascriviamo inverse_STFT perché dava problemi di compatibilità con GPU\n",
        "def inverse_STFT(stft_tensor, n_fft=1024, hop_length=256):\n",
        "    \"\"\"\n",
        "    Input: torch.Tensor (2, time, freq) where 2 is [real, imaginary]\n",
        "\n",
        "    Output: torch.Tensor (samples,) - reconstructed waveform\n",
        "    \"\"\"\n",
        "    # Determina il dispositivo del tensore di input\n",
        "    device = stft_tensor.device\n",
        "\n",
        "    # Permuta il tensore\n",
        "    stft_tensor = stft_tensor.permute(0, 2, 1)  # (2, freq, time)\n",
        "\n",
        "    real_part = stft_tensor[0, :, :]  # (freq, frames)\n",
        "    imag_part = stft_tensor[1, :, :]  # (freq, frames)\n",
        "    stft_complex = torch.complex(real_part, imag_part)  # (freq, frames)\n",
        "\n",
        "    stft_complex = stft_complex.unsqueeze(0)  # (1, freq, frames)\n",
        "\n",
        "    # Crea la finestra e spostala sullo stesso dispositivo del tensore\n",
        "    window = torch.hann_window(n_fft, device=device)\n",
        "\n",
        "    # Inverse STFT\n",
        "    waveform = torch.istft(\n",
        "        stft_complex,\n",
        "        n_fft=n_fft,\n",
        "        hop_length=hop_length,\n",
        "        window=window,\n",
        "        return_complex=False\n",
        "    )\n",
        "\n",
        "    return waveform.squeeze(0)  # (samples,)"
      ],
      "metadata": {
        "id": "fEnr4NmrJfhs"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Crea class_embeddings.pth fittizio\n",
        "\n",
        "# Parametri\n",
        "d_encoder = 256\n",
        "path_class_embeddings = os.path.join(DRIVE_DIR, \"class_embeddings.pth\")\n",
        "\n",
        "# Crea un tensore fittizio di dimensione [2, d_encoder] con valori casuali\n",
        "class_embeddings = torch.randn(2, d_encoder)\n",
        "\n",
        "# Crea la directory di output se non esiste\n",
        "os.makedirs(os.path.dirname(path_class_embeddings), exist_ok=True)\n",
        "\n",
        "# Salva il tensore come file .pth\n",
        "torch.save(class_embeddings, path_class_embeddings)\n",
        "\n",
        "# Verifica che il file sia stato creato\n",
        "if os.path.exists(path_class_embeddings):\n",
        "    print(f\"File {path_class_embeddings} creato con successo!\")\n",
        "    # Verifica il contenuto del file\n",
        "    loaded_tensor = torch.load(path_class_embeddings)\n",
        "    print(f\"Dimensione del tensore caricato: {loaded_tensor.shape}\")\n",
        "else:\n",
        "    print(\"Errore: il file non è stato creato.\")"
      ],
      "metadata": {
        "id": "kvpH2qr0HFM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Monta Google Drive\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "# Path input/output dir\n",
        "'''\n",
        "TEST DIR:\n",
        "/content/drive/MyDrive/test_dataset\n",
        "          -> /piano\n",
        "          -> /violin\n",
        "\n",
        "OUTPUT DIR:\n",
        "\n",
        "/content/drive/MyDrive/output\n",
        "          -> /from_piano_to_violin\n",
        "          -> /from_violin_to_piano\n",
        "          (li crea dopo)\n",
        "'''\n",
        "TEST_DIR = os.path.join(DRIVE_DIR, \"test_dataset\")\n",
        "PATH_DIR = os.path.join(DRIVE_DIR, \"output\")\n",
        "OUTPUT_DIR = os.path.join(DRIVE_DIR, \"output\")\n",
        "\n",
        "\n",
        "SAMPLES_PER_CLASS = 5  # Numero di campioni casuali per classe\n",
        "\n",
        "### class_embeddings pensato come file .pth con tensore [2, d_enc]\n",
        "path_class_embeddings = os.path.join(DRIVE_DIR, \"class_embeddings.pth\")\n",
        "\n",
        "\n",
        "# Configurazioni\n",
        "SAMPLE_RATE = 22050\n",
        "N_FFT = 1024\n",
        "HOP_LENGTH = 256\n",
        "WIN_LENGTH = 1024\n",
        "N_BINS = 84\n",
        "WINDOW_SIZE = 287\n",
        "OVERLAP_PERCENTAGE = 0.3\n",
        "OVERLAP_FRAMES = int(WINDOW_SIZE * OVERLAP_PERCENTAGE)\n",
        "TRANSFORMER_DIM = 256\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "SECTION_LENGTH = 1.0"
      ],
      "metadata": {
        "id": "6B289GmJExDc"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Funzione di style transfer\n",
        "def style_transfer(waveform, sr, content_encoder, decoder, class_embeddings, target_class_id):\n",
        "    stft = get_STFT(waveform, n_fft=N_FFT, hop_length=HOP_LENGTH).to(DEVICE)\n",
        "    cqt = get_CQT(waveform, sample_rate=SAMPLE_RATE, n_bins=N_BINS, hop_length=HOP_LENGTH).to(DEVICE)\n",
        "    input_spectrogram = concat_stft_cqt(stft, cqt)\n",
        "    sections = get_overlap_windows(input_spectrogram, window_size=WINDOW_SIZE, overlap_frames=OVERLAP_FRAMES)\n",
        "    sections = sections.unsqueeze(0)\n",
        "\n",
        "    content_encoder.eval()\n",
        "    with torch.no_grad():\n",
        "        content_emb = content_encoder(sections)\n",
        "\n",
        "    class_emb = class_embeddings[target_class_id].unsqueeze(0)\n",
        "\n",
        "    decoder.eval()\n",
        "    with torch.no_grad():\n",
        "        output_stft = decoder(content_emb, class_emb, target_length=content_emb.size(1))\n",
        "\n",
        "    output_stft = output_stft.squeeze(0)\n",
        "    original_time = stft.size(1)\n",
        "    full_spectrogram = sections2spectrogram(output_stft, original_size=original_time, overlap=OVERLAP_FRAMES)\n",
        "\n",
        "    output_audio = inverse_STFT(full_spectrogram, n_fft=N_FFT, hop_length=HOP_LENGTH)\n",
        "\n",
        "    return output_audio.cpu(), sr\n",
        "\n",
        "# Funzione per processare il dataset di test\n",
        "def process_test_set(test_dir, output_dir, samples_per_class=5):\n",
        "    \"\"\"\n",
        "    Processa un numero specificato di file casuali da ogni cartella di test.\n",
        "\n",
        "    Args:\n",
        "        test_dir: str - directory del dataset di test con sottocartelle 'piano' e 'violin'\n",
        "        output_dir: str - directory per salvare gli audio generati\n",
        "        samples_per_class: int - numero di campioni casuali per classe da processare\n",
        "    \"\"\"\n",
        "    # Crea directory di output (una per piano->violino + una violino -> piano)\n",
        "    piano_to_violin_dir = os.path.join(output_dir, \"from_piano_to_violin\")\n",
        "    violin_to_piano_dir = os.path.join(output_dir, \"from_violin_to_piano\")\n",
        "    Path(piano_to_violin_dir).mkdir(parents=True, exist_ok=True)\n",
        "    Path(violin_to_piano_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Carica i modelli\n",
        "    content_encoder = ContentEncoder().to(DEVICE)\n",
        "    decoder = Decoder().to(DEVICE)\n",
        "\n",
        "    # Carica i pesi dei modelli\n",
        "    checkpoint_path = os.path.join(models_path, \"checkpoint_epoch_100.pth\")\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
        "    content_encoder.load_state_dict(checkpoint['content_encoder'])\n",
        "    decoder.load_state_dict(checkpoint['decoder'])\n",
        "\n",
        "    class_embeddings = torch.load(path_class_embeddings).to(DEVICE)\n",
        "\n",
        "    # Directory delle classi\n",
        "    piano_dir = os.path.join(test_dir, \"piano\")\n",
        "    violin_dir = os.path.join(test_dir, \"violin\")\n",
        "\n",
        "    # Ottieni i file audio e seleziona campioni casuali\n",
        "    piano_files = glob.glob(os.path.join(piano_dir, \"*.mp3\"))\n",
        "    violin_files = glob.glob(os.path.join(violin_dir, \"*.mp3\"))\n",
        "\n",
        "    if len(piano_files) < samples_per_class or len(violin_files) < samples_per_class:\n",
        "        raise ValueError(f\"Non abbastanza file: piano ({len(piano_files)}), violino ({len(violin_files)})\")\n",
        "\n",
        "    piano_files = random.sample(piano_files, samples_per_class)\n",
        "    violin_files = random.sample(violin_files, samples_per_class)\n",
        "\n",
        "    # Processa i file\n",
        "    print(\"Processamento file piano → violino:\")\n",
        "    for audio_path in piano_files:\n",
        "        output_audio, sr = process_file(audio_path, content_encoder, decoder, class_embeddings,\n",
        "                                      source_class=\"piano\", target_class_id=1, target_class=\"violin\",\n",
        "                                      output_dir=piano_to_violin_dir)\n",
        "\n",
        "    print(\"\\nProcessamento file violino → piano:\")\n",
        "    for audio_path in violin_files:\n",
        "        output_audio, sr = process_file(audio_path, content_encoder, decoder, class_embeddings,\n",
        "                                      source_class=\"violin\", target_class_id=0, target_class=\"piano\",\n",
        "                                      output_dir=violin_to_piano_dir)\n",
        "\n",
        "def process_file(audio_path, content_encoder, decoder, class_embeddings, source_class, target_class_id, target_class, output_dir):\n",
        "    waveform, sr = torchaudio.load(audio_path)\n",
        "    if waveform.shape[0] == 2:\n",
        "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "\n",
        "    print(f\"\\nFile: {os.path.basename(audio_path)} ({source_class} → {target_class})\")\n",
        "    print(\"Audio originale:\")\n",
        "    display(Audio(waveform.numpy(), rate=sr))\n",
        "\n",
        "    output_audio, sr = style_transfer(waveform, sr, content_encoder, decoder, class_embeddings, target_class_id)\n",
        "\n",
        "    print(f\"Audio con stile trasferito ({target_class}):\")\n",
        "    display(Audio(output_audio.numpy(), rate=sr))\n",
        "\n",
        "    output_filename = f\"{source_class}_to_{target_class}_{os.path.basename(audio_path)}\"\n",
        "    output_path = os.path.join(output_dir, output_filename)\n",
        "    sf.write(output_path, output_audio.numpy(), sr)\n",
        "    print(f\"Salvato: {output_path}\")\n",
        "\n",
        "    return output_audio, sr"
      ],
      "metadata": {
        "id": "l7BPHm5EFAHz"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process_test_set(TEST_DIR, OUTPUT_DIR, samples_per_class=SAMPLES_PER_CLASS)"
      ],
      "metadata": {
        "id": "oy81WfprFFIP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}