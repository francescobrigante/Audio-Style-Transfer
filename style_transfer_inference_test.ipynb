{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0A4XXR1D3Oi",
        "outputId": "da0be427-f0a2-4023-b7ce-ba633829c955"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import librosa\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "import os\n",
        "import glob\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import random\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "\n",
        "# Mount Google Drive\n",
        "##drive.mount('/content/drive')\n",
        "\n",
        "# Use the first DRIVE_DIR when using Google Drive\n",
        "##DRIVE_DIR = \"/content/drive/MyDrive/DeepLearning_StyleTransfer\"\n",
        "DRIVE_DIR = \"\"\n",
        "##if DRIVE_DIR not in sys.path:\n",
        "    ##sys.path.append(DRIVE_DIR)\n",
        "\n",
        "##models_path = os.path.join(DRIVE_DIR, \"models\")\n",
        "##if models_path not in sys.path:\n",
        "    ##sys.path.append(models_path)\n",
        "\n",
        "\n",
        "from content_encoder import ContentEncoder\n",
        "from SimpleDecoder_TransformerOnly import Decoder\n",
        "#from new_decoder import Decoder\n",
        "from utilityFunctions import get_STFT, get_CQT, inverse_STFT, get_overlap_windows, sections2spectrogram, concat_stft_cqt\n",
        "from dataloader import get_dataloader, diagnose_window_counts\n",
        "from style_encoder import StyleEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fEnr4NmrJfhs"
      },
      "outputs": [],
      "source": [
        "def inverse_STFT(stft_tensor, n_fft=1024, hop_length=256):\n",
        "    \"\"\"\n",
        "    Input: torch.Tensor (2, time, freq) where 2 is [real, imaginary]\n",
        "\n",
        "    Output: torch.Tensor (samples,) - reconstructed waveform\n",
        "    \"\"\"\n",
        "    # Determina il dispositivo del tensore di input\n",
        "    device = stft_tensor.device\n",
        "\n",
        "    # Permuta il tensore\n",
        "    stft_tensor = stft_tensor.permute(0, 2, 1)  # (2, freq, time)\n",
        "\n",
        "    real_part = stft_tensor[0, :, :]  # (freq, frames)\n",
        "    imag_part = stft_tensor[1, :, :]  # (freq, frames)\n",
        "    stft_complex = torch.complex(real_part, imag_part)  # (freq, frames)\n",
        "\n",
        "    stft_complex = stft_complex.unsqueeze(0)  # (1, freq, frames)\n",
        "\n",
        "    # Crea la finestra e spostala sullo stesso dispositivo del tensore\n",
        "    window = torch.hann_window(n_fft, device=device)\n",
        "\n",
        "    # Inverse STFT\n",
        "    waveform = torch.istft(\n",
        "        stft_complex,\n",
        "        n_fft=n_fft,\n",
        "        hop_length=hop_length,\n",
        "        window=window,\n",
        "        return_complex=False\n",
        "    )\n",
        "\n",
        "    return waveform.squeeze(0)  # (samples,)\n",
        "\n",
        "\n",
        "# function to generate class embeddings for style transfer\n",
        "def generate_class_embeddings_from_dataloader(style_encoder, test_loader, device):\n",
        "    \"\"\"\n",
        "    Generate class embeddings using the first batch from dataloader\n",
        "    \"\"\"\n",
        "    style_encoder.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Get first batch\n",
        "        sections, labels = next(iter(test_loader))\n",
        "        sections = sections.to(device)  # (B, S, 2, T, F)\n",
        "        labels = labels.to(device)      # (B,)\n",
        "\n",
        "        print(f\"ðŸ“Š Generating class embeddings from batch shape: {sections.shape}\")\n",
        "        print(f\"ðŸ“‹ Available labels: {labels}\")\n",
        "\n",
        "        class_embeddings = {}\n",
        "\n",
        "        # Find piano and violin samples\n",
        "        piano_idx = torch.where(labels == 0)[0]\n",
        "        violin_idx = torch.where(labels == 1)[0]\n",
        "\n",
        "        if len(piano_idx) > 0:\n",
        "            piano_sections = sections[piano_idx[0]:piano_idx[0]+1]  # (1, S, 2, T, F)\n",
        "            _, piano_class_emb = style_encoder(piano_sections, torch.tensor([0]).to(device))\n",
        "            class_embeddings[\"piano\"] = piano_class_emb.squeeze(0).cpu()\n",
        "            print(f\"âœ… Piano class embedding generated: {piano_class_emb.shape}\")\n",
        "\n",
        "        if len(violin_idx) > 0:\n",
        "            violin_sections = sections[violin_idx[0]:violin_idx[0]+1]  # (1, S, 2, T, F)\n",
        "            _, violin_class_emb = style_encoder(violin_sections, torch.tensor([1]).to(device))\n",
        "            class_embeddings[\"violin\"] = violin_class_emb.squeeze(0).cpu()\n",
        "            print(f\"âœ… Violin class embedding generated: {violin_class_emb.shape}\")\n",
        "\n",
        "        if len(class_embeddings) != 2:\n",
        "            raise ValueError(f\"Could not generate embeddings for both classes. Found: {list(class_embeddings.keys())}\")\n",
        "\n",
        "    return class_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6B289GmJExDc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“‚ Loading checkpoint: checkpoints\\SIMPLEDECODERcheckpoint_epoch_100.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-5-97bcc3332ad2>:61: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… All models loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# Path input/output dir\n",
        "'''\n",
        "TEST DIR:\n",
        "/content/drive/MyDrive/test_dataset\n",
        "          -> /piano\n",
        "          -> /violin\n",
        "\n",
        "OUTPUT DIR:\n",
        "\n",
        "/content/drive/MyDrive/output\n",
        "          -> /from_piano_to_violin\n",
        "          -> /from_violin_to_piano\n",
        "          (li crea dopo)\n",
        "'''\n",
        "\n",
        "# TEST_DIR = os.path.join(DRIVE_DIR, \"test_dataset\")\n",
        "# OUTPUT_DIR = os.path.join(DRIVE_DIR, \"output\")\n",
        "TEST_DIR = \"dataset/test\"\n",
        "OUTPUT_DIR = \"style_transfer_output\"\n",
        "\n",
        "# checkpoint_path = os.path.join(DRIVE_DIR, \"/checkpoints/epoch100_simpleDecoder.pth\")\n",
        "checkpoint_path = \"checkpoints\\SIMPLEDECODERcheckpoint_epoch_100.pth\"\n",
        "\n",
        "# Configurations\n",
        "SAMPLE_RATE = 22050\n",
        "N_FFT = 1024\n",
        "HOP_LENGTH = 256\n",
        "WIN_LENGTH = 1024\n",
        "N_BINS = 84\n",
        "WINDOW_SIZE = 287\n",
        "OVERLAP_PERCENTAGE = 0.3\n",
        "OVERLAP_FRAMES = int(WINDOW_SIZE * OVERLAP_PERCENTAGE)\n",
        "TRANSFORMER_DIM = 256\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "SECTION_LENGTH = 1.0\n",
        "\n",
        "content_encoder = ContentEncoder(\n",
        "  cnn_out_dim=TRANSFORMER_DIM,\n",
        "  transformer_dim=TRANSFORMER_DIM,\n",
        "  num_heads=4,\n",
        "  num_layers=4,\n",
        "  # channels_list=[16, 32, 64, 128, 256]\n",
        "  channels_list=[32, 64, 128, 256, 512, 512]  # Updated channels list\n",
        "  ).to(DEVICE)\n",
        "\n",
        "decoder = Decoder(\n",
        "  d_model=TRANSFORMER_DIM,\n",
        "  nhead=4,\n",
        "  num_layers=4\n",
        "  ).to(DEVICE)\n",
        "\n",
        "style_encoder = StyleEncoder(\n",
        "  cnn_out_dim=TRANSFORMER_DIM,\n",
        "  transformer_dim=TRANSFORMER_DIM,\n",
        "  num_heads=4,\n",
        "  num_layers=4\n",
        "  ).to(DEVICE)\n",
        "\n",
        "if os.path.exists(checkpoint_path):\n",
        "    print(f\"ðŸ“‚ Loading checkpoint: {checkpoint_path}\")\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
        "\n",
        "    try:\n",
        "      content_encoder.load_state_dict(checkpoint['content_encoder'])\n",
        "      style_encoder.load_state_dict(checkpoint['style_encoder'])\n",
        "      decoder.load_state_dict(checkpoint['decoder'])\n",
        "      print(\"âœ… All models loaded successfully!\")\n",
        "    except Exception as e:\n",
        "      print(f\"âš ï¸ Error loading checkpoint: {e}\")\n",
        "      print(\"ðŸ”§ Using randomly initialized models...\")\n",
        "else:\n",
        "  print(f\"âš ï¸ Checkpoint not found: {checkpoint_path}\")\n",
        "  print(\"ðŸ”§ Using randomly initialized models...\")\n",
        "\n",
        "# Upload dataloader\n",
        "piano_dir = os.path.join(TEST_DIR, \"piano\")\n",
        "violin_dir = os.path.join(TEST_DIR, \"violin\")\n",
        "dataloader = get_dataloader(piano_dir, violin_dir, batch_size=16, shuffle=False)\n",
        "\n",
        "\n",
        "# Make output dir\n",
        "p2v_dir = os.path.join(OUTPUT_DIR, \"from_piano_to_violin\")\n",
        "v2p_dir = os.path.join(OUTPUT_DIR, \"from_violin_to_piano\")\n",
        "os.makedirs(p2v_dir, exist_ok=True)\n",
        "os.makedirs(v2p_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8o0X45JrrNv",
        "outputId": "3bb9672b-4813-4f72-ad08-737604eb5380"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“Š Generating class embeddings from batch shape: torch.Size([16, 4, 2, 287, 597])\n",
            "ðŸ“‹ Available labels: tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "âœ… Piano class embedding generated: torch.Size([1, 256])\n",
            "âœ… Violin class embedding generated: torch.Size([1, 256])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Francesco\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:720: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
            "  return torch._transformer_encoder_layer_fwd(\n"
          ]
        }
      ],
      "source": [
        "# Genera le class embeddings dal test_loader\n",
        "class_embeddings = generate_class_embeddings_from_dataloader(style_encoder, dataloader, DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7BPHm5EFAHz",
        "outputId": "611cfc01-03d3-453b-e9d8-a88612166910"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Salvato: style_transfer_output\\from_piano_to_violin\\piano_to_violin_sample0_0.wav\n",
            "Salvato: style_transfer_output\\from_piano_to_violin\\piano_to_violin_sample0_1.wav\n",
            "Salvato: style_transfer_output\\from_piano_to_violin\\piano_to_violin_sample0_2.wav\n",
            "Salvato: style_transfer_output\\from_piano_to_violin\\piano_to_violin_sample0_3.wav\n",
            "Salvato: style_transfer_output\\from_piano_to_violin\\piano_to_violin_sample0_4.wav\n",
            "Salvato: style_transfer_output\\from_piano_to_violin\\piano_to_violin_sample0_5.wav\n",
            "Salvato: style_transfer_output\\from_piano_to_violin\\piano_to_violin_sample0_6.wav\n",
            "Salvato: style_transfer_output\\from_piano_to_violin\\piano_to_violin_sample0_7.wav\n",
            "Salvato: style_transfer_output\\from_violin_to_piano\\violin_to_piano_sample0_8.wav\n",
            "Salvato: style_transfer_output\\from_violin_to_piano\\violin_to_piano_sample0_9.wav\n",
            "Salvato: style_transfer_output\\from_violin_to_piano\\violin_to_piano_sample0_10.wav\n",
            "Salvato: style_transfer_output\\from_violin_to_piano\\violin_to_piano_sample0_11.wav\n",
            "Salvato: style_transfer_output\\from_violin_to_piano\\violin_to_piano_sample0_12.wav\n",
            "Salvato: style_transfer_output\\from_violin_to_piano\\violin_to_piano_sample0_13.wav\n",
            "Salvato: style_transfer_output\\from_violin_to_piano\\violin_to_piano_sample0_14.wav\n",
            "Salvato: style_transfer_output\\from_violin_to_piano\\violin_to_piano_sample0_15.wav\n",
            "Salvato: style_transfer_output\\from_piano_to_violin\\piano_to_violin_sample1_0.wav\n",
            "Salvato: style_transfer_output\\from_piano_to_violin\\piano_to_violin_sample1_1.wav\n",
            "Salvato: style_transfer_output\\from_piano_to_violin\\piano_to_violin_sample1_2.wav\n",
            "Salvato: style_transfer_output\\from_piano_to_violin\\piano_to_violin_sample1_3.wav\n",
            "Salvato: style_transfer_output\\from_piano_to_violin\\piano_to_violin_sample1_4.wav\n",
            "Salvato: style_transfer_output\\from_piano_to_violin\\piano_to_violin_sample1_5.wav\n",
            "Salvato: style_transfer_output\\from_piano_to_violin\\piano_to_violin_sample1_6.wav\n",
            "Salvato: style_transfer_output\\from_piano_to_violin\\piano_to_violin_sample1_7.wav\n",
            "Salvato: style_transfer_output\\from_violin_to_piano\\violin_to_piano_sample1_8.wav\n",
            "Salvato: style_transfer_output\\from_violin_to_piano\\violin_to_piano_sample1_9.wav\n",
            "Salvato: style_transfer_output\\from_violin_to_piano\\violin_to_piano_sample1_10.wav\n",
            "Salvato: style_transfer_output\\from_violin_to_piano\\violin_to_piano_sample1_11.wav\n",
            "Salvato: style_transfer_output\\from_violin_to_piano\\violin_to_piano_sample1_12.wav\n",
            "Salvato: style_transfer_output\\from_violin_to_piano\\violin_to_piano_sample1_13.wav\n",
            "Salvato: style_transfer_output\\from_violin_to_piano\\violin_to_piano_sample1_14.wav\n",
            "Salvato: style_transfer_output\\from_violin_to_piano\\violin_to_piano_sample1_15.wav\n",
            "Salvato: style_transfer_output\\from_piano_to_violin\\piano_to_violin_sample2_0.wav\n",
            "Salvato: style_transfer_output\\from_piano_to_violin\\piano_to_violin_sample2_1.wav\n",
            "Salvato: style_transfer_output\\from_piano_to_violin\\piano_to_violin_sample2_2.wav\n",
            "Salvato: style_transfer_output\\from_piano_to_violin\\piano_to_violin_sample2_3.wav\n",
            "Salvato: style_transfer_output\\from_piano_to_violin\\piano_to_violin_sample2_4.wav\n",
            "Salvato: style_transfer_output\\from_piano_to_violin\\piano_to_violin_sample2_5.wav\n",
            "Salvato: style_transfer_output\\from_piano_to_violin\\piano_to_violin_sample2_6.wav\n",
            "Salvato: style_transfer_output\\from_piano_to_violin\\piano_to_violin_sample2_7.wav\n",
            "Salvato: style_transfer_output\\from_violin_to_piano\\violin_to_piano_sample2_8.wav\n",
            "Salvato: style_transfer_output\\from_violin_to_piano\\violin_to_piano_sample2_9.wav\n",
            "Salvato: style_transfer_output\\from_violin_to_piano\\violin_to_piano_sample2_10.wav\n",
            "Salvato: style_transfer_output\\from_violin_to_piano\\violin_to_piano_sample2_11.wav\n",
            "Salvato: style_transfer_output\\from_violin_to_piano\\violin_to_piano_sample2_12.wav\n",
            "Salvato: style_transfer_output\\from_violin_to_piano\\violin_to_piano_sample2_13.wav\n",
            "Salvato: style_transfer_output\\from_violin_to_piano\\violin_to_piano_sample2_14.wav\n",
            "Salvato: style_transfer_output\\from_violin_to_piano\\violin_to_piano_sample2_15.wav\n",
            "Salvato: style_transfer_output\\from_piano_to_violin\\piano_to_violin_sample3_0.wav\n",
            "Salvato: style_transfer_output\\from_piano_to_violin\\piano_to_violin_sample3_1.wav\n",
            "Salvato: style_transfer_output\\from_piano_to_violin\\piano_to_violin_sample3_2.wav\n",
            "Salvato: style_transfer_output\\from_piano_to_violin\\piano_to_violin_sample3_3.wav\n",
            "Salvato: style_transfer_output\\from_piano_to_violin\\piano_to_violin_sample3_4.wav\n",
            "Salvato: style_transfer_output\\from_piano_to_violin\\piano_to_violin_sample3_5.wav\n",
            "Salvato: style_transfer_output\\from_piano_to_violin\\piano_to_violin_sample3_6.wav\n",
            "Salvato: style_transfer_output\\from_piano_to_violin\\piano_to_violin_sample3_7.wav\n",
            "Salvato: style_transfer_output\\from_violin_to_piano\\violin_to_piano_sample3_8.wav\n",
            "Salvato: style_transfer_output\\from_violin_to_piano\\violin_to_piano_sample3_9.wav\n",
            "Salvato: style_transfer_output\\from_violin_to_piano\\violin_to_piano_sample3_10.wav\n",
            "Salvato: style_transfer_output\\from_violin_to_piano\\violin_to_piano_sample3_11.wav\n",
            "Salvato: style_transfer_output\\from_violin_to_piano\\violin_to_piano_sample3_12.wav\n",
            "Salvato: style_transfer_output\\from_violin_to_piano\\violin_to_piano_sample3_13.wav\n",
            "Salvato: style_transfer_output\\from_violin_to_piano\\violin_to_piano_sample3_14.wav\n",
            "Salvato: style_transfer_output\\from_violin_to_piano\\violin_to_piano_sample3_15.wav\n"
          ]
        }
      ],
      "source": [
        "# Inference\n",
        "content_encoder.eval()\n",
        "decoder.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (inputs, labels) in enumerate(dataloader):\n",
        "        # inputs: (B, S, 2, T, F), labels: (B,)\n",
        "        B = inputs.size(0)\n",
        "        inputs = inputs.to(DEVICE)  # move to GPU\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "        # Estrai embeddings di contenuto\n",
        "        content_embeddings = content_encoder(inputs)\n",
        "\n",
        "        # Costruisci class embeddings inversi per style transfer\n",
        "        inverse_labels = 1 - labels  # 0->1, 1->0\n",
        "        class_emb_list = []\n",
        "\n",
        "        for l in inverse_labels:\n",
        "            class_name = \"piano\" if l.item() == 0 else \"violin\"\n",
        "            class_emb_list.append(class_embeddings[class_name])\n",
        "\n",
        "        # Stack into tensor and move to DEVICE\n",
        "        class_emb = torch.stack(class_emb_list).to(DEVICE)  # shape (B, 256)\n",
        "\n",
        "        # Ricostruisci con stile opposto\n",
        "        output_stfts = decoder(content_embeddings, class_emb, target_length=content_embeddings.size(1))\n",
        "\n",
        "        # Loop sugli elementi del batch\n",
        "        for i in range(B):\n",
        "            label = labels[i].item()  # 0 = piano, 1 = violin\n",
        "            source_class = \"piano\" if label == 0 else \"violin\"\n",
        "            target_class = \"violin\" if label == 0 else \"piano\"\n",
        "            save_dir = p2v_dir if label == 0 else v2p_dir\n",
        "\n",
        "            stft_output = output_stfts[i]  # (S, 2, T, F)\n",
        "            S = inputs[i].size(0)        # Numero di sezioni\n",
        "            T = inputs[i].size(3)        # Lunghezza temporale di ciascuna sezione\n",
        "            original_time = (S - 1) * (T - OVERLAP_FRAMES) + T\n",
        "            full_stft = sections2spectrogram(stft_output, original_size=original_time, overlap=OVERLAP_FRAMES)\n",
        "\n",
        "            waveform = inverse_STFT(full_stft)\n",
        "            filename = f\"{source_class}_to_{target_class}_sample{batch_idx}_{i}.wav\"\n",
        "            output_path = os.path.join(save_dir, filename)\n",
        "            sf.write(output_path, waveform.cpu().numpy(), 22050)\n",
        "            print(f\"Salvato: {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
