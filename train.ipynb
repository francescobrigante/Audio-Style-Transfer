{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205dcdf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3080\n",
      "GPU Memory: 10.0 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# models\n",
    "from style_encoder import StyleEncoder, initialize_weights\n",
    "from content_encoder import ContentEncoder\n",
    "from discriminator import Discriminator\n",
    "from new_decoder import Decoder, compute_comprehensive_loss  # Nuovo decoder dinamico\n",
    "from losses import infoNCE_loss, margin_loss, adversarial_loss, disentanglement_loss\n",
    "from Dataloader import get_dataloader\n",
    "\n",
    "# device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "# hyperparameters\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 8\n",
    "LR_GEN = 5e-5  # Learning rate per Encoders + Decoder\n",
    "LR_DISC = 1e-5  # Learning rate per Discriminator\n",
    "TRANSFORMER_DIM = 256\n",
    "NUM_FRAMES = 4\n",
    "STFT_T, STFT_F = 287, 513\n",
    "CQT_T, CQT_F = 287, 84\n",
    "\n",
    "# loss weights\n",
    "LAMBDA_RECON = 1.0\n",
    "LAMBDA_INFO_NCE = 0.5\n",
    "LAMBDA_MARGIN = 0.5\n",
    "LAMBDA_DISENTANGLE = 0.5\n",
    "LAMBDA_ADV_GEN = 0.01 # Peso per la loss avversaria del generatore\n",
    "\n",
    "# Pesi per le loss comprehensive del decoder\n",
    "LAMBDA_TEMPORAL = 0.3\n",
    "LAMBDA_PHASE = 0.2\n",
    "LAMBDA_SPECTRAL = 0.1\n",
    "LAMBDA_CONSISTENCY = 0.1\n",
    "\n",
    "MODEL_SAVE_PATH = \"./saved_models\"\n",
    "os.makedirs(MODEL_SAVE_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b43ed26",
   "metadata": {},
   "source": [
    "aggiungere data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3042828e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights_conservative(m):\n",
    "    \"\"\"\n",
    "    Inizializzazione conservativa dei pesi per prevenire NaN\n",
    "    \"\"\"\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        # Xavier uniforme con gain ridotto\n",
    "        nn.init.xavier_uniform_(m.weight, gain=0.2)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        # Xavier uniforme con gain ridotto\n",
    "        nn.init.xavier_uniform_(m.weight, gain=0.2)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        nn.init.ones_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.LayerNorm):\n",
    "        nn.init.ones_(m.weight)\n",
    "        nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09e7c204",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    print(f\"Training on device: {device}\")\n",
    "\n",
    "    # load models\n",
    "    style_encoder = StyleEncoder(transformer_dim=TRANSFORMER_DIM).to(device)\n",
    "    content_encoder = ContentEncoder(transformer_dim=TRANSFORMER_DIM).to(device)\n",
    "\n",
    "    # decoder\n",
    "    decoder = Decoder(\n",
    "        d_model=TRANSFORMER_DIM,\n",
    "        nhead=4,\n",
    "        num_layers=4,  # Ridotto per Colab\n",
    "        dim_feedforward=TRANSFORMER_DIM * 2,\n",
    "        dropout=0.1\n",
    "    ).to(device)\n",
    "    \n",
    "    discriminator = Discriminator(input_dim=TRANSFORMER_DIM).to(device)\n",
    "\n",
    "    # initialize weights\n",
    "    # initialize_weights(style_encoder)\n",
    "    # initialize_weights(content_encoder)\n",
    "    # initialize_weights(discriminator)\n",
    "    # decoder già ha la sua inizializzazione\n",
    "    models = [style_encoder, content_encoder, discriminator, decoder]\n",
    "    model_names = [\"style_encoder\", \"content_encoder\", \"discriminator\", \"decoder\"]\n",
    "\n",
    "    for model, name in zip(models, model_names):\n",
    "        model.apply(init_weights_conservative)\n",
    "        print(f\"✅ {name} initialized\")\n",
    "\n",
    "    # optimizer for generators (style encoder, content encoder, decoder)\n",
    "    optimizer_G = optim.Adam(\n",
    "        list(style_encoder.parameters()) + list(content_encoder.parameters()) + list(decoder.parameters()),\n",
    "        lr=LR_GEN, betas=(0.5, 0.999)\n",
    "    )\n",
    "    \n",
    "    # optimizer for discriminator\n",
    "    optimizer_D = optim.Adam(discriminator.parameters(), lr=LR_DISC, betas=(0.5, 0.999))\n",
    "\n",
    "    # Create train and validation dataloaders\n",
    "    train_dataloader = get_dataloader(\n",
    "        piano_dir=\"dataset/train/piano\",\n",
    "        violin_dir=\"dataset/train/violin\",\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        stats_path=\"stats_stft_cqt.npz\"\n",
    "    )\n",
    "    \n",
    "    val_dataloader = get_dataloader(\n",
    "        piano_dir=\"dataset/val/piano\", \n",
    "        violin_dir=\"dataset/val/violin\",\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,  # No shuffle for validation\n",
    "        stats_path=\"stats_stft_cqt.npz\"\n",
    "    )\n",
    "    \n",
    "    print(f\"Training batches: {len(train_dataloader)}\")\n",
    "    print(f\"Validation batches: {len(val_dataloader)}\")\n",
    "    \n",
    "    # loss function for reconstruction\n",
    "    def recon_loss_fn(output, target):\n",
    "        loss_dict = compute_comprehensive_loss(\n",
    "            output, target, \n",
    "            lambda_temporal=LAMBDA_TEMPORAL,\n",
    "            lambda_phase=LAMBDA_PHASE,\n",
    "            lambda_spectral=LAMBDA_SPECTRAL,\n",
    "            lambda_consistency=LAMBDA_CONSISTENCY\n",
    "        )\n",
    "        return loss_dict['total_loss'], loss_dict\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    # train loop with epoch progress bar\n",
    "    epoch_pbar = tqdm(range(EPOCHS), desc=\"Training\", unit=\"epoch\")\n",
    "    \n",
    "    for epoch in epoch_pbar:\n",
    "        epoch_pbar.set_description(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "        \n",
    "        # ================================================================== #\n",
    "        #                             TRAINING                               #\n",
    "        # ================================================================== #\n",
    "        style_encoder.train()\n",
    "        content_encoder.train()\n",
    "        decoder.train()\n",
    "        discriminator.train()\n",
    "        \n",
    "        train_loss_epoch = 0\n",
    "        train_recon_loss_epoch = 0\n",
    "        train_batches = 0\n",
    "        \n",
    "        for i, (x, labels) in enumerate(train_dataloader):\n",
    "            x, labels = x.to(device), labels.to(device) # x: (B, S, 2, T, F)\n",
    "            stft_part = x[:, :, :, :, :STFT_F]  # STFT part\n",
    "\n",
    "            # ================================================================== #\n",
    "            #                             Discriminator                          #\n",
    "            # ================================================================== #\n",
    "            optimizer_D.zero_grad()\n",
    "            \n",
    "            # with torch.no_grad() to avoid computing gradients for the encoders    <----------------\n",
    "            style_emb, class_emb = style_encoder(x, labels)\n",
    "            content_emb = content_encoder(x)\n",
    "            \n",
    "            # adversarial loss for the discriminator\n",
    "            discriminator_loss, _ = adversarial_loss(style_emb.detach(), class_emb.detach(), \n",
    "                                                     content_emb.detach(), discriminator, labels, \n",
    "                                                     compute_for_discriminator=True)\n",
    "            # discriminator_loss = torch.tensor(0.0, device=device)\n",
    "            \n",
    "            discriminator_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            # ================================================================== #\n",
    "            #               Generators (Style Encoder, Content Encoder)          #\n",
    "            # ================================================================== #\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            # forward pass\n",
    "            style_emb, class_emb = style_encoder(x, labels)\n",
    "            content_emb = content_encoder(x)\n",
    "\n",
    "            # adversarial loss for the generator\n",
    "            _, adv_generator_loss = adversarial_loss(style_emb, class_emb, content_emb, discriminator, labels,\n",
    "                                                 compute_for_discriminator=False)\n",
    "\n",
    "            # adv_generator_loss = torch.tensor(0.0, device=device)\n",
    "\n",
    "            # disentanglement loss\n",
    "            disent_loss = disentanglement_loss(style_emb, content_emb.mean(dim=1), use_hsic=True)\n",
    "\n",
    "            if len(torch.unique(labels)) > 1:\n",
    "                # contrastive losses\n",
    "                loss_infonce = infoNCE_loss(style_emb, labels)\n",
    "                loss_margin = margin_loss(class_emb)\n",
    "            else:\n",
    "                # Fallback se tutti i label sono uguali in questo batch\n",
    "                loss_infonce = torch.tensor(0.0, device=device)\n",
    "                loss_margin = torch.tensor(0.0, device=device)\n",
    "\n",
    "            # reconstruction loss\n",
    "            reconstructed_spec = decoder(content_emb, style_emb, y=stft_part)  # y=x per teacher forcing\n",
    "            loss_recon, loss_dict = recon_loss_fn(reconstructed_spec, stft_part)\n",
    "\n",
    "            if torch.isnan(loss_recon):\n",
    "                print(f\"⚠️ NaN detected in reconstruction loss at batch {i+1}\")\n",
    "                print(f\"   Reconstructed spec stats: min={reconstructed_spec.min():.4f}, max={reconstructed_spec.max():.4f}\")\n",
    "                print(f\"   Target spec stats: min={stft_part.min():.4f}, max={stft_part.max():.4f}\")\n",
    "                continue  # Skip this batch\n",
    "            \n",
    "            # total generator loss\n",
    "            total_gen_loss = (\n",
    "                LAMBDA_RECON * loss_recon +\n",
    "                LAMBDA_INFO_NCE * loss_infonce +\n",
    "                LAMBDA_MARGIN * loss_margin +\n",
    "                LAMBDA_DISENTANGLE * disent_loss \n",
    "                # LAMBDA_ADV_GEN * adv_generator_loss\n",
    "            )\n",
    "\n",
    "            total_gen_loss.backward()\n",
    "\n",
    "            # gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                list(style_encoder.parameters()) + \n",
    "                list(content_encoder.parameters()) + \n",
    "                list(decoder.parameters()), \n",
    "                max_norm=0.5\n",
    "            )\n",
    "\n",
    "            # AGGIUNGI CONTROLLO NaN sui gradienti\n",
    "            has_nan_grad = False\n",
    "            for name, param in decoder.named_parameters():\n",
    "                if param.grad is not None and torch.isnan(param.grad).any():\n",
    "                    print(f\"⚠️ NaN gradient detected in {name}\")\n",
    "                    has_nan_grad = True\n",
    "            \n",
    "            if has_nan_grad:\n",
    "                print(\"⚠️ Skipping optimizer step due to NaN gradients\")\n",
    "                continue\n",
    "\n",
    "            optimizer_G.step()\n",
    "\n",
    "            # Accumulate losses\n",
    "            train_loss_epoch += total_gen_loss.item()\n",
    "            train_recon_loss_epoch += loss_recon.item()\n",
    "            train_batches += 1\n",
    "\n",
    "            # Print batch metrics every batch\n",
    "            print(f\"Epoch {epoch+1}/{EPOCHS} - Batch {i+1}/{len(train_dataloader)} | \"\n",
    "                  f\"D_loss: {discriminator_loss.item():.4f} | \"\n",
    "                  f\"G_loss: {total_gen_loss.item():.4f} | \"\n",
    "                  f\"Recon: {loss_recon.item():.4f} | \"\n",
    "                  f\"InfoNCE: {loss_infonce.item():.4f} | \"\n",
    "                  f\"Margin: {loss_margin.item():.4f} | \"\n",
    "                  f\"Disentangle: {disent_loss.item():.4f}\")\n",
    "        \n",
    "        # Average training losses\n",
    "        avg_train_loss = train_loss_epoch / train_batches\n",
    "        avg_train_recon_loss = train_recon_loss_epoch / train_batches\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # ================================================================== #\n",
    "        #                            VALIDATION                              #\n",
    "        # ================================================================== #\n",
    "        style_encoder.eval()\n",
    "        content_encoder.eval()\n",
    "        decoder.eval()\n",
    "        discriminator.eval()\n",
    "        \n",
    "        val_loss_epoch = 0\n",
    "        val_recon_loss_epoch = 0\n",
    "        val_batches = 0\n",
    "        \n",
    "        print(f\"\\n🔍 Running validation for epoch {epoch+1}...\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i, (x, labels) in enumerate(val_dataloader):\n",
    "                x, labels = x.to(device), labels.to(device)\n",
    "                stft_part = x[:, :, :, :, :STFT_F]\n",
    "\n",
    "                # Forward pass\n",
    "                style_emb, class_emb = style_encoder(x, labels)\n",
    "                content_emb = content_encoder(x)\n",
    "\n",
    "                # Validation losses (only the main ones)\n",
    "                if len(torch.unique(labels)) > 1:\n",
    "                    loss_infonce = infoNCE_loss(style_emb, labels)\n",
    "                    loss_margin = margin_loss(class_emb)\n",
    "                else:\n",
    "                    loss_infonce = torch.tensor(0.0, device=device)\n",
    "                    loss_margin = torch.tensor(0.0, device=device)\n",
    "\n",
    "                disent_loss = disentanglement_loss(style_emb, content_emb.mean(dim=1), use_hsic=True)\n",
    "                \n",
    "                # Reconstruction loss\n",
    "                reconstructed_spec = decoder(content_emb, style_emb, y=stft_part)\n",
    "                loss_recon, _ = recon_loss_fn(reconstructed_spec, stft_part)\n",
    "                \n",
    "                # Total validation loss\n",
    "                total_val_loss = (\n",
    "                    LAMBDA_RECON * loss_recon +\n",
    "                    LAMBDA_INFO_NCE * loss_infonce +\n",
    "                    LAMBDA_MARGIN * loss_margin +\n",
    "                    LAMBDA_DISENTANGLE * disent_loss\n",
    "                )\n",
    "\n",
    "                val_loss_epoch += total_val_loss.item()\n",
    "                val_recon_loss_epoch += loss_recon.item()\n",
    "                val_batches += 1\n",
    "\n",
    "        # Average validation losses\n",
    "        avg_val_loss = val_loss_epoch / val_batches\n",
    "        avg_val_recon_loss = val_recon_loss_epoch / val_batches\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f\"\\nEpoch {epoch+1}/{EPOCHS} Summary:\")\n",
    "        print(f\"    Train Loss: {avg_train_loss:.4f} | Train Recon: {avg_train_recon_loss:.4f}\")\n",
    "        print(f\"    Val Loss:   {avg_val_loss:.4f} | Val Recon:   {avg_val_recon_loss:.4f}\")\n",
    "        \n",
    "        # Update progress bar with current losses\n",
    "        epoch_pbar.set_postfix({\n",
    "            'Train_Loss': f'{avg_train_loss:.4f}',\n",
    "            'Val_Loss': f'{avg_val_loss:.4f}',\n",
    "            'Best_Val': f'{best_val_loss:.4f}'\n",
    "        })\n",
    "        \n",
    "        # Save best model based on validation loss\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            print(f\"New best validation loss: {best_val_loss:.4f}. Saving model...\")\n",
    "            \n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'style_encoder_state_dict': style_encoder.state_dict(),\n",
    "                'content_encoder_state_dict': content_encoder.state_dict(),\n",
    "                'decoder_state_dict': decoder.state_dict(),\n",
    "                'discriminator_state_dict': discriminator.state_dict(),\n",
    "                'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
    "                'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
    "                'best_val_loss': best_val_loss,\n",
    "                'train_losses': train_losses,\n",
    "                'val_losses': val_losses,\n",
    "            }, os.path.join(MODEL_SAVE_PATH, 'best_model.pth'))\n",
    "        \n",
    "        # Early stopping check (opzionale)\n",
    "        if epoch > 10 and avg_val_loss > max(val_losses[-5:]):\n",
    "            print(\"⚠️ Validation loss not improving. Consider early stopping.\")\n",
    "    \n",
    "    print(f\"\\nTraining completed!\")\n",
    "    print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df6100b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device: cuda\n",
      "✅ style_encoder initialized\n",
      "✅ content_encoder initialized\n",
      "✅ discriminator initialized\n",
      "✅ decoder initialized\n",
      "Training batches: 77\n",
      "Validation batches: 10\n",
      "Training batches: 77\n",
      "Validation batches: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   0%|          | 0/50 [00:00<?, ?epoch/s]c:\\Users\\Francesco\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:5560: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n",
      "c:\\Users\\Francesco\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:5560: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Batch 1/77 | D_loss: 1.7333 | G_loss: 41.8426 | Recon: 3.9903 | InfoNCE: 1.9390 | Margin: 0.0000 | Disentangle: 0.0072\n",
      "Epoch 1/50 - Batch 2/77 | D_loss: 1.7329 | G_loss: 28.8424 | Recon: 2.6938 | InfoNCE: 1.9045 | Margin: 0.0000 | Disentangle: 0.0012\n",
      "Epoch 1/50 - Batch 2/77 | D_loss: 1.7329 | G_loss: 28.8424 | Recon: 2.6938 | InfoNCE: 1.9045 | Margin: 0.0000 | Disentangle: 0.0012\n",
      "Epoch 1/50 - Batch 3/77 | D_loss: 1.7326 | G_loss: 67.2175 | Recon: 6.5580 | InfoNCE: 1.6376 | Margin: 0.0000 | Disentangle: 0.0005\n",
      "Epoch 1/50 - Batch 3/77 | D_loss: 1.7326 | G_loss: 67.2175 | Recon: 6.5580 | InfoNCE: 1.6376 | Margin: 0.0000 | Disentangle: 0.0005\n",
      "Epoch 1/50 - Batch 4/77 | D_loss: 1.7293 | G_loss: 27.1641 | Recon: 2.5911 | InfoNCE: 1.2531 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 4/77 | D_loss: 1.7293 | G_loss: 27.1641 | Recon: 2.5911 | InfoNCE: 1.2531 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 5/77 | D_loss: 1.7246 | G_loss: 35.7803 | Recon: 3.4675 | InfoNCE: 1.1050 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 5/77 | D_loss: 1.7246 | G_loss: 35.7803 | Recon: 3.4675 | InfoNCE: 1.1050 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 6/77 | D_loss: 1.7247 | G_loss: 55.5410 | Recon: 5.1962 | InfoNCE: 3.5790 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 6/77 | D_loss: 1.7247 | G_loss: 55.5410 | Recon: 5.1962 | InfoNCE: 3.5790 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 7/77 | D_loss: 1.7246 | G_loss: 32.6617 | Recon: 3.0442 | InfoNCE: 2.2197 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 7/77 | D_loss: 1.7246 | G_loss: 32.6617 | Recon: 3.0442 | InfoNCE: 2.2197 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 8/77 | D_loss: 1.7198 | G_loss: 176.8982 | Recon: 17.5057 | InfoNCE: 1.8415 | Margin: 0.0000 | Disentangle: 0.0002\n",
      "Epoch 1/50 - Batch 8/77 | D_loss: 1.7198 | G_loss: 176.8982 | Recon: 17.5057 | InfoNCE: 1.8415 | Margin: 0.0000 | Disentangle: 0.0002\n",
      "Epoch 1/50 - Batch 9/77 | D_loss: 1.7272 | G_loss: 43.8179 | Recon: 4.2014 | InfoNCE: 1.8039 | Margin: 0.0000 | Disentangle: 0.0012\n",
      "Epoch 1/50 - Batch 9/77 | D_loss: 1.7272 | G_loss: 43.8179 | Recon: 4.2014 | InfoNCE: 1.8039 | Margin: 0.0000 | Disentangle: 0.0012\n",
      "Epoch 1/50 - Batch 10/77 | D_loss: 1.7232 | G_loss: 39.9937 | Recon: 3.8309 | InfoNCE: 1.6842 | Margin: 0.0000 | Disentangle: 0.0004\n",
      "Epoch 1/50 - Batch 10/77 | D_loss: 1.7232 | G_loss: 39.9937 | Recon: 3.8309 | InfoNCE: 1.6842 | Margin: 0.0000 | Disentangle: 0.0004\n",
      "Epoch 1/50 - Batch 11/77 | D_loss: 1.7188 | G_loss: 28.0624 | Recon: 2.6567 | InfoNCE: 1.4957 | Margin: 0.0000 | Disentangle: 0.0005\n",
      "Epoch 1/50 - Batch 11/77 | D_loss: 1.7188 | G_loss: 28.0624 | Recon: 2.6567 | InfoNCE: 1.4957 | Margin: 0.0000 | Disentangle: 0.0005\n",
      "Epoch 1/50 - Batch 12/77 | D_loss: 1.7112 | G_loss: 30.2558 | Recon: 2.8960 | InfoNCE: 1.2956 | Margin: 0.0000 | Disentangle: 0.0002\n",
      "Epoch 1/50 - Batch 12/77 | D_loss: 1.7112 | G_loss: 30.2558 | Recon: 2.8960 | InfoNCE: 1.2956 | Margin: 0.0000 | Disentangle: 0.0002\n",
      "Epoch 1/50 - Batch 13/77 | D_loss: 1.7096 | G_loss: 40.5848 | Recon: 3.8482 | InfoNCE: 2.1023 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 13/77 | D_loss: 1.7096 | G_loss: 40.5848 | Recon: 3.8482 | InfoNCE: 2.1023 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 14/77 | D_loss: 1.7174 | G_loss: 44.2295 | Recon: 4.2553 | InfoNCE: 1.6763 | Margin: 0.0000 | Disentangle: 0.0003\n",
      "Epoch 1/50 - Batch 14/77 | D_loss: 1.7174 | G_loss: 44.2295 | Recon: 4.2553 | InfoNCE: 1.6763 | Margin: 0.0000 | Disentangle: 0.0003\n",
      "Epoch 1/50 - Batch 15/77 | D_loss: 1.7071 | G_loss: 27.8279 | Recon: 2.6303 | InfoNCE: 1.5247 | Margin: 0.0000 | Disentangle: 0.0003\n",
      "Epoch 1/50 - Batch 15/77 | D_loss: 1.7071 | G_loss: 27.8279 | Recon: 2.6303 | InfoNCE: 1.5247 | Margin: 0.0000 | Disentangle: 0.0003\n",
      "Epoch 1/50 - Batch 16/77 | D_loss: 1.6999 | G_loss: 40.8062 | Recon: 3.9434 | InfoNCE: 1.3726 | Margin: 0.0000 | Disentangle: 0.0002\n",
      "Epoch 1/50 - Batch 16/77 | D_loss: 1.6999 | G_loss: 40.8062 | Recon: 3.9434 | InfoNCE: 1.3726 | Margin: 0.0000 | Disentangle: 0.0002\n",
      "Epoch 1/50 - Batch 17/77 | D_loss: 1.6837 | G_loss: 28.2794 | Recon: 2.7129 | InfoNCE: 1.1500 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 17/77 | D_loss: 1.6837 | G_loss: 28.2794 | Recon: 2.7129 | InfoNCE: 1.1500 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 18/77 | D_loss: 1.6755 | G_loss: 32.9608 | Recon: 3.1852 | InfoNCE: 1.1087 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 18/77 | D_loss: 1.6755 | G_loss: 32.9608 | Recon: 3.1852 | InfoNCE: 1.1087 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 19/77 | D_loss: 1.6641 | G_loss: 31.0230 | Recon: 2.9919 | InfoNCE: 1.1038 | Margin: 0.0000 | Disentangle: 0.0002\n",
      "Epoch 1/50 - Batch 19/77 | D_loss: 1.6641 | G_loss: 31.0230 | Recon: 2.9919 | InfoNCE: 1.1038 | Margin: 0.0000 | Disentangle: 0.0002\n",
      "Epoch 1/50 - Batch 20/77 | D_loss: 1.6555 | G_loss: 83.8179 | Recon: 8.2713 | InfoNCE: 1.1053 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 20/77 | D_loss: 1.6555 | G_loss: 83.8179 | Recon: 8.2713 | InfoNCE: 1.1053 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 21/77 | D_loss: 1.6478 | G_loss: 24.6735 | Recon: 2.3494 | InfoNCE: 1.1800 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 21/77 | D_loss: 1.6478 | G_loss: 24.6735 | Recon: 2.3494 | InfoNCE: 1.1800 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 22/77 | D_loss: 1.6359 | G_loss: 32.2280 | Recon: 3.1114 | InfoNCE: 1.1143 | Margin: 0.0000 | Disentangle: 0.0002\n",
      "Epoch 1/50 - Batch 22/77 | D_loss: 1.6359 | G_loss: 32.2280 | Recon: 3.1114 | InfoNCE: 1.1143 | Margin: 0.0000 | Disentangle: 0.0002\n",
      "Epoch 1/50 - Batch 23/77 | D_loss: 1.6210 | G_loss: 19.2500 | Recon: 1.8151 | InfoNCE: 1.0992 | Margin: 0.0000 | Disentangle: 0.0002\n",
      "Epoch 1/50 - Batch 23/77 | D_loss: 1.6210 | G_loss: 19.2500 | Recon: 1.8151 | InfoNCE: 1.0992 | Margin: 0.0000 | Disentangle: 0.0002\n",
      "Epoch 1/50 - Batch 24/77 | D_loss: 1.6352 | G_loss: 26.1249 | Recon: 2.4112 | InfoNCE: 2.0130 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 24/77 | D_loss: 1.6352 | G_loss: 26.1249 | Recon: 2.4112 | InfoNCE: 2.0130 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 25/77 | D_loss: 1.6210 | G_loss: 27.4601 | Recon: 2.6274 | InfoNCE: 1.1864 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 25/77 | D_loss: 1.6210 | G_loss: 27.4601 | Recon: 2.6274 | InfoNCE: 1.1864 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 26/77 | D_loss: 1.6388 | G_loss: 38.8832 | Recon: 3.6725 | InfoNCE: 2.1584 | Margin: 0.0000 | Disentangle: 0.0002\n",
      "Epoch 1/50 - Batch 26/77 | D_loss: 1.6388 | G_loss: 38.8832 | Recon: 3.6725 | InfoNCE: 2.1584 | Margin: 0.0000 | Disentangle: 0.0002\n",
      "Epoch 1/50 - Batch 27/77 | D_loss: 1.6539 | G_loss: 25.3509 | Recon: 2.3923 | InfoNCE: 1.4276 | Margin: 0.0000 | Disentangle: 0.0004\n",
      "Epoch 1/50 - Batch 27/77 | D_loss: 1.6539 | G_loss: 25.3509 | Recon: 2.3923 | InfoNCE: 1.4276 | Margin: 0.0000 | Disentangle: 0.0004\n",
      "Epoch 1/50 - Batch 28/77 | D_loss: 1.6267 | G_loss: 22.7891 | Recon: 2.1502 | InfoNCE: 1.2867 | Margin: 0.0000 | Disentangle: 0.0003\n",
      "Epoch 1/50 - Batch 28/77 | D_loss: 1.6267 | G_loss: 22.7891 | Recon: 2.1502 | InfoNCE: 1.2867 | Margin: 0.0000 | Disentangle: 0.0003\n",
      "Epoch 1/50 - Batch 29/77 | D_loss: 1.6044 | G_loss: 32.4975 | Recon: 3.1102 | InfoNCE: 1.3956 | Margin: 0.0000 | Disentangle: 0.0002\n",
      "Epoch 1/50 - Batch 29/77 | D_loss: 1.6044 | G_loss: 32.4975 | Recon: 3.1102 | InfoNCE: 1.3956 | Margin: 0.0000 | Disentangle: 0.0002\n",
      "Epoch 1/50 - Batch 30/77 | D_loss: 1.5681 | G_loss: 27.5971 | Recon: 2.6492 | InfoNCE: 1.1048 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 30/77 | D_loss: 1.5681 | G_loss: 27.5971 | Recon: 2.6492 | InfoNCE: 1.1048 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 31/77 | D_loss: 1.5783 | G_loss: 30.8742 | Recon: 2.9324 | InfoNCE: 1.5506 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 31/77 | D_loss: 1.5783 | G_loss: 30.8742 | Recon: 2.9324 | InfoNCE: 1.5506 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 32/77 | D_loss: 1.5357 | G_loss: 23.7240 | Recon: 2.2622 | InfoNCE: 1.1016 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 32/77 | D_loss: 1.5357 | G_loss: 23.7240 | Recon: 2.2622 | InfoNCE: 1.1016 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 33/77 | D_loss: 1.5305 | G_loss: 32.8313 | Recon: 3.1668 | InfoNCE: 1.1629 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 33/77 | D_loss: 1.5305 | G_loss: 32.8313 | Recon: 3.1668 | InfoNCE: 1.1629 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 34/77 | D_loss: 1.5028 | G_loss: 30.3922 | Recon: 2.9291 | InfoNCE: 1.1010 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 34/77 | D_loss: 1.5028 | G_loss: 30.3922 | Recon: 2.9291 | InfoNCE: 1.1010 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 35/77 | D_loss: 1.4874 | G_loss: 42.5406 | Recon: 4.1436 | InfoNCE: 1.1049 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 35/77 | D_loss: 1.4874 | G_loss: 42.5406 | Recon: 4.1436 | InfoNCE: 1.1049 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 36/77 | D_loss: 1.4567 | G_loss: 45.5851 | Recon: 4.4485 | InfoNCE: 1.0997 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 36/77 | D_loss: 1.4567 | G_loss: 45.5851 | Recon: 4.4485 | InfoNCE: 1.0997 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 37/77 | D_loss: 1.5008 | G_loss: 29.7105 | Recon: 2.7094 | InfoNCE: 2.6166 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 37/77 | D_loss: 1.5008 | G_loss: 29.7105 | Recon: 2.7094 | InfoNCE: 2.6166 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 38/77 | D_loss: 1.4451 | G_loss: 33.2649 | Recon: 3.2145 | InfoNCE: 1.1194 | Margin: 0.0000 | Disentangle: 0.0002\n",
      "Epoch 1/50 - Batch 38/77 | D_loss: 1.4451 | G_loss: 33.2649 | Recon: 3.2145 | InfoNCE: 1.1194 | Margin: 0.0000 | Disentangle: 0.0002\n",
      "Epoch 1/50 - Batch 39/77 | D_loss: 1.4938 | G_loss: 41.7786 | Recon: 4.0104 | InfoNCE: 1.6748 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 39/77 | D_loss: 1.4938 | G_loss: 41.7786 | Recon: 4.0104 | InfoNCE: 1.6748 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 40/77 | D_loss: 1.5012 | G_loss: 59.6701 | Recon: 5.8385 | InfoNCE: 1.2852 | Margin: 0.0000 | Disentangle: 0.0002\n",
      "Epoch 1/50 - Batch 40/77 | D_loss: 1.5012 | G_loss: 59.6701 | Recon: 5.8385 | InfoNCE: 1.2852 | Margin: 0.0000 | Disentangle: 0.0002\n",
      "Epoch 1/50 - Batch 41/77 | D_loss: 1.4649 | G_loss: 44.2357 | Recon: 4.2691 | InfoNCE: 1.5443 | Margin: 0.0000 | Disentangle: 0.0002\n",
      "Epoch 1/50 - Batch 41/77 | D_loss: 1.4649 | G_loss: 44.2357 | Recon: 4.2691 | InfoNCE: 1.5443 | Margin: 0.0000 | Disentangle: 0.0002\n",
      "Epoch 1/50 - Batch 42/77 | D_loss: 1.3809 | G_loss: 36.1645 | Recon: 3.5049 | InfoNCE: 1.1156 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 42/77 | D_loss: 1.3809 | G_loss: 36.1645 | Recon: 3.5049 | InfoNCE: 1.1156 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 43/77 | D_loss: 1.3629 | G_loss: 18.9468 | Recon: 1.7786 | InfoNCE: 1.1606 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 43/77 | D_loss: 1.3629 | G_loss: 18.9468 | Recon: 1.7786 | InfoNCE: 1.1606 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 44/77 | D_loss: 1.3281 | G_loss: 31.9135 | Recon: 3.0806 | InfoNCE: 1.1074 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 44/77 | D_loss: 1.3281 | G_loss: 31.9135 | Recon: 3.0806 | InfoNCE: 1.1074 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 45/77 | D_loss: 1.3034 | G_loss: 29.0666 | Recon: 2.7967 | InfoNCE: 1.0992 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 45/77 | D_loss: 1.3034 | G_loss: 29.0666 | Recon: 2.7967 | InfoNCE: 1.0992 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 46/77 | D_loss: 1.3397 | G_loss: 39.1023 | Recon: 3.7399 | InfoNCE: 1.7033 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 46/77 | D_loss: 1.3397 | G_loss: 39.1023 | Recon: 3.7399 | InfoNCE: 1.7033 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 47/77 | D_loss: 1.2675 | G_loss: 33.1018 | Recon: 3.1991 | InfoNCE: 1.1109 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 47/77 | D_loss: 1.2675 | G_loss: 33.1018 | Recon: 3.1991 | InfoNCE: 1.1109 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 48/77 | D_loss: 1.2420 | G_loss: 33.2160 | Recon: 3.2116 | InfoNCE: 1.0997 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 48/77 | D_loss: 1.2420 | G_loss: 33.2160 | Recon: 3.2116 | InfoNCE: 1.0997 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 49/77 | D_loss: 1.2468 | G_loss: 44.9555 | Recon: 4.3784 | InfoNCE: 1.1713 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 49/77 | D_loss: 1.2468 | G_loss: 44.9555 | Recon: 4.3784 | InfoNCE: 1.1713 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 50/77 | D_loss: 1.2013 | G_loss: 26.3496 | Recon: 2.5247 | InfoNCE: 1.1023 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 50/77 | D_loss: 1.2013 | G_loss: 26.3496 | Recon: 2.5247 | InfoNCE: 1.1023 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 51/77 | D_loss: 1.2402 | G_loss: 98.4009 | Recon: 9.7010 | InfoNCE: 1.3909 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 51/77 | D_loss: 1.2402 | G_loss: 98.4009 | Recon: 9.7010 | InfoNCE: 1.3909 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 52/77 | D_loss: 1.1581 | G_loss: 30.7183 | Recon: 2.9619 | InfoNCE: 1.0996 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 52/77 | D_loss: 1.1581 | G_loss: 30.7183 | Recon: 2.9619 | InfoNCE: 1.0996 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 53/77 | D_loss: 1.1854 | G_loss: 29.2457 | Recon: 2.7939 | InfoNCE: 1.3068 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 53/77 | D_loss: 1.1854 | G_loss: 29.2457 | Recon: 2.7939 | InfoNCE: 1.3068 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 54/77 | D_loss: 1.1273 | G_loss: 24.5377 | Recon: 2.3437 | InfoNCE: 1.1009 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 54/77 | D_loss: 1.1273 | G_loss: 24.5377 | Recon: 2.3437 | InfoNCE: 1.1009 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 55/77 | D_loss: 1.1628 | G_loss: 36.2296 | Recon: 3.4843 | InfoNCE: 1.3863 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 55/77 | D_loss: 1.1628 | G_loss: 36.2296 | Recon: 3.4843 | InfoNCE: 1.3863 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 56/77 | D_loss: 1.2000 | G_loss: 49.6276 | Recon: 4.7842 | InfoNCE: 1.7860 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 56/77 | D_loss: 1.2000 | G_loss: 49.6276 | Recon: 4.7842 | InfoNCE: 1.7860 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 57/77 | D_loss: 1.0701 | G_loss: 47.1276 | Recon: 4.6027 | InfoNCE: 1.1007 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 57/77 | D_loss: 1.0701 | G_loss: 47.1276 | Recon: 4.6027 | InfoNCE: 1.1007 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 58/77 | D_loss: 1.3269 | G_loss: 34.1382 | Recon: 3.1387 | InfoNCE: 2.7514 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 58/77 | D_loss: 1.3269 | G_loss: 34.1382 | Recon: 3.1387 | InfoNCE: 2.7514 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 59/77 | D_loss: 1.2487 | G_loss: 27.0661 | Recon: 2.4947 | InfoNCE: 2.1193 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 59/77 | D_loss: 1.2487 | G_loss: 27.0661 | Recon: 2.4947 | InfoNCE: 2.1193 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 60/77 | D_loss: 1.0620 | G_loss: 30.0065 | Recon: 2.8893 | InfoNCE: 1.1132 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 60/77 | D_loss: 1.0620 | G_loss: 30.0065 | Recon: 2.8893 | InfoNCE: 1.1132 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 61/77 | D_loss: 1.1365 | G_loss: 29.9311 | Recon: 2.8324 | InfoNCE: 1.6069 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 61/77 | D_loss: 1.1365 | G_loss: 29.9311 | Recon: 2.8324 | InfoNCE: 1.6069 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 62/77 | D_loss: 1.0907 | G_loss: 28.1132 | Recon: 2.6981 | InfoNCE: 1.1323 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 62/77 | D_loss: 1.0907 | G_loss: 28.1132 | Recon: 2.6981 | InfoNCE: 1.1323 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 63/77 | D_loss: 0.9982 | G_loss: 32.3649 | Recon: 3.1261 | InfoNCE: 1.1040 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 63/77 | D_loss: 0.9982 | G_loss: 32.3649 | Recon: 3.1261 | InfoNCE: 1.1040 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 64/77 | D_loss: 1.0345 | G_loss: 28.5303 | Recon: 2.7330 | InfoNCE: 1.2007 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 64/77 | D_loss: 1.0345 | G_loss: 28.5303 | Recon: 2.7330 | InfoNCE: 1.2007 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 65/77 | D_loss: 0.9709 | G_loss: 33.1490 | Recon: 3.2050 | InfoNCE: 1.0994 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 65/77 | D_loss: 0.9709 | G_loss: 33.1490 | Recon: 3.2050 | InfoNCE: 1.0994 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 66/77 | D_loss: 0.9607 | G_loss: 19.6241 | Recon: 1.8525 | InfoNCE: 1.0990 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 66/77 | D_loss: 0.9607 | G_loss: 19.6241 | Recon: 1.8525 | InfoNCE: 1.0990 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 67/77 | D_loss: 0.9494 | G_loss: 40.1696 | Recon: 3.9067 | InfoNCE: 1.1025 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 67/77 | D_loss: 0.9494 | G_loss: 40.1696 | Recon: 3.9067 | InfoNCE: 1.1025 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 68/77 | D_loss: 0.9328 | G_loss: 42.5395 | Recon: 4.1440 | InfoNCE: 1.0996 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 68/77 | D_loss: 0.9328 | G_loss: 42.5395 | Recon: 4.1440 | InfoNCE: 1.0996 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 69/77 | D_loss: 0.9230 | G_loss: 16.4877 | Recon: 1.5389 | InfoNCE: 1.0991 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 69/77 | D_loss: 0.9230 | G_loss: 16.4877 | Recon: 1.5389 | InfoNCE: 1.0991 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 70/77 | D_loss: 0.9103 | G_loss: 28.9170 | Recon: 2.7818 | InfoNCE: 1.0993 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 70/77 | D_loss: 0.9103 | G_loss: 28.9170 | Recon: 2.7818 | InfoNCE: 1.0993 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 71/77 | D_loss: 0.8932 | G_loss: 35.3666 | Recon: 3.4267 | InfoNCE: 1.0994 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 71/77 | D_loss: 0.8932 | G_loss: 35.3666 | Recon: 3.4267 | InfoNCE: 1.0994 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 72/77 | D_loss: 0.8910 | G_loss: 41.9078 | Recon: 4.0809 | InfoNCE: 1.0990 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 72/77 | D_loss: 0.8910 | G_loss: 41.9078 | Recon: 4.0809 | InfoNCE: 1.0990 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 73/77 | D_loss: 1.1798 | G_loss: 53.0773 | Recon: 5.0094 | InfoNCE: 2.9833 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 73/77 | D_loss: 1.1798 | G_loss: 53.0773 | Recon: 5.0094 | InfoNCE: 2.9833 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 1/50 - Batch 74/77 | D_loss: 0.9354 | G_loss: 21.6454 | Recon: 2.0537 | InfoNCE: 1.1086 | Margin: 0.0000 | Disentangle: 0.0002\n",
      "Epoch 1/50 - Batch 74/77 | D_loss: 0.9354 | G_loss: 21.6454 | Recon: 2.0537 | InfoNCE: 1.1086 | Margin: 0.0000 | Disentangle: 0.0002\n",
      "Epoch 1/50 - Batch 75/77 | D_loss: 0.9543 | G_loss: 32.2993 | Recon: 3.1171 | InfoNCE: 1.1287 | Margin: 0.0000 | Disentangle: 0.0002\n",
      "Epoch 1/50 - Batch 75/77 | D_loss: 0.9543 | G_loss: 32.2993 | Recon: 3.1171 | InfoNCE: 1.1287 | Margin: 0.0000 | Disentangle: 0.0002\n",
      "Epoch 1/50 - Batch 76/77 | D_loss: 1.1916 | G_loss: 34.6671 | Recon: 3.2248 | InfoNCE: 2.4186 | Margin: 0.0000 | Disentangle: 0.0002\n",
      "Epoch 1/50 - Batch 76/77 | D_loss: 1.1916 | G_loss: 34.6671 | Recon: 3.2248 | InfoNCE: 2.4186 | Margin: 0.0000 | Disentangle: 0.0002\n",
      "Epoch 1/50 - Batch 77/77 | D_loss: 1.0331 | G_loss: 34.9207 | Recon: 3.4103 | InfoNCE: 0.8179 | Margin: 0.0000 | Disentangle: 0.0004\n",
      "\n",
      "🔍 Running validation for epoch 1...\n",
      "Epoch 1/50 - Batch 77/77 | D_loss: 1.0331 | G_loss: 34.9207 | Recon: 3.4103 | InfoNCE: 0.8179 | Margin: 0.0000 | Disentangle: 0.0004\n",
      "\n",
      "🔍 Running validation for epoch 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   2%|▏         | 1/50 [00:57<47:12, 57.81s/epoch, Train_Loss=37.4376, Val_Loss=41.9694, Best_Val=inf]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/50 Summary:\n",
      "    Train Loss: 37.4376 | Train Recon: 3.6004\n",
      "    Val Loss:   41.9694 | Val Recon:   4.0653\n",
      "New best validation loss: 41.9694. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50:   2%|▏         | 1/50 [00:57<47:12, 57.81s/epoch, Train_Loss=37.4376, Val_Loss=41.9694, Best_Val=inf]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50 - Batch 1/77 | D_loss: 0.9301 | G_loss: 27.5369 | Recon: 2.6400 | InfoNCE: 1.1364 | Margin: 0.0000 | Disentangle: 0.0002\n",
      "Epoch 2/50 - Batch 2/77 | D_loss: 0.8717 | G_loss: 30.3047 | Recon: 2.9203 | InfoNCE: 1.1016 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 2/77 | D_loss: 0.8717 | G_loss: 30.3047 | Recon: 2.9203 | InfoNCE: 1.1016 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 3/77 | D_loss: 0.8649 | G_loss: 58.1938 | Recon: 5.7094 | InfoNCE: 1.0998 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 3/77 | D_loss: 0.8649 | G_loss: 58.1938 | Recon: 5.7094 | InfoNCE: 1.0998 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 4/77 | D_loss: 0.8638 | G_loss: 33.1460 | Recon: 3.2045 | InfoNCE: 1.1008 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 4/77 | D_loss: 0.8638 | G_loss: 33.1460 | Recon: 3.2045 | InfoNCE: 1.1008 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 5/77 | D_loss: 0.8529 | G_loss: 103.2068 | Recon: 10.2107 | InfoNCE: 1.0994 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 5/77 | D_loss: 0.8529 | G_loss: 103.2068 | Recon: 10.2107 | InfoNCE: 1.0994 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 6/77 | D_loss: 0.8441 | G_loss: 24.7108 | Recon: 2.3612 | InfoNCE: 1.0991 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 6/77 | D_loss: 0.8441 | G_loss: 24.7108 | Recon: 2.3612 | InfoNCE: 1.0991 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 7/77 | D_loss: 0.8404 | G_loss: 48.0438 | Recon: 4.6945 | InfoNCE: 1.0993 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 7/77 | D_loss: 0.8404 | G_loss: 48.0438 | Recon: 4.6945 | InfoNCE: 1.0993 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 8/77 | D_loss: 0.8239 | G_loss: 27.4561 | Recon: 2.6356 | InfoNCE: 1.0997 | Margin: 0.0000 | Disentangle: 0.0002\n",
      "Epoch 2/50 - Batch 8/77 | D_loss: 0.8239 | G_loss: 27.4561 | Recon: 2.6356 | InfoNCE: 1.0997 | Margin: 0.0000 | Disentangle: 0.0002\n",
      "Epoch 2/50 - Batch 9/77 | D_loss: 0.8253 | G_loss: 22.3290 | Recon: 2.1229 | InfoNCE: 1.1001 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 9/77 | D_loss: 0.8253 | G_loss: 22.3290 | Recon: 2.1229 | InfoNCE: 1.1001 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 10/77 | D_loss: 0.8177 | G_loss: 36.8878 | Recon: 3.5788 | InfoNCE: 1.0999 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 10/77 | D_loss: 0.8177 | G_loss: 36.8878 | Recon: 3.5788 | InfoNCE: 1.0999 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 11/77 | D_loss: 0.8139 | G_loss: 29.7614 | Recon: 2.8662 | InfoNCE: 1.0989 | Margin: 0.0000 | Disentangle: 0.0002\n",
      "Epoch 2/50 - Batch 11/77 | D_loss: 0.8139 | G_loss: 29.7614 | Recon: 2.8662 | InfoNCE: 1.0989 | Margin: 0.0000 | Disentangle: 0.0002\n",
      "Epoch 2/50 - Batch 12/77 | D_loss: 0.8087 | G_loss: 26.2283 | Recon: 2.5129 | InfoNCE: 1.0997 | Margin: 0.0000 | Disentangle: 0.0002\n",
      "Epoch 2/50 - Batch 12/77 | D_loss: 0.8087 | G_loss: 26.2283 | Recon: 2.5129 | InfoNCE: 1.0997 | Margin: 0.0000 | Disentangle: 0.0002\n",
      "Epoch 2/50 - Batch 13/77 | D_loss: 0.8002 | G_loss: 22.2099 | Recon: 2.1111 | InfoNCE: 1.0993 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 13/77 | D_loss: 0.8002 | G_loss: 22.2099 | Recon: 2.1111 | InfoNCE: 1.0993 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 14/77 | D_loss: 0.7976 | G_loss: 21.3553 | Recon: 2.0254 | InfoNCE: 1.1014 | Margin: 0.0000 | Disentangle: 0.0002\n",
      "Epoch 2/50 - Batch 14/77 | D_loss: 0.7976 | G_loss: 21.3553 | Recon: 2.0254 | InfoNCE: 1.1014 | Margin: 0.0000 | Disentangle: 0.0002\n",
      "Epoch 2/50 - Batch 15/77 | D_loss: 0.7918 | G_loss: 34.1060 | Recon: 3.3007 | InfoNCE: 1.0992 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 15/77 | D_loss: 0.7918 | G_loss: 34.1060 | Recon: 3.3007 | InfoNCE: 1.0992 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 16/77 | D_loss: 0.9700 | G_loss: 24.1867 | Recon: 2.2379 | InfoNCE: 1.8076 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 16/77 | D_loss: 0.9700 | G_loss: 24.1867 | Recon: 2.2379 | InfoNCE: 1.8076 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 17/77 | D_loss: 0.7843 | G_loss: 20.9558 | Recon: 1.9856 | InfoNCE: 1.0995 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 17/77 | D_loss: 0.7843 | G_loss: 20.9558 | Recon: 1.9856 | InfoNCE: 1.0995 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 18/77 | D_loss: 0.7810 | G_loss: 20.6580 | Recon: 1.9557 | InfoNCE: 1.1006 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 18/77 | D_loss: 0.7810 | G_loss: 20.6580 | Recon: 1.9557 | InfoNCE: 1.1006 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 19/77 | D_loss: 0.7791 | G_loss: 28.2170 | Recon: 2.7118 | InfoNCE: 1.0991 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 19/77 | D_loss: 0.7791 | G_loss: 28.2170 | Recon: 2.7118 | InfoNCE: 1.0991 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 20/77 | D_loss: 1.4490 | G_loss: 68.4847 | Recon: 6.3850 | InfoNCE: 4.6342 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 20/77 | D_loss: 1.4490 | G_loss: 68.4847 | Recon: 6.3850 | InfoNCE: 4.6342 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 21/77 | D_loss: 0.7772 | G_loss: 44.0411 | Recon: 4.2942 | InfoNCE: 1.0993 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 21/77 | D_loss: 0.7772 | G_loss: 44.0411 | Recon: 4.2942 | InfoNCE: 1.0993 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 22/77 | D_loss: 0.8084 | G_loss: 42.0863 | Recon: 4.0831 | InfoNCE: 1.2555 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 22/77 | D_loss: 0.8084 | G_loss: 42.0863 | Recon: 4.0831 | InfoNCE: 1.2555 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 23/77 | D_loss: 0.8012 | G_loss: 32.9274 | Recon: 3.1753 | InfoNCE: 1.1741 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 23/77 | D_loss: 0.8012 | G_loss: 32.9274 | Recon: 3.1753 | InfoNCE: 1.1741 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 24/77 | D_loss: 0.7596 | G_loss: 19.9572 | Recon: 1.8858 | InfoNCE: 1.0993 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 24/77 | D_loss: 0.7596 | G_loss: 19.9572 | Recon: 1.8858 | InfoNCE: 1.0993 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 25/77 | D_loss: 0.7547 | G_loss: 33.9409 | Recon: 3.2839 | InfoNCE: 1.1024 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 25/77 | D_loss: 0.7547 | G_loss: 33.9409 | Recon: 3.2839 | InfoNCE: 1.1024 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 26/77 | D_loss: 0.7645 | G_loss: 44.8185 | Recon: 4.3719 | InfoNCE: 1.0997 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 26/77 | D_loss: 0.7645 | G_loss: 44.8185 | Recon: 4.3719 | InfoNCE: 1.0997 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 27/77 | D_loss: 0.7502 | G_loss: 38.9144 | Recon: 3.7815 | InfoNCE: 1.0994 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 27/77 | D_loss: 0.7502 | G_loss: 38.9144 | Recon: 3.7815 | InfoNCE: 1.0994 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 28/77 | D_loss: 0.7581 | G_loss: 29.5930 | Recon: 2.8493 | InfoNCE: 1.1000 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 28/77 | D_loss: 0.7581 | G_loss: 29.5930 | Recon: 2.8493 | InfoNCE: 1.1000 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 29/77 | D_loss: 0.7576 | G_loss: 180.8579 | Recon: 17.9746 | InfoNCE: 1.1120 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 29/77 | D_loss: 0.7576 | G_loss: 180.8579 | Recon: 17.9746 | InfoNCE: 1.1120 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 30/77 | D_loss: 0.7569 | G_loss: 26.9549 | Recon: 2.5856 | InfoNCE: 1.0994 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 30/77 | D_loss: 0.7569 | G_loss: 26.9549 | Recon: 2.5856 | InfoNCE: 1.0994 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 31/77 | D_loss: 0.7463 | G_loss: 33.3151 | Recon: 3.2216 | InfoNCE: 1.0991 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 31/77 | D_loss: 0.7463 | G_loss: 33.3151 | Recon: 3.2216 | InfoNCE: 1.0991 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 32/77 | D_loss: 0.7591 | G_loss: 35.8648 | Recon: 3.4761 | InfoNCE: 1.1042 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 32/77 | D_loss: 0.7591 | G_loss: 35.8648 | Recon: 3.4761 | InfoNCE: 1.1042 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 33/77 | D_loss: 0.7380 | G_loss: 18.4893 | Recon: 1.7390 | InfoNCE: 1.0991 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 33/77 | D_loss: 0.7380 | G_loss: 18.4893 | Recon: 1.7390 | InfoNCE: 1.0991 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 34/77 | D_loss: 0.7488 | G_loss: 21.4509 | Recon: 2.0352 | InfoNCE: 1.0991 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 34/77 | D_loss: 0.7488 | G_loss: 21.4509 | Recon: 2.0352 | InfoNCE: 1.0991 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 35/77 | D_loss: 0.7403 | G_loss: 19.3878 | Recon: 1.8288 | InfoNCE: 1.1002 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 35/77 | D_loss: 0.7403 | G_loss: 19.3878 | Recon: 1.8288 | InfoNCE: 1.1002 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 36/77 | D_loss: 0.7400 | G_loss: 26.6685 | Recon: 2.5568 | InfoNCE: 1.1009 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 36/77 | D_loss: 0.7400 | G_loss: 26.6685 | Recon: 2.5568 | InfoNCE: 1.1009 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 37/77 | D_loss: 0.7410 | G_loss: 31.4392 | Recon: 3.0339 | InfoNCE: 1.0998 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 37/77 | D_loss: 0.7410 | G_loss: 31.4392 | Recon: 3.0339 | InfoNCE: 1.0998 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 38/77 | D_loss: 0.7396 | G_loss: 33.1144 | Recon: 3.2015 | InfoNCE: 1.0994 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 38/77 | D_loss: 0.7396 | G_loss: 33.1144 | Recon: 3.2015 | InfoNCE: 1.0994 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 39/77 | D_loss: 0.7341 | G_loss: 40.1495 | Recon: 3.9050 | InfoNCE: 1.0995 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 39/77 | D_loss: 0.7341 | G_loss: 40.1495 | Recon: 3.9050 | InfoNCE: 1.0995 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 40/77 | D_loss: 0.7378 | G_loss: 22.0360 | Recon: 2.0937 | InfoNCE: 1.0993 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 40/77 | D_loss: 0.7378 | G_loss: 22.0360 | Recon: 2.0937 | InfoNCE: 1.0993 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 41/77 | D_loss: 0.7440 | G_loss: 34.7300 | Recon: 3.3631 | InfoNCE: 1.0990 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 41/77 | D_loss: 0.7440 | G_loss: 34.7300 | Recon: 3.3631 | InfoNCE: 1.0990 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 42/77 | D_loss: 0.7349 | G_loss: 47.3022 | Recon: 4.6203 | InfoNCE: 1.0993 | Margin: 0.0000 | Disentangle: 0.0002\n",
      "Epoch 2/50 - Batch 42/77 | D_loss: 0.7349 | G_loss: 47.3022 | Recon: 4.6203 | InfoNCE: 1.0993 | Margin: 0.0000 | Disentangle: 0.0002\n",
      "Epoch 2/50 - Batch 43/77 | D_loss: 0.7235 | G_loss: 26.3683 | Recon: 2.5269 | InfoNCE: 1.0990 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 43/77 | D_loss: 0.7235 | G_loss: 26.3683 | Recon: 2.5269 | InfoNCE: 1.0990 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 44/77 | D_loss: 0.7202 | G_loss: 28.0483 | Recon: 2.6949 | InfoNCE: 1.0990 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 44/77 | D_loss: 0.7202 | G_loss: 28.0483 | Recon: 2.6949 | InfoNCE: 1.0990 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 45/77 | D_loss: 0.7371 | G_loss: 31.4346 | Recon: 3.0336 | InfoNCE: 1.0990 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 45/77 | D_loss: 0.7371 | G_loss: 31.4346 | Recon: 3.0336 | InfoNCE: 1.0990 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 46/77 | D_loss: 0.7274 | G_loss: 49.1624 | Recon: 4.8063 | InfoNCE: 1.0991 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 46/77 | D_loss: 0.7274 | G_loss: 49.1624 | Recon: 4.8063 | InfoNCE: 1.0991 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 47/77 | D_loss: 0.7217 | G_loss: 21.4609 | Recon: 2.0361 | InfoNCE: 1.0997 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 47/77 | D_loss: 0.7217 | G_loss: 21.4609 | Recon: 2.0361 | InfoNCE: 1.0997 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 48/77 | D_loss: 0.7196 | G_loss: 36.2168 | Recon: 3.5116 | InfoNCE: 1.1004 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 48/77 | D_loss: 0.7196 | G_loss: 36.2168 | Recon: 3.5116 | InfoNCE: 1.1004 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 49/77 | D_loss: 0.7240 | G_loss: 63.3761 | Recon: 6.2277 | InfoNCE: 1.0991 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 49/77 | D_loss: 0.7240 | G_loss: 63.3761 | Recon: 6.2277 | InfoNCE: 1.0991 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 50/77 | D_loss: 0.7202 | G_loss: 29.4696 | Recon: 2.8371 | InfoNCE: 1.0989 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 50/77 | D_loss: 0.7202 | G_loss: 29.4696 | Recon: 2.8371 | InfoNCE: 1.0989 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 51/77 | D_loss: 0.8009 | G_loss: 19.7158 | Recon: 1.8173 | InfoNCE: 1.5427 | Margin: 0.0000 | Disentangle: 0.0001\n",
      "Epoch 2/50 - Batch 51/77 | D_loss: 0.8009 | G_loss: 19.7158 | Recon: 1.8173 | InfoNCE: 1.5427 | Margin: 0.0000 | Disentangle: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50:   2%|▏         | 1/50 [01:32<1:15:11, 92.08s/epoch, Train_Loss=37.4376, Val_Loss=41.9694, Best_Val=inf]\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-b398ef5f7850>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Avvia il training e cattura le loss curves\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_losses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_losses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-baf174d9782a>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    159\u001b[0m             )\n\u001b[0;32m    160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m             \u001b[0mtotal_gen_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m             \u001b[1;31m# gradient clipping\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Francesco\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    519\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m             )\n\u001b[1;32m--> 521\u001b[1;33m         torch.autograd.backward(\n\u001b[0m\u001b[0;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m         )\n",
      "\u001b[1;32mc:\\Users\\Francesco\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    287\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 289\u001b[1;33m     _engine_run_backward(\n\u001b[0m\u001b[0;32m    290\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Francesco\\anaconda3\\lib\\site-packages\\torch\\autograd\\graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    768\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 769\u001b[1;33m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    770\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    771\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Avvia il training e cattura le loss curves\n",
    "train_losses, val_losses = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db7c5d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "📊 ANALISI PARAMETRI ALLENABILI DEL MODELLO\n",
      "============================================================\n",
      "  Style Encoder: 12,905,312 parametri (12,905,312 allenabili) |   49.2 MB\n",
      "Content Encoder:  4,450,480 parametri ( 4,450,480 allenabili) |   17.0 MB\n",
      "        Decoder:  3,681,515 parametri ( 3,681,515 allenabili) |   14.0 MB\n",
      "  Discriminator:     49,666 parametri (    49,666 allenabili) |    0.2 MB\n",
      "------------------------------------------------------------\n",
      "         TOTALE: 21,086,973 parametri (21,086,973 allenabili)\n",
      "     DIMENSIONE:                                            80.4 MB\n",
      "   MEM. STIMATA:                                          322 MB (solo modello)\n",
      "\n",
      "============================================================\n",
      "💾 ANALISI MEMORIA PER BATCH SIZE\n",
      "============================================================\n",
      "Batch size  4:    364 MB ✅\n",
      "Batch size  8:    405 MB ✅\n",
      "Batch size 16:    489 MB ✅\n",
      "Batch size 32:    656 MB ✅\n",
      "\n",
      "============================================================\n",
      "🎯 CONFIGURAZIONE RACCOMANDATA\n",
      "============================================================\n",
      "✅ Modello di dimensioni ragionevoli per il training\n",
      "\n",
      "🚀 Per GPU con 12GB: Batch size raccomandato = 8\n",
      "🔧 Per GPU con 8GB:  Batch size raccomandato = 4\n"
     ]
    }
   ],
   "source": [
    "def count_trainable_parameters():\n",
    "    \"\"\"Conta i parametri allenabili di tutto il modello\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"📊 ANALISI PARAMETRI ALLENABILI DEL MODELLO\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Crea tutti i modelli\n",
    "    style_encoder = StyleEncoder(transformer_dim=TRANSFORMER_DIM)\n",
    "    content_encoder = ContentEncoder(transformer_dim=TRANSFORMER_DIM)\n",
    "    decoder = Decoder(\n",
    "        d_model=TRANSFORMER_DIM,\n",
    "        nhead=4,\n",
    "        num_layers=4,\n",
    "        dim_feedforward=TRANSFORMER_DIM * 2,\n",
    "        dropout=0.1\n",
    "    )\n",
    "    discriminator = Discriminator(input_dim=TRANSFORMER_DIM)\n",
    "    \n",
    "    models = [\n",
    "        (\"Style Encoder\", style_encoder),\n",
    "        (\"Content Encoder\", content_encoder),\n",
    "        (\"Decoder\", decoder),\n",
    "        (\"Discriminator\", discriminator)\n",
    "    ]\n",
    "    \n",
    "    total_params = 0\n",
    "    total_trainable_params = 0\n",
    "    \n",
    "    for name, model in models:\n",
    "        # Conta parametri totali\n",
    "        model_params = sum(p.numel() for p in model.parameters())\n",
    "        # Conta parametri allenabili\n",
    "        model_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        \n",
    "        total_params += model_params\n",
    "        total_trainable_params += model_trainable_params\n",
    "        \n",
    "        # Dimensione in MB\n",
    "        param_size_mb = sum(p.numel() * p.element_size() for p in model.parameters()) / (1024**2)\n",
    "        \n",
    "        print(f\"{name:>15}: {model_params:>10,} parametri ({model_trainable_params:>10,} allenabili) | {param_size_mb:>6.1f} MB\")\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'TOTALE':>15}: {total_params:>10,} parametri ({total_trainable_params:>10,} allenabili)\")\n",
    "    \n",
    "    # Calcola dimensioni totali\n",
    "    total_size_mb = 0\n",
    "    for name, model in models:\n",
    "        total_size_mb += sum(p.numel() * p.element_size() for p in model.parameters()) / (1024**2)\n",
    "    \n",
    "    print(f\"{'DIMENSIONE':>15}: {total_size_mb:>47.1f} MB\")\n",
    "    \n",
    "    # Stima memoria GPU per training\n",
    "    # Modello + gradients + optimizer states (Adam ~ 2x params) + attivazioni\n",
    "    estimated_gpu_memory = total_size_mb * 4  # rough estimate\n",
    "    print(f\"{'MEM. STIMATA':>15}: {estimated_gpu_memory:>44.0f} MB (solo modello)\")\n",
    "    \n",
    "    # Analisi per batch size\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"💾 ANALISI MEMORIA PER BATCH SIZE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    batch_sizes = [4, 8, 16, 32]\n",
    "    for bs in batch_sizes:\n",
    "        # Stima memoria per batch (approssimativa)\n",
    "        # Input: (B, S, 2, T, F) * 4 bytes per float32\n",
    "        input_size_mb = bs * NUM_FRAMES * 2 * STFT_T * (STFT_F + CQT_F) * 4 / (1024**2)\n",
    "        \n",
    "        # Memoria totale stimata\n",
    "        total_mem_mb = estimated_gpu_memory + input_size_mb * 2  # input + gradients\n",
    "        \n",
    "        status = \"✅\" if total_mem_mb < 8000 else \"⚠️\" if total_mem_mb < 12000 else \"❌\"\n",
    "        print(f\"Batch size {bs:>2}: {total_mem_mb:>6.0f} MB {status}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"🎯 CONFIGURAZIONE RACCOMANDATA\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if total_trainable_params < 50_000_000:  # 50M\n",
    "        print(\"✅ Modello di dimensioni ragionevoli per il training\")\n",
    "    elif total_trainable_params < 100_000_000:  # 100M\n",
    "        print(\"⚠️ Modello grande - considerare mixed precision training\")\n",
    "    else:\n",
    "        print(\"❌ Modello molto grande - ridurre dimensioni o usare tecniche avanzate\")\n",
    "    \n",
    "    # Raccomandazioni finali\n",
    "    print(f\"\\n🚀 Per GPU con 12GB: Batch size raccomandato = {8 if estimated_gpu_memory < 4000 else 4}\")\n",
    "    print(f\"🔧 Per GPU con 8GB:  Batch size raccomandato = {4 if estimated_gpu_memory < 3000 else 2}\")\n",
    "    \n",
    "    return total_trainable_params, total_size_mb\n",
    "\n",
    "# Esegui l'analisi\n",
    "total_params, model_size = count_trainable_parameters()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
