{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "205dcdf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3080\n",
      "GPU Memory: 10.0 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# models\n",
    "from style_encoder import StyleEncoder, initialize_weights\n",
    "from content_encoder import ContentEncoder\n",
    "from discriminator import Discriminator\n",
    "from new_decoder import Decoder, compute_comprehensive_loss  # Nuovo decoder dinamico\n",
    "from losses import infoNCE_loss, margin_loss, adversarial_loss, disentanglement_loss\n",
    "from dummy_dataloader import get_dummy_dataloader\n",
    "\n",
    "# device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "# hyperparameters\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 8\n",
    "LR_GEN = 1e-4  # Learning rate per Encoders + Decoder\n",
    "LR_DISC = 1e-4  # Learning rate per Discriminator\n",
    "TRANSFORMER_DIM = 256\n",
    "NUM_FRAMES = 4 # S (ora solo per riferimento, il decoder √® dinamico)\n",
    "STFT_T, STFT_F = 287, 513 # Dimensioni STFT (assumendo n_fft=1024)\n",
    "CQT_T, CQT_F = 287, 84   # Dimensioni CQT\n",
    "\n",
    "# loss weights\n",
    "LAMBDA_RECON = 10.0\n",
    "LAMBDA_INFO_NCE = 1.0\n",
    "LAMBDA_MARGIN = 1.0\n",
    "LAMBDA_DISENTANGLE = 0.1\n",
    "LAMBDA_ADV_GEN = 0.5 # Peso per la loss avversaria del generatore\n",
    "\n",
    "# Pesi per le loss comprehensive del decoder\n",
    "LAMBDA_TEMPORAL = 0.3\n",
    "LAMBDA_PHASE = 0.2\n",
    "LAMBDA_SPECTRAL = 0.1\n",
    "LAMBDA_CONSISTENCY = 0.1\n",
    "\n",
    "MODEL_SAVE_PATH = \"./saved_models\"\n",
    "os.makedirs(MODEL_SAVE_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b43ed26",
   "metadata": {},
   "source": [
    "aggiungere data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09e7c204",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    print(f\"Training on device: {device}\")\n",
    "\n",
    "    # load models\n",
    "    style_encoder = StyleEncoder(transformer_dim=TRANSFORMER_DIM).to(device)\n",
    "    content_encoder = ContentEncoder(transformer_dim=TRANSFORMER_DIM).to(device)\n",
    "\n",
    "    # decoder\n",
    "    decoder = Decoder(\n",
    "        d_model=TRANSFORMER_DIM,\n",
    "        nhead=4,\n",
    "        num_layers=4,  # Ridotto per Colab\n",
    "        dim_feedforward=TRANSFORMER_DIM * 2,\n",
    "        dropout=0.1\n",
    "    ).to(device)\n",
    "    \n",
    "    discriminator = Discriminator(input_dim=TRANSFORMER_DIM).to(device)\n",
    "\n",
    "    # initialize weights\n",
    "    initialize_weights(style_encoder)\n",
    "    initialize_weights(content_encoder)\n",
    "    initialize_weights(discriminator)\n",
    "    # decoder gi√† ha la sua inizializzazione\n",
    "\n",
    "    # optimizer for generators (style encoder, content encoder, decoder)\n",
    "    optimizer_G = optim.Adam(\n",
    "        list(style_encoder.parameters()) + list(content_encoder.parameters()) + list(decoder.parameters()),\n",
    "        lr=LR_GEN, betas=(0.5, 0.999)\n",
    "    )\n",
    "    \n",
    "    # optimizer for discriminator\n",
    "    optimizer_D = optim.Adam(discriminator.parameters(), lr=LR_DISC, betas=(0.5, 0.999))\n",
    "\n",
    "    dataloader = get_dummy_dataloader(batch_size=BATCH_SIZE)\n",
    "    \n",
    "    # loss function for reconstruction\n",
    "    def recon_loss_fn(output, target):\n",
    "        loss_dict = compute_comprehensive_loss(\n",
    "            output, target, \n",
    "            lambda_temporal=LAMBDA_TEMPORAL,\n",
    "            lambda_phase=LAMBDA_PHASE,\n",
    "            lambda_spectral=LAMBDA_SPECTRAL,\n",
    "            lambda_consistency=LAMBDA_CONSISTENCY\n",
    "        )\n",
    "        return loss_dict['total_loss'], loss_dict\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "\n",
    "    # train loop\n",
    "    for epoch in range(EPOCHS):\n",
    "        \n",
    "        style_encoder.train()\n",
    "        content_encoder.train()\n",
    "        decoder.train()\n",
    "        discriminator.train()\n",
    "        \n",
    "        for i, (x, labels) in enumerate(tqdm(dataloader, unit=\"batch\", desc=f\"Epoch {epoch+1}/{EPOCHS}\")):\n",
    "            x, labels = x.to(device), labels.to(device) # x: (B, S, 2, T, F)\n",
    "            stft_part = x[:, :, :, :, :STFT_F]  # STFT part\n",
    "\n",
    "            # ================================================================== #\n",
    "            #                             Discriminator                          #\n",
    "            # ================================================================== #\n",
    "            optimizer_D.zero_grad()\n",
    "            \n",
    "            # compute embeddings with no_grad to avoid backpropagation through the encoders\n",
    "            with torch.no_grad():\n",
    "                style_emb, class_emb = style_encoder(x, labels)\n",
    "                content_emb = content_encoder(x)\n",
    "            \n",
    "            # adversarial loss for the discriminator\n",
    "            discriminator_loss, _ = adversarial_loss(style_emb.detach(), class_emb.detach(), \n",
    "                                                     content_emb.detach(), discriminator, labels, \n",
    "                                                     compute_for_discriminator=True)\n",
    "            \n",
    "            discriminator_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "\n",
    "            # ================================================================== #\n",
    "            #               Generators (Style Encoder, Content Encoder)          #\n",
    "            # ================================================================== #\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            # forward pass\n",
    "            style_emb, class_emb = style_encoder(x, labels)\n",
    "            content_emb = content_encoder(x)\n",
    "\n",
    "            # adversarial loss for the generator\n",
    "            _, adv_generator_loss = adversarial_loss(style_emb, class_emb, content_emb, discriminator, labels,\n",
    "                                                 compute_for_discriminator=False)\n",
    "\n",
    "            # disentanglement loss\n",
    "            disent_loss = disentanglement_loss(style_emb, content_emb.mean(dim=1), use_hsic=True)\n",
    "\n",
    "            if len(torch.unique(labels)) > 1:\n",
    "                # contrastive losses\n",
    "                loss_infonce = infoNCE_loss(style_emb, labels)\n",
    "                loss_margin = margin_loss(class_emb)\n",
    "            else:\n",
    "                raise ValueError(\"Labels must contain at least two unique classes for contrastive losses.\")\n",
    "\n",
    "            # reconstruction loss\n",
    "            reconstructed_spec = decoder(content_emb, style_emb, y=stft_part)  # y=x per teacher forcing\n",
    "            loss_recon, loss_dict = recon_loss_fn(reconstructed_spec, stft_part)\n",
    "            \n",
    "            # total generator loss\n",
    "            total_gen_loss = (\n",
    "                LAMBDA_RECON * loss_recon +\n",
    "                LAMBDA_INFO_NCE * loss_infonce +\n",
    "                LAMBDA_MARGIN * loss_margin +\n",
    "                LAMBDA_DISENTANGLE * disent_loss +\n",
    "                LAMBDA_ADV_GEN * adv_generator_loss\n",
    "            )\n",
    "\n",
    "            total_gen_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            # loss logging \n",
    "            if i % 1 == 0:\n",
    "                # Converte tensor scalari a float per evitare errori\n",
    "                temporal_loss_val = loss_dict['temporal_loss'].item() if loss_dict['temporal_loss'].numel() > 0 else 0.0\n",
    "                spectral_loss_val = loss_dict['spectral_loss'].item() if loss_dict['spectral_loss'].numel() > 0 else 0.0\n",
    "                \n",
    "                tqdm.write(\n",
    "                    f\"Batch {i}/{len(dataloader)} | \"\n",
    "                    f\"Discriminator loss: {discriminator_loss.item():.4f} | \"\n",
    "                    f\"Total Generator loss: {total_gen_loss.item():.4f} | \"\n",
    "                    f\"Reconstruction loss: {loss_recon.item():.4f} | \"\n",
    "                    f\"  - MSE: {loss_dict['mse_loss'].item():.4f} | \"\n",
    "                    f\"  - Magnitude: {loss_dict['mag_loss'].item():.4f} | \"\n",
    "                    f\"  - Phase: {loss_dict['phase_loss'].item():.4f} | \"\n",
    "                    f\"  - Temporal: {temporal_loss_val:.4f} | \"\n",
    "                    f\"  - Spectral: {spectral_loss_val:.4f} | \"\n",
    "                    f\"Adversary Generator loss: {adv_generator_loss.item():.4f} | \"\n",
    "                    f\"Disentanglement loss: {disent_loss.item():.4f}\"\n",
    "                )\n",
    "                \n",
    "        # saving best model\n",
    "        current_recon_loss = loss_recon.item()\n",
    "        if current_recon_loss < best_loss:\n",
    "            best_loss = current_recon_loss\n",
    "            print(f\"\\nNew best reconstruction loss: {best_loss:.4f}. Saving model...\")\n",
    "            \n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'style_encoder_state_dict': style_encoder.state_dict(),\n",
    "                'content_encoder_state_dict': content_encoder.state_dict(),\n",
    "                'decoder_state_dict': decoder.state_dict(),\n",
    "                'discriminator_state_dict': discriminator.state_dict(),\n",
    "                'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
    "                'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
    "                'best_loss': best_loss,\n",
    "            }, os.path.join(MODEL_SAVE_PATH, 'best_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df6100b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   0%|          | 0/13 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of S: 20.0259\n",
      "Norm of C: 11.9199\n",
      "Norm of K: 5.7436\n",
      "Norm of L: 7.0742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   8%|‚ñä         | 1/13 [00:11<02:18, 11.51s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0/13 | Discriminator loss: 1.9618 | Total Generator loss: 36.2249 | Reconstruction loss: 3.4588 |   - MSE: 1.0003 |   - Magnitude: 2.0003 |   - Phase: 3.2910 |   - Temporal: 2.0003 |   - Spectral: 2.0010 | Adversary Generator loss: -0.6501 | Disentanglement loss: 0.0063\n",
      "Norm of S: 19.5589\n",
      "Norm of C: 11.3297\n",
      "Norm of K: 5.8023\n",
      "Norm of L: 7.1444\n",
      "Norm of S: 19.5589\n",
      "Norm of C: 11.3297\n",
      "Norm of K: 5.8023\n",
      "Norm of L: 7.1444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:  15%|‚ñà‚ñå        | 2/13 [00:16<01:23,  7.58s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/13 | Discriminator loss: 2.2692 | Total Generator loss: nan | Reconstruction loss: nan |   - MSE: nan |   - Magnitude: nan |   - Phase: nan |   - Temporal: nan |   - Spectral: nan | Adversary Generator loss: -0.3872 | Disentanglement loss: 0.0058\n",
      "Norm of S: 21.3380\n",
      "Norm of C: 11.0624\n",
      "Norm of K: 5.9204\n",
      "Norm of L: 7.3465\n",
      "Norm of S: 21.3380\n",
      "Norm of C: 11.0624\n",
      "Norm of K: 5.9204\n",
      "Norm of L: 7.3465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:  23%|‚ñà‚ñà‚ñé       | 3/13 [00:21<01:05,  6.59s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2/13 | Discriminator loss: 1.8719 | Total Generator loss: nan | Reconstruction loss: nan |   - MSE: nan |   - Magnitude: nan |   - Phase: nan |   - Temporal: nan |   - Spectral: nan | Adversary Generator loss: -0.6505 | Disentanglement loss: 0.0041\n",
      "Norm of S: 19.5825\n",
      "Norm of C: 10.9957\n",
      "Norm of K: 5.6172\n",
      "Norm of L: 7.1132\n",
      "Norm of S: 19.5825\n",
      "Norm of C: 10.9957\n",
      "Norm of K: 5.6172\n",
      "Norm of L: 7.1132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:  31%|‚ñà‚ñà‚ñà       | 4/13 [00:26<00:53,  5.89s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3/13 | Discriminator loss: 1.5510 | Total Generator loss: nan | Reconstruction loss: nan |   - MSE: nan |   - Magnitude: nan |   - Phase: nan |   - Temporal: nan |   - Spectral: nan | Adversary Generator loss: -0.6140 | Disentanglement loss: 0.0064\n",
      "Norm of S: 19.6506\n",
      "Norm of C: 10.6715\n",
      "Norm of K: 5.6612\n",
      "Norm of L: 7.1873\n",
      "Norm of S: 19.6506\n",
      "Norm of C: 10.6715\n",
      "Norm of K: 5.6612\n",
      "Norm of L: 7.1873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:  38%|‚ñà‚ñà‚ñà‚ñä      | 5/13 [00:31<00:45,  5.63s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4/13 | Discriminator loss: 1.7785 | Total Generator loss: nan | Reconstruction loss: nan |   - MSE: nan |   - Magnitude: nan |   - Phase: nan |   - Temporal: nan |   - Spectral: nan | Adversary Generator loss: -0.6813 | Disentanglement loss: 0.0057\n",
      "Norm of S: 19.3497\n",
      "Norm of C: 10.0267\n",
      "Norm of K: 5.6401\n",
      "Norm of L: 7.2467\n",
      "Norm of S: 19.3497\n",
      "Norm of C: 10.0267\n",
      "Norm of K: 5.6401\n",
      "Norm of L: 7.2467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 6/13 [00:36<00:38,  5.47s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5/13 | Discriminator loss: 1.9700 | Total Generator loss: nan | Reconstruction loss: nan |   - MSE: nan |   - Magnitude: nan |   - Phase: nan |   - Temporal: nan |   - Spectral: nan | Adversary Generator loss: -0.6590 | Disentanglement loss: 0.0054\n",
      "Norm of S: 18.9831\n",
      "Norm of C: 10.1108\n",
      "Norm of K: 5.7365\n",
      "Norm of L: 7.2443\n",
      "Norm of S: 18.9831\n",
      "Norm of C: 10.1108\n",
      "Norm of K: 5.7365\n",
      "Norm of L: 7.2443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 7/13 [00:42<00:32,  5.41s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6/13 | Discriminator loss: 1.8440 | Total Generator loss: nan | Reconstruction loss: nan |   - MSE: nan |   - Magnitude: nan |   - Phase: nan |   - Temporal: nan |   - Spectral: nan | Adversary Generator loss: -0.6850 | Disentanglement loss: 0.0052\n",
      "Norm of S: 18.4729\n",
      "Norm of C: 10.2994\n",
      "Norm of K: 5.7524\n",
      "Norm of L: 7.1844\n",
      "Norm of S: 18.4729\n",
      "Norm of C: 10.2994\n",
      "Norm of K: 5.7524\n",
      "Norm of L: 7.1844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 7/13 [00:45<00:38,  6.44s/batch]\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-d369c471f9b5>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    103\u001b[0m             \u001b[1;31m# reconstruction loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m             \u001b[0mreconstructed_spec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent_emb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstyle_emb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstft_part\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# y=x per teacher forcing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m             \u001b[0mloss_recon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrecon_loss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreconstructed_spec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstft_part\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m             \u001b[1;31m# total generator loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-d369c471f9b5>\u001b[0m in \u001b[0;36mrecon_loss_fn\u001b[1;34m(output, target)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;31m# loss function for reconstruction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrecon_loss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         loss_dict = compute_comprehensive_loss(\n\u001b[0m\u001b[0;32m     39\u001b[0m             \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[0mlambda_temporal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mLAMBDA_TEMPORAL\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Francesco\\Desktop\\Audio-Style-Transfer\\new_decoder.py\u001b[0m in \u001b[0;36mcompute_comprehensive_loss\u001b[1;34m(output, target, lambda_temporal, lambda_phase, lambda_spectral, lambda_consistency)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m     \u001b[1;31m# ================== TEMPORAL CONSISTENCY LOSS ==================\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 371\u001b[1;33m     \u001b[0mtemporal_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    372\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mS\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m         \u001b[1;31m# Consistenza temporale dei frame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db7c5d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üìä ANALISI PARAMETRI ALLENABILI DEL MODELLO\n",
      "============================================================\n",
      "  Style Encoder: 12,905,312 parametri (12,905,312 allenabili) |   49.2 MB\n",
      "Content Encoder:  4,450,480 parametri ( 4,450,480 allenabili) |   17.0 MB\n",
      "        Decoder:  3,681,515 parametri ( 3,681,515 allenabili) |   14.0 MB\n",
      "  Discriminator:     49,666 parametri (    49,666 allenabili) |    0.2 MB\n",
      "------------------------------------------------------------\n",
      "         TOTALE: 21,086,973 parametri (21,086,973 allenabili)\n",
      "     DIMENSIONE:                                            80.4 MB\n",
      "   MEM. STIMATA:                                          322 MB (solo modello)\n",
      "\n",
      "============================================================\n",
      "üíæ ANALISI MEMORIA PER BATCH SIZE\n",
      "============================================================\n",
      "Batch size  4:    364 MB ‚úÖ\n",
      "Batch size  8:    405 MB ‚úÖ\n",
      "Batch size 16:    489 MB ‚úÖ\n",
      "Batch size 32:    656 MB ‚úÖ\n",
      "\n",
      "============================================================\n",
      "üéØ CONFIGURAZIONE RACCOMANDATA\n",
      "============================================================\n",
      "‚úÖ Modello di dimensioni ragionevoli per il training\n",
      "\n",
      "üöÄ Per GPU con 12GB: Batch size raccomandato = 8\n",
      "üîß Per GPU con 8GB:  Batch size raccomandato = 4\n"
     ]
    }
   ],
   "source": [
    "def count_trainable_parameters():\n",
    "    \"\"\"Conta i parametri allenabili di tutto il modello\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"üìä ANALISI PARAMETRI ALLENABILI DEL MODELLO\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Crea tutti i modelli\n",
    "    style_encoder = StyleEncoder(transformer_dim=TRANSFORMER_DIM)\n",
    "    content_encoder = ContentEncoder(transformer_dim=TRANSFORMER_DIM)\n",
    "    decoder = Decoder(\n",
    "        d_model=TRANSFORMER_DIM,\n",
    "        nhead=4,\n",
    "        num_layers=4,\n",
    "        dim_feedforward=TRANSFORMER_DIM * 2,\n",
    "        dropout=0.1\n",
    "    )\n",
    "    discriminator = Discriminator(input_dim=TRANSFORMER_DIM)\n",
    "    \n",
    "    models = [\n",
    "        (\"Style Encoder\", style_encoder),\n",
    "        (\"Content Encoder\", content_encoder),\n",
    "        (\"Decoder\", decoder),\n",
    "        (\"Discriminator\", discriminator)\n",
    "    ]\n",
    "    \n",
    "    total_params = 0\n",
    "    total_trainable_params = 0\n",
    "    \n",
    "    for name, model in models:\n",
    "        # Conta parametri totali\n",
    "        model_params = sum(p.numel() for p in model.parameters())\n",
    "        # Conta parametri allenabili\n",
    "        model_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        \n",
    "        total_params += model_params\n",
    "        total_trainable_params += model_trainable_params\n",
    "        \n",
    "        # Dimensione in MB\n",
    "        param_size_mb = sum(p.numel() * p.element_size() for p in model.parameters()) / (1024**2)\n",
    "        \n",
    "        print(f\"{name:>15}: {model_params:>10,} parametri ({model_trainable_params:>10,} allenabili) | {param_size_mb:>6.1f} MB\")\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'TOTALE':>15}: {total_params:>10,} parametri ({total_trainable_params:>10,} allenabili)\")\n",
    "    \n",
    "    # Calcola dimensioni totali\n",
    "    total_size_mb = 0\n",
    "    for name, model in models:\n",
    "        total_size_mb += sum(p.numel() * p.element_size() for p in model.parameters()) / (1024**2)\n",
    "    \n",
    "    print(f\"{'DIMENSIONE':>15}: {total_size_mb:>47.1f} MB\")\n",
    "    \n",
    "    # Stima memoria GPU per training\n",
    "    # Modello + gradients + optimizer states (Adam ~ 2x params) + attivazioni\n",
    "    estimated_gpu_memory = total_size_mb * 4  # rough estimate\n",
    "    print(f\"{'MEM. STIMATA':>15}: {estimated_gpu_memory:>44.0f} MB (solo modello)\")\n",
    "    \n",
    "    # Analisi per batch size\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üíæ ANALISI MEMORIA PER BATCH SIZE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    batch_sizes = [4, 8, 16, 32]\n",
    "    for bs in batch_sizes:\n",
    "        # Stima memoria per batch (approssimativa)\n",
    "        # Input: (B, S, 2, T, F) * 4 bytes per float32\n",
    "        input_size_mb = bs * NUM_FRAMES * 2 * STFT_T * (STFT_F + CQT_F) * 4 / (1024**2)\n",
    "        \n",
    "        # Memoria totale stimata\n",
    "        total_mem_mb = estimated_gpu_memory + input_size_mb * 2  # input + gradients\n",
    "        \n",
    "        status = \"‚úÖ\" if total_mem_mb < 8000 else \"‚ö†Ô∏è\" if total_mem_mb < 12000 else \"‚ùå\"\n",
    "        print(f\"Batch size {bs:>2}: {total_mem_mb:>6.0f} MB {status}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üéØ CONFIGURAZIONE RACCOMANDATA\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if total_trainable_params < 50_000_000:  # 50M\n",
    "        print(\"‚úÖ Modello di dimensioni ragionevoli per il training\")\n",
    "    elif total_trainable_params < 100_000_000:  # 100M\n",
    "        print(\"‚ö†Ô∏è Modello grande - considerare mixed precision training\")\n",
    "    else:\n",
    "        print(\"‚ùå Modello molto grande - ridurre dimensioni o usare tecniche avanzate\")\n",
    "    \n",
    "    # Raccomandazioni finali\n",
    "    print(f\"\\nüöÄ Per GPU con 12GB: Batch size raccomandato = {8 if estimated_gpu_memory < 4000 else 4}\")\n",
    "    print(f\"üîß Per GPU con 8GB:  Batch size raccomandato = {4 if estimated_gpu_memory < 3000 else 2}\")\n",
    "    \n",
    "    return total_trainable_params, total_size_mb\n",
    "\n",
    "# Esegui l'analisi\n",
    "total_params, model_size = count_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c212db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzioni di utilit√† per Colab\n",
    "import psutil\n",
    "import gc\n",
    "\n",
    "def monitor_resources():\n",
    "    \"\"\"Monitora le risorse di sistema\"\"\"\n",
    "    # CPU\n",
    "    cpu_percent = psutil.cpu_percent()\n",
    "    \n",
    "    # RAM\n",
    "    ram = psutil.virtual_memory()\n",
    "    ram_percent = ram.percent\n",
    "    \n",
    "    # GPU\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory = torch.cuda.memory_allocated() / 1024**2\n",
    "        gpu_cached = torch.cuda.memory_reserved() / 1024**2\n",
    "        \n",
    "        print(f\"CPU: {cpu_percent:.1f}% | RAM: {ram_percent:.1f}% | GPU: {gpu_memory:.0f}MB/{gpu_cached:.0f}MB\")\n",
    "        \n",
    "        # Allarme se troppa memoria\n",
    "        if gpu_memory > 8000:  # >8GB\n",
    "            print(\"‚ö†Ô∏è ATTENZIONE: Memoria GPU alta!\")\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"Libera memoria GPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        print(\"üßπ Memoria GPU liberata\")\n",
    "\n",
    "def get_model_size(model):\n",
    "    \"\"\"Calcola la dimensione del modello\"\"\"\n",
    "    param_count = sum(p.numel() for p in model.parameters())\n",
    "    param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "    buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())\n",
    "    size_mb = (param_size + buffer_size) / 1024**2\n",
    "    return param_count, size_mb\n",
    "\n",
    "# Test delle risorse\n",
    "print(\"=== RESOURCE MONITORING ===\")\n",
    "monitor_resources()\n",
    "\n",
    "# ‚úÖ Crea un decoder dinamico di test (senza parametro S!)\n",
    "test_decoder = DynamicDecoder(d_model=256, nhead=4, num_layers=3)\n",
    "param_count, size_mb = get_model_size(test_decoder)\n",
    "print(f\"Dynamic Decoder: {param_count:,} parametri ({size_mb:.1f} MB)\")\n",
    "\n",
    "del test_decoder  # Libera memoria\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d237fc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test del decoder dinamico prima del training\n",
    "def test_dynamic_decoder():\n",
    "    \"\"\"Test del decoder dinamico con sequenze di lunghezze diverse\"\"\"\n",
    "    print(\"üîß Testing Dynamic Decoder...\")\n",
    "    \n",
    "    # Parametri di test\n",
    "    B, d_model = 4, 256\n",
    "    \n",
    "    # ‚úÖ Crea decoder dinamico (SENZA parametro S!)\n",
    "    decoder = DynamicDecoder(d_model=d_model, nhead=4, num_layers=2)\n",
    "    \n",
    "    # Test con sequenze di lunghezze diverse\n",
    "    test_cases = [\n",
    "        (2, \"sequenza molto corta\"),\n",
    "        (4, \"sequenza standard\"), \n",
    "        (6, \"sequenza lunga\"),\n",
    "        (8, \"sequenza molto lunga\")\n",
    "    ]\n",
    "    \n",
    "    for S, description in test_cases:\n",
    "        print(f\"\\n--- Test: {description} (S={S}) ---\")\n",
    "        \n",
    "        # Input di test\n",
    "        content_emb = torch.randn(B, S, d_model)  # [B, S, d_model] - S dinamico!\n",
    "        class_emb = torch.randn(B, d_model)       # [B, d_model]\n",
    "        y_target = torch.randn(B, S, 2, 287, 513)  # [B, S, 2, 287, 513] - S dinamico!\n",
    "        \n",
    "        print(f\"Content embedding shape: {content_emb.shape}\")\n",
    "        print(f\"Class embedding shape: {class_emb.shape}\")\n",
    "        print(f\"Target shape: {y_target.shape}\")\n",
    "        \n",
    "        # Test training mode\n",
    "        decoder.train()\n",
    "        try:\n",
    "            output = decoder(content_emb, class_emb, y=y_target)\n",
    "            print(f\"‚úÖ Training mode output shape: {output.shape}\")\n",
    "            assert output.shape == y_target.shape, f\"Shape mismatch: {output.shape} vs {y_target.shape}\"\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Training mode error: {e}\")\n",
    "            return False\n",
    "        \n",
    "        # Test inference mode con target_length specificato\n",
    "        decoder.eval()\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                output = decoder(content_emb, class_emb, target_length=S)\n",
    "                print(f\"‚úÖ Inference mode output shape: {output.shape}\")\n",
    "                assert output.shape == (B, S, 2, 287, 513), f\"Inference shape mismatch: {output.shape}\"\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Inference mode error: {e}\")\n",
    "            return False\n",
    "        \n",
    "        # Test inference mode senza target_length (usa default)\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                output_auto = decoder(content_emb, class_emb)\n",
    "                print(f\"‚úÖ Auto-length inference output shape: {output_auto.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Auto-length inference error: {e}\")\n",
    "            return False\n",
    "        \n",
    "        # Test loss\n",
    "        try:\n",
    "            loss_dict = compute_comprehensive_loss(output, y_target)\n",
    "            print(f\"‚úÖ Loss computation successful: {loss_dict['total_loss'].item():.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Loss computation error: {e}\")\n",
    "            return False\n",
    "    \n",
    "    print(\"\\nüéâ Dynamic Decoder test successful! Decoder can handle variable sequence lengths!\")\n",
    "    return True\n",
    "\n",
    "# Esegui test\n",
    "print(\"Testing the new Dynamic Decoder...\")\n",
    "if test_dynamic_decoder():\n",
    "    print(\"‚úÖ Dynamic Decoder pronto per il training!\")\n",
    "else:\n",
    "    print(\"‚ùå Dynamic Decoder ha problemi, controlla gli errori.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1c6ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug utilities per il training dinamico\n",
    "def debug_shapes(x, content_emb, style_emb, class_emb, step=\"forward\"):\n",
    "    \"\"\"Debug delle dimensioni durante il training\"\"\"\n",
    "    print(f\"\\n=== DEBUG SHAPES - {step} ===\")\n",
    "    print(f\"Input x shape: {x.shape}\")\n",
    "    print(f\"Content embedding shape: {content_emb.shape}\")\n",
    "    print(f\"Style embedding shape: {style_emb.shape}\")\n",
    "    print(f\"Class embedding shape: {class_emb.shape}\")\n",
    "    \n",
    "    # Verifica dimensioni attese\n",
    "    B, S, C, H, W = x.shape\n",
    "    B_c, S_c, D_c = content_emb.shape\n",
    "    B_s, D_s = style_emb.shape\n",
    "    B_cl, D_cl = class_emb.shape\n",
    "    \n",
    "    print(f\"Batch size: {B}\")\n",
    "    print(f\"Sequence length: {S}\")\n",
    "    print(f\"STFT dims: {C}x{H}x{W}\")\n",
    "    \n",
    "    # Verifica coerenza\n",
    "    assert B == B_c == B_s == B_cl, f\"Batch size mismatch: {B}, {B_c}, {B_s}, {B_cl}\"\n",
    "    assert S == S_c, f\"Sequence length mismatch: {S}, {S_c}\"\n",
    "    assert D_c == D_s == D_cl, f\"Embedding dimension mismatch: {D_c}, {D_s}, {D_cl}\"\n",
    "    \n",
    "    print(\"‚úÖ All dimensions are consistent!\")\n",
    "    return True\n",
    "\n",
    "# Versione safe del training con debug del decoder dinamico\n",
    "def safe_debug_train():\n",
    "    \"\"\"Training con debug delle dimensioni per decoder dinamico\"\"\"\n",
    "    print(f\"Debug training on device: {device}\")\n",
    "    \n",
    "    # ‚úÖ Inizializza modelli con decoder dinamico\n",
    "    style_encoder = StyleEncoder(transformer_dim=TRANSFORMER_DIM).to(device)\n",
    "    content_encoder = ContentEncoder(transformer_dim=TRANSFORMER_DIM).to(device)\n",
    "    decoder = DynamicDecoder(d_model=TRANSFORMER_DIM, nhead=4, num_layers=2).to(device)  # Senza S!\n",
    "    \n",
    "    # Test con un batch\n",
    "    dataloader = get_dummy_dataloader(batch_size=2)  # Batch piccolo per debug\n",
    "    \n",
    "    print(\"\\nüîç Testing forward pass with Dynamic Decoder...\")\n",
    "    \n",
    "    for i, (x, labels) in enumerate(dataloader):\n",
    "        x, labels = x.to(device), labels.to(device)\n",
    "        \n",
    "        print(f\"\\nBatch {i+1} shapes:\")\n",
    "        print(f\"  x: {x.shape}\")\n",
    "        print(f\"  labels: {labels.shape}\")\n",
    "        \n",
    "        # Forward pass encoders\n",
    "        style_emb, class_emb = style_encoder(x, labels)\n",
    "        content_emb = content_encoder(x)\n",
    "        \n",
    "        # Debug shapes\n",
    "        debug_shapes(x, content_emb, style_emb, class_emb)\n",
    "        \n",
    "        # Test decoder dinamico\n",
    "        print(f\"\\nüß™ Testing Dynamic Decoder...\")\n",
    "        try:\n",
    "            # Training mode con teacher forcing\n",
    "            decoder.train()\n",
    "            reconstructed = decoder(content_emb, style_emb, y=x)\n",
    "            print(f\"‚úÖ Training mode output shape: {reconstructed.shape}\")\n",
    "            print(f\"Expected shape: {x.shape}\")\n",
    "            assert reconstructed.shape == x.shape, f\"Shape mismatch: {reconstructed.shape} vs {x.shape}\"\n",
    "            \n",
    "            # Inference mode\n",
    "            decoder.eval()\n",
    "            with torch.no_grad():\n",
    "                reconstructed_inf = decoder(content_emb, style_emb)\n",
    "                print(f\"‚úÖ Inference mode output shape: {reconstructed_inf.shape}\")\n",
    "            \n",
    "            # Test loss\n",
    "            loss_dict = compute_comprehensive_loss(reconstructed, x)\n",
    "            print(f\"‚úÖ Loss computation: {loss_dict['total_loss'].item():.4f}\")\n",
    "            \n",
    "            print(\"‚úÖ Dynamic Decoder test successful!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Dynamic Decoder error: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return False\n",
    "        \n",
    "        # Test solo il primo batch\n",
    "        break\n",
    "    \n",
    "    print(\"\\nüéâ Debug test completed successfully!\")\n",
    "    print(\"üöÄ Dynamic Decoder is ready for full training!\")\n",
    "    return True\n",
    "\n",
    "# Test delle dimensioni del modello dinamico\n",
    "def check_dynamic_model_size():\n",
    "    \"\"\"Controlla le dimensioni del modello dinamico\"\"\"\n",
    "    print(\"\\nüìä Checking Dynamic Model Sizes...\")\n",
    "    \n",
    "    # Crea modelli\n",
    "    style_encoder = StyleEncoder(transformer_dim=TRANSFORMER_DIM)\n",
    "    content_encoder = ContentEncoder(transformer_dim=TRANSFORMER_DIM)\n",
    "    decoder = DynamicDecoder(d_model=TRANSFORMER_DIM, nhead=4, num_layers=3)\n",
    "    discriminator = Discriminator(input_dim=TRANSFORMER_DIM)\n",
    "    \n",
    "    models = [\n",
    "        (\"Style Encoder\", style_encoder),\n",
    "        (\"Content Encoder\", content_encoder), \n",
    "        (\"Dynamic Decoder\", decoder),\n",
    "        (\"Discriminator\", discriminator)\n",
    "    ]\n",
    "    \n",
    "    total_params = 0\n",
    "    total_size = 0\n",
    "    \n",
    "    for name, model in models:\n",
    "        param_count, size_mb = get_model_size(model)\n",
    "        total_params += param_count\n",
    "        total_size += size_mb\n",
    "        print(f\"{name}: {param_count:,} parametri ({size_mb:.1f} MB)\")\n",
    "    \n",
    "    print(f\"\\nüìà TOTALE: {total_params:,} parametri ({total_size:.1f} MB)\")\n",
    "    \n",
    "    # Stima memoria per batch_size=8\n",
    "    estimated_memory = total_size * 3 + (8 * 4 * 2 * 287 * 513 * 4) / (1024**2)  # modello + gradients + data\n",
    "    print(f\"üíæ Memoria stimata (batch_size=8): ~{estimated_memory:.0f} MB\")\n",
    "    print(f\"üñ•Ô∏è Using device: {device}\")\n",
    "    \n",
    "    if estimated_memory < 10000:  # <10GB\n",
    "        print(\"‚úÖ Dovrebbe funzionare su Colab Free!\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Potrebbe essere troppo per Colab Free\")\n",
    "\n",
    "# Esegui controlli\n",
    "check_dynamic_model_size()\n",
    "\n",
    "# Uncomment to run debug test\n",
    "# safe_debug_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d6b367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training con monitoraggio delle risorse\n",
    "def safe_train():\n",
    "    \"\"\"Training con monitoraggio delle risorse per Colab\"\"\"\n",
    "    try:\n",
    "        print(f\"üöÄ Avvio training con monitoraggio risorse su {device}...\")\n",
    "        monitor_resources()\n",
    "        \n",
    "        # Mostra info GPU se disponibile\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"üíæ GPU Memory before training: {torch.cuda.memory_allocated()/1024**2:.1f}MB / {torch.cuda.memory_reserved()/1024**2:.1f}MB\")\n",
    "        \n",
    "        # Avvia il training\n",
    "        train()\n",
    "        \n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e):\n",
    "            print(\"üí• ERRORE: Memoria GPU insufficiente!\")\n",
    "            print(\"Prova a ridurre:\")\n",
    "            print(\"- BATCH_SIZE (attualmente {})\".format(BATCH_SIZE))\n",
    "            print(\"- NUM_FRAMES (attualmente {})\".format(NUM_FRAMES))\n",
    "            print(\"- TRANSFORMER_DIM (attualmente {})\".format(TRANSFORMER_DIM))\n",
    "            clear_memory()\n",
    "        else:\n",
    "            print(f\"‚ùå Errore durante il training: {e}\")\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"‚èπÔ∏è Training interrotto dall'utente\")\n",
    "        clear_memory()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Errore imprevisto: {e}\")\n",
    "        clear_memory()\n",
    "    finally:\n",
    "        print(f\"üèÅ Training terminato su {device}\")\n",
    "        monitor_resources()\n",
    "\n",
    "# Uncomment to run training\n",
    "# safe_train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
