{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "205dcdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# models\n",
    "from style_encoder import StyleEncoder, initialize_weights\n",
    "from content_encoder import ContentEncoder\n",
    "from discriminator import Discriminator\n",
    "from new_decoder import Decoder, compute_comprehensive_loss  # Nuovo decoder dinamico\n",
    "from losses import infoNCE_loss, margin_loss, adversarial_loss, disentanglement_loss\n",
    "from dummy_dataloader import get_dummy_dataloader\n",
    "\n",
    "# hyperparameters\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 8\n",
    "LR_GEN = 1e-4  # Learning rate per Encoders + Decoder\n",
    "LR_DISC = 1e-4  # Learning rate per Discriminator\n",
    "TRANSFORMER_DIM = 256\n",
    "NUM_FRAMES = 4 # S (ora solo per riferimento, il decoder è dinamico)\n",
    "STFT_T, STFT_F = 287, 513 # Dimensioni STFT (assumendo n_fft=1024)\n",
    "CQT_T, CQT_F = 287, 84   # Dimensioni CQT\n",
    "\n",
    "# loss weights\n",
    "LAMBDA_RECON = 10.0\n",
    "LAMBDA_INFO_NCE = 1.0\n",
    "LAMBDA_MARGIN = 1.0\n",
    "LAMBDA_DISENTANGLE = 0.1\n",
    "LAMBDA_ADV_GEN = 0.5 # Peso per la loss avversaria del generatore\n",
    "\n",
    "# Pesi per le loss comprehensive del decoder\n",
    "LAMBDA_TEMPORAL = 0.3\n",
    "LAMBDA_PHASE = 0.2\n",
    "LAMBDA_SPECTRAL = 0.1\n",
    "LAMBDA_CONSISTENCY = 0.1\n",
    "\n",
    "MODEL_SAVE_PATH = \"./saved_models\"\n",
    "os.makedirs(MODEL_SAVE_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b43ed26",
   "metadata": {},
   "source": [
    "aggiungere data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09e7c204",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # load models\n",
    "    style_encoder = StyleEncoder(transformer_dim=TRANSFORMER_DIM).to(device)\n",
    "    content_encoder = ContentEncoder(transformer_dim=TRANSFORMER_DIM).to(device)\n",
    "\n",
    "    # decoder\n",
    "    decoder = Decoder(\n",
    "        d_model=TRANSFORMER_DIM,\n",
    "        nhead=4,\n",
    "        num_layers=3,  # Ridotto per Colab\n",
    "        dim_feedforward=TRANSFORMER_DIM * 2,\n",
    "        dropout=0.1\n",
    "    ).to(device)\n",
    "    \n",
    "    discriminator = Discriminator(input_dim=TRANSFORMER_DIM).to(device)\n",
    "\n",
    "    # initialize weights\n",
    "    initialize_weights(style_encoder)\n",
    "    initialize_weights(content_encoder)\n",
    "    initialize_weights(discriminator)\n",
    "    # decoder già ha la sua inizializzazione\n",
    "\n",
    "    # optimizer for generators (style encoder, content encoder, decoder)\n",
    "    optimizer_G = optim.Adam(\n",
    "        list(style_encoder.parameters()) + list(content_encoder.parameters()) + list(decoder.parameters()),\n",
    "        lr=LR_GEN, betas=(0.5, 0.999)\n",
    "    )\n",
    "    \n",
    "    # optimizer for discriminator\n",
    "    optimizer_D = optim.Adam(discriminator.parameters(), lr=LR_DISC, betas=(0.5, 0.999))\n",
    "\n",
    "    dataloader = get_dummy_dataloader(batch_size=BATCH_SIZE)\n",
    "    \n",
    "    # loss function for reconstruction\n",
    "    def recon_loss_fn(output, target):\n",
    "        loss_dict = compute_comprehensive_loss(\n",
    "            output, target, \n",
    "            lambda_temporal=LAMBDA_TEMPORAL,\n",
    "            lambda_phase=LAMBDA_PHASE,\n",
    "            lambda_spectral=LAMBDA_SPECTRAL,\n",
    "            lambda_consistency=LAMBDA_CONSISTENCY\n",
    "        )\n",
    "        return loss_dict['total_loss'], loss_dict\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "\n",
    "    # train loop\n",
    "    for epoch in range(EPOCHS):\n",
    "        \n",
    "        style_encoder.train()\n",
    "        content_encoder.train()\n",
    "        decoder.train()\n",
    "        discriminator.train()\n",
    "        \n",
    "        for i, (x, labels) in enumerate(tqdm(dataloader, unit=\"batch\", desc=f\"Epoch {epoch+1}/{EPOCHS}\")):\n",
    "            x, labels = x.to(device), labels.to(device) # x: (B, S, 2, T, F)\n",
    "\n",
    "            # ================================================================== #\n",
    "            #                             Discriminator                          #\n",
    "            # ================================================================== #\n",
    "            optimizer_D.zero_grad()\n",
    "            \n",
    "            # compute embeddings with no_grad to avoid backpropagation through the encoders\n",
    "            with torch.no_grad():\n",
    "                style_emb, class_emb = style_encoder(x, labels)\n",
    "                content_emb = content_encoder(x)\n",
    "            \n",
    "            # adversarial loss for the discriminator\n",
    "            discriminator_loss, _ = adversarial_loss(style_emb.detach(), class_emb.detach(), \n",
    "                                                     content_emb.detach(), discriminator, labels, \n",
    "                                                     compute_for_discriminator=True)\n",
    "            \n",
    "            discriminator_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "\n",
    "            # ================================================================== #\n",
    "            #               Generators (Style Encoder, Content Encoder)          #\n",
    "            # ================================================================== #\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            # forward pass\n",
    "            style_emb, class_emb = style_encoder(x, labels)\n",
    "            content_emb = content_encoder(x)\n",
    "\n",
    "            # adversarial loss for the generator\n",
    "            _, adv_generator_loss = adversarial_loss(style_emb, class_emb, content_emb, discriminator, labels,\n",
    "                                                 compute_for_discriminator=False)\n",
    "\n",
    "            # disentanglement loss\n",
    "            disent_loss = disentanglement_loss(style_emb, content_emb.mean(dim=1), use_hsic=True)\n",
    "\n",
    "            if len(torch.unique(labels)) > 1:\n",
    "                # contrastive losses\n",
    "                loss_infonce = infoNCE_loss(style_emb, labels)\n",
    "                loss_margin = margin_loss(class_emb)\n",
    "            else:\n",
    "                raise ValueError(\"Labels must contain at least two unique classes for contrastive losses.\")\n",
    "\n",
    "            # reconstruction loss\n",
    "            reconstructed_spec = decoder(content_emb, style_emb, y=x)  # y=x per teacher forcing\n",
    "            loss_recon, loss_dict = recon_loss_fn(reconstructed_spec, x)\n",
    "            \n",
    "            # total generator loss\n",
    "            total_gen_loss = (\n",
    "                LAMBDA_RECON * loss_recon +\n",
    "                LAMBDA_INFO_NCE * loss_infonce +\n",
    "                LAMBDA_MARGIN * loss_margin +\n",
    "                LAMBDA_DISENTANGLE * disent_loss +\n",
    "                LAMBDA_ADV_GEN * adv_generator_loss\n",
    "            )\n",
    "\n",
    "            total_gen_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            # loss logging \n",
    "            if i % 1 == 0:\n",
    "                # Converte tensor scalari a float per evitare errori\n",
    "                temporal_loss_val = loss_dict['temporal_loss'].item() if loss_dict['temporal_loss'].numel() > 0 else 0.0\n",
    "                spectral_loss_val = loss_dict['spectral_loss'].item() if loss_dict['spectral_loss'].numel() > 0 else 0.0\n",
    "                \n",
    "                tqdm.write(\n",
    "                    f\"Batch {i}/{len(dataloader)} | \"\n",
    "                    f\"Discriminator loss: {discriminator_loss.item():.4f} | \"\n",
    "                    f\"Total Generator loss: {total_gen_loss.item():.4f} | \"\n",
    "                    f\"Reconstruction loss: {loss_recon.item():.4f} | \"\n",
    "                    f\"  - MSE: {loss_dict['mse_loss'].item():.4f} | \"\n",
    "                    f\"  - Magnitude: {loss_dict['mag_loss'].item():.4f} | \"\n",
    "                    f\"  - Phase: {loss_dict['phase_loss'].item():.4f} | \"\n",
    "                    f\"  - Temporal: {temporal_loss_val:.4f} | \"\n",
    "                    f\"  - Spectral: {spectral_loss_val:.4f} | \"\n",
    "                    f\"Adversary Generator loss: {adv_generator_loss.item():.4f} | \"\n",
    "                    f\"Disentanglement loss: {disent_loss.item():.4f}\"\n",
    "                )\n",
    "                \n",
    "        # saving best model\n",
    "        current_recon_loss = loss_recon.item()\n",
    "        if current_recon_loss < best_loss:\n",
    "            best_loss = current_recon_loss\n",
    "            print(f\"\\nNew best reconstruction loss: {best_loss:.4f}. Saving model...\")\n",
    "            \n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'style_encoder_state_dict': style_encoder.state_dict(),\n",
    "                'content_encoder_state_dict': content_encoder.state_dict(),\n",
    "                'decoder_state_dict': decoder.state_dict(),\n",
    "                'discriminator_state_dict': discriminator.state_dict(),\n",
    "                'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
    "                'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
    "                'best_loss': best_loss,\n",
    "            }, os.path.join(MODEL_SAVE_PATH, 'best_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df6100b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   0%|          | 0/13 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   0%|          | 0/13 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug shapes - Full x: torch.Size([8, 4, 2, 287, 597]), STFT: torch.Size([8, 4, 2, 287, 513]), CQT: torch.Size([8, 4, 2, 287, 84])\n",
      "Norm of S: 20.5219\n",
      "Norm of C: 11.4853\n",
      "Norm of K: 5.7190\n",
      "Norm of L: 7.1617\n",
      "Norm of S: 20.5219\n",
      "Norm of C: 11.4853\n",
      "Norm of K: 5.7190\n",
      "Norm of L: 7.1617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   0%|          | 0/13 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug shapes - Full x: torch.Size([8, 4, 2, 287, 597]), STFT: torch.Size([8, 4, 2, 287, 513]), CQT: torch.Size([8, 4, 2, 287, 84])\n",
      "Norm of S: 20.5219\n",
      "Norm of C: 11.4853\n",
      "Norm of K: 5.7190\n",
      "Norm of L: 7.1617\n",
      "Norm of S: 20.5219\n",
      "Norm of C: 11.4853\n",
      "Norm of K: 5.7190\n",
      "Norm of L: 7.1617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   8%|▊         | 1/13 [00:19<03:55, 19.63s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   0%|          | 0/13 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug shapes - Full x: torch.Size([8, 4, 2, 287, 597]), STFT: torch.Size([8, 4, 2, 287, 513]), CQT: torch.Size([8, 4, 2, 287, 84])\n",
      "Norm of S: 20.5219\n",
      "Norm of C: 11.4853\n",
      "Norm of K: 5.7190\n",
      "Norm of L: 7.1617\n",
      "Norm of S: 20.5219\n",
      "Norm of C: 11.4853\n",
      "Norm of K: 5.7190\n",
      "Norm of L: 7.1617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   8%|▊         | 1/13 [00:19<03:55, 19.63s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0/13 | Discriminator loss: 1.9071 | Total Generator loss: 36.1859 | Reconstruction loss: 3.4575 |   - MSE: 0.9999 |   - Magnitude: 1.9996 |   - Phase: 3.2891 |   - Temporal: 2.0000 |   - Spectral: 1.9995 | Adversary Generator loss: -0.6908 | Disentanglement loss: 0.0058\n",
      "Debug shapes - Full x: torch.Size([8, 4, 2, 287, 597]), STFT: torch.Size([8, 4, 2, 287, 513]), CQT: torch.Size([8, 4, 2, 287, 84])\n",
      "Norm of S: 20.4255\n",
      "Norm of C: 11.5970\n",
      "Norm of K: 5.6970\n",
      "Norm of L: 7.1310\n",
      "Norm of S: 20.4255\n",
      "Norm of C: 11.5970\n",
      "Norm of K: 5.6970\n",
      "Norm of L: 7.1310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   0%|          | 0/13 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug shapes - Full x: torch.Size([8, 4, 2, 287, 597]), STFT: torch.Size([8, 4, 2, 287, 513]), CQT: torch.Size([8, 4, 2, 287, 84])\n",
      "Norm of S: 20.5219\n",
      "Norm of C: 11.4853\n",
      "Norm of K: 5.7190\n",
      "Norm of L: 7.1617\n",
      "Norm of S: 20.5219\n",
      "Norm of C: 11.4853\n",
      "Norm of K: 5.7190\n",
      "Norm of L: 7.1617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   8%|▊         | 1/13 [00:19<03:55, 19.63s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0/13 | Discriminator loss: 1.9071 | Total Generator loss: 36.1859 | Reconstruction loss: 3.4575 |   - MSE: 0.9999 |   - Magnitude: 1.9996 |   - Phase: 3.2891 |   - Temporal: 2.0000 |   - Spectral: 1.9995 | Adversary Generator loss: -0.6908 | Disentanglement loss: 0.0058\n",
      "Debug shapes - Full x: torch.Size([8, 4, 2, 287, 597]), STFT: torch.Size([8, 4, 2, 287, 513]), CQT: torch.Size([8, 4, 2, 287, 84])\n",
      "Norm of S: 20.4255\n",
      "Norm of C: 11.5970\n",
      "Norm of K: 5.6970\n",
      "Norm of L: 7.1310\n",
      "Norm of S: 20.4255\n",
      "Norm of C: 11.5970\n",
      "Norm of K: 5.6970\n",
      "Norm of L: 7.1310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:  15%|█▌        | 2/13 [00:36<03:17, 17.92s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   0%|          | 0/13 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug shapes - Full x: torch.Size([8, 4, 2, 287, 597]), STFT: torch.Size([8, 4, 2, 287, 513]), CQT: torch.Size([8, 4, 2, 287, 84])\n",
      "Norm of S: 20.5219\n",
      "Norm of C: 11.4853\n",
      "Norm of K: 5.7190\n",
      "Norm of L: 7.1617\n",
      "Norm of S: 20.5219\n",
      "Norm of C: 11.4853\n",
      "Norm of K: 5.7190\n",
      "Norm of L: 7.1617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   8%|▊         | 1/13 [00:19<03:55, 19.63s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0/13 | Discriminator loss: 1.9071 | Total Generator loss: 36.1859 | Reconstruction loss: 3.4575 |   - MSE: 0.9999 |   - Magnitude: 1.9996 |   - Phase: 3.2891 |   - Temporal: 2.0000 |   - Spectral: 1.9995 | Adversary Generator loss: -0.6908 | Disentanglement loss: 0.0058\n",
      "Debug shapes - Full x: torch.Size([8, 4, 2, 287, 597]), STFT: torch.Size([8, 4, 2, 287, 513]), CQT: torch.Size([8, 4, 2, 287, 84])\n",
      "Norm of S: 20.4255\n",
      "Norm of C: 11.5970\n",
      "Norm of K: 5.6970\n",
      "Norm of L: 7.1310\n",
      "Norm of S: 20.4255\n",
      "Norm of C: 11.5970\n",
      "Norm of K: 5.6970\n",
      "Norm of L: 7.1310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:  15%|█▌        | 2/13 [00:36<03:17, 17.92s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/13 | Discriminator loss: 1.9018 | Total Generator loss: nan | Reconstruction loss: nan |   - MSE: nan |   - Magnitude: nan |   - Phase: nan |   - Temporal: nan |   - Spectral: nan | Adversary Generator loss: -0.3888 | Disentanglement loss: 0.0061\n",
      "Debug shapes - Full x: torch.Size([8, 4, 2, 287, 597]), STFT: torch.Size([8, 4, 2, 287, 513]), CQT: torch.Size([8, 4, 2, 287, 84])\n",
      "Norm of S: 20.2762\n",
      "Norm of C: 11.1437\n",
      "Norm of K: 5.7873\n",
      "Norm of L: 7.2178\n",
      "Norm of S: 20.2762\n",
      "Norm of C: 11.1437\n",
      "Norm of K: 5.7873\n",
      "Norm of L: 7.2178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   0%|          | 0/13 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug shapes - Full x: torch.Size([8, 4, 2, 287, 597]), STFT: torch.Size([8, 4, 2, 287, 513]), CQT: torch.Size([8, 4, 2, 287, 84])\n",
      "Norm of S: 20.5219\n",
      "Norm of C: 11.4853\n",
      "Norm of K: 5.7190\n",
      "Norm of L: 7.1617\n",
      "Norm of S: 20.5219\n",
      "Norm of C: 11.4853\n",
      "Norm of K: 5.7190\n",
      "Norm of L: 7.1617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   8%|▊         | 1/13 [00:19<03:55, 19.63s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0/13 | Discriminator loss: 1.9071 | Total Generator loss: 36.1859 | Reconstruction loss: 3.4575 |   - MSE: 0.9999 |   - Magnitude: 1.9996 |   - Phase: 3.2891 |   - Temporal: 2.0000 |   - Spectral: 1.9995 | Adversary Generator loss: -0.6908 | Disentanglement loss: 0.0058\n",
      "Debug shapes - Full x: torch.Size([8, 4, 2, 287, 597]), STFT: torch.Size([8, 4, 2, 287, 513]), CQT: torch.Size([8, 4, 2, 287, 84])\n",
      "Norm of S: 20.4255\n",
      "Norm of C: 11.5970\n",
      "Norm of K: 5.6970\n",
      "Norm of L: 7.1310\n",
      "Norm of S: 20.4255\n",
      "Norm of C: 11.5970\n",
      "Norm of K: 5.6970\n",
      "Norm of L: 7.1310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:  15%|█▌        | 2/13 [00:36<03:17, 17.92s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/13 | Discriminator loss: 1.9018 | Total Generator loss: nan | Reconstruction loss: nan |   - MSE: nan |   - Magnitude: nan |   - Phase: nan |   - Temporal: nan |   - Spectral: nan | Adversary Generator loss: -0.3888 | Disentanglement loss: 0.0061\n",
      "Debug shapes - Full x: torch.Size([8, 4, 2, 287, 597]), STFT: torch.Size([8, 4, 2, 287, 513]), CQT: torch.Size([8, 4, 2, 287, 84])\n",
      "Norm of S: 20.2762\n",
      "Norm of C: 11.1437\n",
      "Norm of K: 5.7873\n",
      "Norm of L: 7.2178\n",
      "Norm of S: 20.2762\n",
      "Norm of C: 11.1437\n",
      "Norm of K: 5.7873\n",
      "Norm of L: 7.2178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:  23%|██▎       | 3/13 [00:53<02:55, 17.50s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   0%|          | 0/13 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug shapes - Full x: torch.Size([8, 4, 2, 287, 597]), STFT: torch.Size([8, 4, 2, 287, 513]), CQT: torch.Size([8, 4, 2, 287, 84])\n",
      "Norm of S: 20.5219\n",
      "Norm of C: 11.4853\n",
      "Norm of K: 5.7190\n",
      "Norm of L: 7.1617\n",
      "Norm of S: 20.5219\n",
      "Norm of C: 11.4853\n",
      "Norm of K: 5.7190\n",
      "Norm of L: 7.1617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   8%|▊         | 1/13 [00:19<03:55, 19.63s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0/13 | Discriminator loss: 1.9071 | Total Generator loss: 36.1859 | Reconstruction loss: 3.4575 |   - MSE: 0.9999 |   - Magnitude: 1.9996 |   - Phase: 3.2891 |   - Temporal: 2.0000 |   - Spectral: 1.9995 | Adversary Generator loss: -0.6908 | Disentanglement loss: 0.0058\n",
      "Debug shapes - Full x: torch.Size([8, 4, 2, 287, 597]), STFT: torch.Size([8, 4, 2, 287, 513]), CQT: torch.Size([8, 4, 2, 287, 84])\n",
      "Norm of S: 20.4255\n",
      "Norm of C: 11.5970\n",
      "Norm of K: 5.6970\n",
      "Norm of L: 7.1310\n",
      "Norm of S: 20.4255\n",
      "Norm of C: 11.5970\n",
      "Norm of K: 5.6970\n",
      "Norm of L: 7.1310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:  15%|█▌        | 2/13 [00:36<03:17, 17.92s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/13 | Discriminator loss: 1.9018 | Total Generator loss: nan | Reconstruction loss: nan |   - MSE: nan |   - Magnitude: nan |   - Phase: nan |   - Temporal: nan |   - Spectral: nan | Adversary Generator loss: -0.3888 | Disentanglement loss: 0.0061\n",
      "Debug shapes - Full x: torch.Size([8, 4, 2, 287, 597]), STFT: torch.Size([8, 4, 2, 287, 513]), CQT: torch.Size([8, 4, 2, 287, 84])\n",
      "Norm of S: 20.2762\n",
      "Norm of C: 11.1437\n",
      "Norm of K: 5.7873\n",
      "Norm of L: 7.2178\n",
      "Norm of S: 20.2762\n",
      "Norm of C: 11.1437\n",
      "Norm of K: 5.7873\n",
      "Norm of L: 7.2178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:  23%|██▎       | 3/13 [00:53<02:55, 17.50s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2/13 | Discriminator loss: 2.0765 | Total Generator loss: nan | Reconstruction loss: nan |   - MSE: nan |   - Magnitude: nan |   - Phase: nan |   - Temporal: nan |   - Spectral: nan | Adversary Generator loss: -0.6728 | Disentanglement loss: 0.0052\n",
      "Debug shapes - Full x: torch.Size([8, 4, 2, 287, 597]), STFT: torch.Size([8, 4, 2, 287, 513]), CQT: torch.Size([8, 4, 2, 287, 84])\n",
      "Norm of S: 19.6433\n",
      "Norm of C: 11.2639\n",
      "Norm of K: 5.8253\n",
      "Norm of L: 7.1717\n",
      "Norm of S: 19.6433\n",
      "Norm of C: 11.2639\n",
      "Norm of K: 5.8253\n",
      "Norm of L: 7.1717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   0%|          | 0/13 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug shapes - Full x: torch.Size([8, 4, 2, 287, 597]), STFT: torch.Size([8, 4, 2, 287, 513]), CQT: torch.Size([8, 4, 2, 287, 84])\n",
      "Norm of S: 20.5219\n",
      "Norm of C: 11.4853\n",
      "Norm of K: 5.7190\n",
      "Norm of L: 7.1617\n",
      "Norm of S: 20.5219\n",
      "Norm of C: 11.4853\n",
      "Norm of K: 5.7190\n",
      "Norm of L: 7.1617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   8%|▊         | 1/13 [00:19<03:55, 19.63s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0/13 | Discriminator loss: 1.9071 | Total Generator loss: 36.1859 | Reconstruction loss: 3.4575 |   - MSE: 0.9999 |   - Magnitude: 1.9996 |   - Phase: 3.2891 |   - Temporal: 2.0000 |   - Spectral: 1.9995 | Adversary Generator loss: -0.6908 | Disentanglement loss: 0.0058\n",
      "Debug shapes - Full x: torch.Size([8, 4, 2, 287, 597]), STFT: torch.Size([8, 4, 2, 287, 513]), CQT: torch.Size([8, 4, 2, 287, 84])\n",
      "Norm of S: 20.4255\n",
      "Norm of C: 11.5970\n",
      "Norm of K: 5.6970\n",
      "Norm of L: 7.1310\n",
      "Norm of S: 20.4255\n",
      "Norm of C: 11.5970\n",
      "Norm of K: 5.6970\n",
      "Norm of L: 7.1310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:  15%|█▌        | 2/13 [00:36<03:17, 17.92s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/13 | Discriminator loss: 1.9018 | Total Generator loss: nan | Reconstruction loss: nan |   - MSE: nan |   - Magnitude: nan |   - Phase: nan |   - Temporal: nan |   - Spectral: nan | Adversary Generator loss: -0.3888 | Disentanglement loss: 0.0061\n",
      "Debug shapes - Full x: torch.Size([8, 4, 2, 287, 597]), STFT: torch.Size([8, 4, 2, 287, 513]), CQT: torch.Size([8, 4, 2, 287, 84])\n",
      "Norm of S: 20.2762\n",
      "Norm of C: 11.1437\n",
      "Norm of K: 5.7873\n",
      "Norm of L: 7.2178\n",
      "Norm of S: 20.2762\n",
      "Norm of C: 11.1437\n",
      "Norm of K: 5.7873\n",
      "Norm of L: 7.2178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:  23%|██▎       | 3/13 [00:53<02:55, 17.50s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2/13 | Discriminator loss: 2.0765 | Total Generator loss: nan | Reconstruction loss: nan |   - MSE: nan |   - Magnitude: nan |   - Phase: nan |   - Temporal: nan |   - Spectral: nan | Adversary Generator loss: -0.6728 | Disentanglement loss: 0.0052\n",
      "Debug shapes - Full x: torch.Size([8, 4, 2, 287, 597]), STFT: torch.Size([8, 4, 2, 287, 513]), CQT: torch.Size([8, 4, 2, 287, 84])\n",
      "Norm of S: 19.6433\n",
      "Norm of C: 11.2639\n",
      "Norm of K: 5.8253\n",
      "Norm of L: 7.1717\n",
      "Norm of S: 19.6433\n",
      "Norm of C: 11.2639\n",
      "Norm of K: 5.8253\n",
      "Norm of L: 7.1717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:  31%|███       | 4/13 [01:09<02:33, 17.04s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   0%|          | 0/13 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug shapes - Full x: torch.Size([8, 4, 2, 287, 597]), STFT: torch.Size([8, 4, 2, 287, 513]), CQT: torch.Size([8, 4, 2, 287, 84])\n",
      "Norm of S: 20.5219\n",
      "Norm of C: 11.4853\n",
      "Norm of K: 5.7190\n",
      "Norm of L: 7.1617\n",
      "Norm of S: 20.5219\n",
      "Norm of C: 11.4853\n",
      "Norm of K: 5.7190\n",
      "Norm of L: 7.1617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   8%|▊         | 1/13 [00:19<03:55, 19.63s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0/13 | Discriminator loss: 1.9071 | Total Generator loss: 36.1859 | Reconstruction loss: 3.4575 |   - MSE: 0.9999 |   - Magnitude: 1.9996 |   - Phase: 3.2891 |   - Temporal: 2.0000 |   - Spectral: 1.9995 | Adversary Generator loss: -0.6908 | Disentanglement loss: 0.0058\n",
      "Debug shapes - Full x: torch.Size([8, 4, 2, 287, 597]), STFT: torch.Size([8, 4, 2, 287, 513]), CQT: torch.Size([8, 4, 2, 287, 84])\n",
      "Norm of S: 20.4255\n",
      "Norm of C: 11.5970\n",
      "Norm of K: 5.6970\n",
      "Norm of L: 7.1310\n",
      "Norm of S: 20.4255\n",
      "Norm of C: 11.5970\n",
      "Norm of K: 5.6970\n",
      "Norm of L: 7.1310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:  15%|█▌        | 2/13 [00:36<03:17, 17.92s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/13 | Discriminator loss: 1.9018 | Total Generator loss: nan | Reconstruction loss: nan |   - MSE: nan |   - Magnitude: nan |   - Phase: nan |   - Temporal: nan |   - Spectral: nan | Adversary Generator loss: -0.3888 | Disentanglement loss: 0.0061\n",
      "Debug shapes - Full x: torch.Size([8, 4, 2, 287, 597]), STFT: torch.Size([8, 4, 2, 287, 513]), CQT: torch.Size([8, 4, 2, 287, 84])\n",
      "Norm of S: 20.2762\n",
      "Norm of C: 11.1437\n",
      "Norm of K: 5.7873\n",
      "Norm of L: 7.2178\n",
      "Norm of S: 20.2762\n",
      "Norm of C: 11.1437\n",
      "Norm of K: 5.7873\n",
      "Norm of L: 7.2178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:  23%|██▎       | 3/13 [00:53<02:55, 17.50s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2/13 | Discriminator loss: 2.0765 | Total Generator loss: nan | Reconstruction loss: nan |   - MSE: nan |   - Magnitude: nan |   - Phase: nan |   - Temporal: nan |   - Spectral: nan | Adversary Generator loss: -0.6728 | Disentanglement loss: 0.0052\n",
      "Debug shapes - Full x: torch.Size([8, 4, 2, 287, 597]), STFT: torch.Size([8, 4, 2, 287, 513]), CQT: torch.Size([8, 4, 2, 287, 84])\n",
      "Norm of S: 19.6433\n",
      "Norm of C: 11.2639\n",
      "Norm of K: 5.8253\n",
      "Norm of L: 7.1717\n",
      "Norm of S: 19.6433\n",
      "Norm of C: 11.2639\n",
      "Norm of K: 5.8253\n",
      "Norm of L: 7.1717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:  31%|███       | 4/13 [01:09<02:33, 17.04s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3/13 | Discriminator loss: 2.3182 | Total Generator loss: nan | Reconstruction loss: nan |   - MSE: nan |   - Magnitude: nan |   - Phase: nan |   - Temporal: nan |   - Spectral: nan | Adversary Generator loss: -0.6894 | Disentanglement loss: 0.0054\n",
      "Debug shapes - Full x: torch.Size([8, 4, 2, 287, 597]), STFT: torch.Size([8, 4, 2, 287, 513]), CQT: torch.Size([8, 4, 2, 287, 84])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   0%|          | 0/13 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug shapes - Full x: torch.Size([8, 4, 2, 287, 597]), STFT: torch.Size([8, 4, 2, 287, 513]), CQT: torch.Size([8, 4, 2, 287, 84])\n",
      "Norm of S: 20.5219\n",
      "Norm of C: 11.4853\n",
      "Norm of K: 5.7190\n",
      "Norm of L: 7.1617\n",
      "Norm of S: 20.5219\n",
      "Norm of C: 11.4853\n",
      "Norm of K: 5.7190\n",
      "Norm of L: 7.1617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   8%|▊         | 1/13 [00:19<03:55, 19.63s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0/13 | Discriminator loss: 1.9071 | Total Generator loss: 36.1859 | Reconstruction loss: 3.4575 |   - MSE: 0.9999 |   - Magnitude: 1.9996 |   - Phase: 3.2891 |   - Temporal: 2.0000 |   - Spectral: 1.9995 | Adversary Generator loss: -0.6908 | Disentanglement loss: 0.0058\n",
      "Debug shapes - Full x: torch.Size([8, 4, 2, 287, 597]), STFT: torch.Size([8, 4, 2, 287, 513]), CQT: torch.Size([8, 4, 2, 287, 84])\n",
      "Norm of S: 20.4255\n",
      "Norm of C: 11.5970\n",
      "Norm of K: 5.6970\n",
      "Norm of L: 7.1310\n",
      "Norm of S: 20.4255\n",
      "Norm of C: 11.5970\n",
      "Norm of K: 5.6970\n",
      "Norm of L: 7.1310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:  15%|█▌        | 2/13 [00:36<03:17, 17.92s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/13 | Discriminator loss: 1.9018 | Total Generator loss: nan | Reconstruction loss: nan |   - MSE: nan |   - Magnitude: nan |   - Phase: nan |   - Temporal: nan |   - Spectral: nan | Adversary Generator loss: -0.3888 | Disentanglement loss: 0.0061\n",
      "Debug shapes - Full x: torch.Size([8, 4, 2, 287, 597]), STFT: torch.Size([8, 4, 2, 287, 513]), CQT: torch.Size([8, 4, 2, 287, 84])\n",
      "Norm of S: 20.2762\n",
      "Norm of C: 11.1437\n",
      "Norm of K: 5.7873\n",
      "Norm of L: 7.2178\n",
      "Norm of S: 20.2762\n",
      "Norm of C: 11.1437\n",
      "Norm of K: 5.7873\n",
      "Norm of L: 7.2178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:  23%|██▎       | 3/13 [00:53<02:55, 17.50s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2/13 | Discriminator loss: 2.0765 | Total Generator loss: nan | Reconstruction loss: nan |   - MSE: nan |   - Magnitude: nan |   - Phase: nan |   - Temporal: nan |   - Spectral: nan | Adversary Generator loss: -0.6728 | Disentanglement loss: 0.0052\n",
      "Debug shapes - Full x: torch.Size([8, 4, 2, 287, 597]), STFT: torch.Size([8, 4, 2, 287, 513]), CQT: torch.Size([8, 4, 2, 287, 84])\n",
      "Norm of S: 19.6433\n",
      "Norm of C: 11.2639\n",
      "Norm of K: 5.8253\n",
      "Norm of L: 7.1717\n",
      "Norm of S: 19.6433\n",
      "Norm of C: 11.2639\n",
      "Norm of K: 5.8253\n",
      "Norm of L: 7.1717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:  31%|███       | 4/13 [01:09<02:33, 17.04s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3/13 | Discriminator loss: 2.3182 | Total Generator loss: nan | Reconstruction loss: nan |   - MSE: nan |   - Magnitude: nan |   - Phase: nan |   - Temporal: nan |   - Spectral: nan | Adversary Generator loss: -0.6894 | Disentanglement loss: 0.0054\n",
      "Debug shapes - Full x: torch.Size([8, 4, 2, 287, 597]), STFT: torch.Size([8, 4, 2, 287, 513]), CQT: torch.Size([8, 4, 2, 287, 84])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:  31%|███       | 4/13 [01:16<02:52, 19.21s/batch]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   0%|          | 0/13 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug shapes - Full x: torch.Size([8, 4, 2, 287, 597]), STFT: torch.Size([8, 4, 2, 287, 513]), CQT: torch.Size([8, 4, 2, 287, 84])\n",
      "Norm of S: 20.5219\n",
      "Norm of C: 11.4853\n",
      "Norm of K: 5.7190\n",
      "Norm of L: 7.1617\n",
      "Norm of S: 20.5219\n",
      "Norm of C: 11.4853\n",
      "Norm of K: 5.7190\n",
      "Norm of L: 7.1617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   8%|▊         | 1/13 [00:19<03:55, 19.63s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0/13 | Discriminator loss: 1.9071 | Total Generator loss: 36.1859 | Reconstruction loss: 3.4575 |   - MSE: 0.9999 |   - Magnitude: 1.9996 |   - Phase: 3.2891 |   - Temporal: 2.0000 |   - Spectral: 1.9995 | Adversary Generator loss: -0.6908 | Disentanglement loss: 0.0058\n",
      "Debug shapes - Full x: torch.Size([8, 4, 2, 287, 597]), STFT: torch.Size([8, 4, 2, 287, 513]), CQT: torch.Size([8, 4, 2, 287, 84])\n",
      "Norm of S: 20.4255\n",
      "Norm of C: 11.5970\n",
      "Norm of K: 5.6970\n",
      "Norm of L: 7.1310\n",
      "Norm of S: 20.4255\n",
      "Norm of C: 11.5970\n",
      "Norm of K: 5.6970\n",
      "Norm of L: 7.1310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:  15%|█▌        | 2/13 [00:36<03:17, 17.92s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/13 | Discriminator loss: 1.9018 | Total Generator loss: nan | Reconstruction loss: nan |   - MSE: nan |   - Magnitude: nan |   - Phase: nan |   - Temporal: nan |   - Spectral: nan | Adversary Generator loss: -0.3888 | Disentanglement loss: 0.0061\n",
      "Debug shapes - Full x: torch.Size([8, 4, 2, 287, 597]), STFT: torch.Size([8, 4, 2, 287, 513]), CQT: torch.Size([8, 4, 2, 287, 84])\n",
      "Norm of S: 20.2762\n",
      "Norm of C: 11.1437\n",
      "Norm of K: 5.7873\n",
      "Norm of L: 7.2178\n",
      "Norm of S: 20.2762\n",
      "Norm of C: 11.1437\n",
      "Norm of K: 5.7873\n",
      "Norm of L: 7.2178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:  23%|██▎       | 3/13 [00:53<02:55, 17.50s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2/13 | Discriminator loss: 2.0765 | Total Generator loss: nan | Reconstruction loss: nan |   - MSE: nan |   - Magnitude: nan |   - Phase: nan |   - Temporal: nan |   - Spectral: nan | Adversary Generator loss: -0.6728 | Disentanglement loss: 0.0052\n",
      "Debug shapes - Full x: torch.Size([8, 4, 2, 287, 597]), STFT: torch.Size([8, 4, 2, 287, 513]), CQT: torch.Size([8, 4, 2, 287, 84])\n",
      "Norm of S: 19.6433\n",
      "Norm of C: 11.2639\n",
      "Norm of K: 5.8253\n",
      "Norm of L: 7.1717\n",
      "Norm of S: 19.6433\n",
      "Norm of C: 11.2639\n",
      "Norm of K: 5.8253\n",
      "Norm of L: 7.1717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:  31%|███       | 4/13 [01:09<02:33, 17.04s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3/13 | Discriminator loss: 2.3182 | Total Generator loss: nan | Reconstruction loss: nan |   - MSE: nan |   - Magnitude: nan |   - Phase: nan |   - Temporal: nan |   - Spectral: nan | Adversary Generator loss: -0.6894 | Disentanglement loss: 0.0054\n",
      "Debug shapes - Full x: torch.Size([8, 4, 2, 287, 597]), STFT: torch.Size([8, 4, 2, 287, 513]), CQT: torch.Size([8, 4, 2, 287, 84])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:  31%|███       | 4/13 [01:16<02:52, 19.21s/batch]\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 96\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# forward pass - use full data for encoders\u001b[39;00m\n\u001b[1;32m     95\u001b[0m style_emb, class_emb \u001b[38;5;241m=\u001b[39m style_encoder(x, labels)\n\u001b[0;32m---> 96\u001b[0m content_emb \u001b[38;5;241m=\u001b[39m \u001b[43mcontent_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# adversarial loss for the generator\u001b[39;00m\n\u001b[1;32m     99\u001b[0m _, adv_generator_loss \u001b[38;5;241m=\u001b[39m adversarial_loss(style_emb, class_emb, content_emb, discriminator, labels,\n\u001b[1;32m    100\u001b[0m                                      compute_for_discriminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/Audio-Style-Transfer/content_encoder.py:72\u001b[0m, in \u001b[0;36mContentEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# CNN\u001b[39;00m\n\u001b[1;32m     71\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(B \u001b[38;5;241m*\u001b[39m S, C, T, F)                                                      \u001b[38;5;66;03m# (B*S, 2, T, F)\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m cnn_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m                                                      \u001b[38;5;66;03m# (B*S, last_chan_size, 1, 1)\u001b[39;00m\n\u001b[1;32m     73\u001b[0m cnn_features \u001b[38;5;241m=\u001b[39m cnn_features\u001b[38;5;241m.\u001b[39mview(cnn_features\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)                      \u001b[38;5;66;03m# (B*S, last_chan_size)\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Project to cnn_out_dim if last_chan_size != cnn_out_dim\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.10/site-packages/torch/nn/modules/container.py:240\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 240\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/Audio-Style-Transfer/style_encoder.py:83\u001b[0m, in \u001b[0;36mResBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     81\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(out)\n\u001b[1;32m     82\u001b[0m out \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mReLU()(out)\n\u001b[0;32m---> 83\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(out)\n\u001b[1;32m     85\u001b[0m out \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m identity  \u001b[38;5;66;03m# skip connection\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.10/site-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.10/site-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c212db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzioni di utilità per Colab\n",
    "import psutil\n",
    "import gc\n",
    "\n",
    "def monitor_resources():\n",
    "    \"\"\"Monitora le risorse di sistema\"\"\"\n",
    "    # CPU\n",
    "    cpu_percent = psutil.cpu_percent()\n",
    "    \n",
    "    # RAM\n",
    "    ram = psutil.virtual_memory()\n",
    "    ram_percent = ram.percent\n",
    "    \n",
    "    # GPU\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory = torch.cuda.memory_allocated() / 1024**2\n",
    "        gpu_cached = torch.cuda.memory_reserved() / 1024**2\n",
    "        \n",
    "        print(f\"CPU: {cpu_percent:.1f}% | RAM: {ram_percent:.1f}% | GPU: {gpu_memory:.0f}MB/{gpu_cached:.0f}MB\")\n",
    "        \n",
    "        # Allarme se troppa memoria\n",
    "        if gpu_memory > 8000:  # >8GB\n",
    "            print(\"⚠️ ATTENZIONE: Memoria GPU alta!\")\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"Libera memoria GPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        print(\"🧹 Memoria GPU liberata\")\n",
    "\n",
    "def get_model_size(model):\n",
    "    \"\"\"Calcola la dimensione del modello\"\"\"\n",
    "    param_count = sum(p.numel() for p in model.parameters())\n",
    "    param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "    buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())\n",
    "    size_mb = (param_size + buffer_size) / 1024**2\n",
    "    return param_count, size_mb\n",
    "\n",
    "# Test delle risorse\n",
    "print(\"=== RESOURCE MONITORING ===\")\n",
    "monitor_resources()\n",
    "\n",
    "# ✅ Crea un decoder dinamico di test (senza parametro S!)\n",
    "test_decoder = DynamicDecoder(d_model=256, nhead=4, num_layers=3)\n",
    "param_count, size_mb = get_model_size(test_decoder)\n",
    "print(f\"Dynamic Decoder: {param_count:,} parametri ({size_mb:.1f} MB)\")\n",
    "\n",
    "del test_decoder  # Libera memoria\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d237fc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test del decoder dinamico prima del training\n",
    "def test_dynamic_decoder():\n",
    "    \"\"\"Test del decoder dinamico con sequenze di lunghezze diverse\"\"\"\n",
    "    print(\"🔧 Testing Dynamic Decoder...\")\n",
    "    \n",
    "    # Parametri di test\n",
    "    B, d_model = 4, 256\n",
    "    \n",
    "    # ✅ Crea decoder dinamico (SENZA parametro S!)\n",
    "    decoder = DynamicDecoder(d_model=d_model, nhead=4, num_layers=2)\n",
    "    \n",
    "    # Test con sequenze di lunghezze diverse\n",
    "    test_cases = [\n",
    "        (2, \"sequenza molto corta\"),\n",
    "        (4, \"sequenza standard\"), \n",
    "        (6, \"sequenza lunga\"),\n",
    "        (8, \"sequenza molto lunga\")\n",
    "    ]\n",
    "    \n",
    "    for S, description in test_cases:\n",
    "        print(f\"\\n--- Test: {description} (S={S}) ---\")\n",
    "        \n",
    "        # Input di test\n",
    "        content_emb = torch.randn(B, S, d_model)  # [B, S, d_model] - S dinamico!\n",
    "        class_emb = torch.randn(B, d_model)       # [B, d_model]\n",
    "        y_target = torch.randn(B, S, 2, 287, 513)  # [B, S, 2, 287, 513] - S dinamico!\n",
    "        \n",
    "        print(f\"Content embedding shape: {content_emb.shape}\")\n",
    "        print(f\"Class embedding shape: {class_emb.shape}\")\n",
    "        print(f\"Target shape: {y_target.shape}\")\n",
    "        \n",
    "        # Test training mode\n",
    "        decoder.train()\n",
    "        try:\n",
    "            output = decoder(content_emb, class_emb, y=y_target)\n",
    "            print(f\"✅ Training mode output shape: {output.shape}\")\n",
    "            assert output.shape == y_target.shape, f\"Shape mismatch: {output.shape} vs {y_target.shape}\"\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Training mode error: {e}\")\n",
    "            return False\n",
    "        \n",
    "        # Test inference mode con target_length specificato\n",
    "        decoder.eval()\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                output = decoder(content_emb, class_emb, target_length=S)\n",
    "                print(f\"✅ Inference mode output shape: {output.shape}\")\n",
    "                assert output.shape == (B, S, 2, 287, 513), f\"Inference shape mismatch: {output.shape}\"\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Inference mode error: {e}\")\n",
    "            return False\n",
    "        \n",
    "        # Test inference mode senza target_length (usa default)\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                output_auto = decoder(content_emb, class_emb)\n",
    "                print(f\"✅ Auto-length inference output shape: {output_auto.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Auto-length inference error: {e}\")\n",
    "            return False\n",
    "        \n",
    "        # Test loss\n",
    "        try:\n",
    "            loss_dict = compute_comprehensive_loss(output, y_target)\n",
    "            print(f\"✅ Loss computation successful: {loss_dict['total_loss'].item():.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Loss computation error: {e}\")\n",
    "            return False\n",
    "    \n",
    "    print(\"\\n🎉 Dynamic Decoder test successful! Decoder can handle variable sequence lengths!\")\n",
    "    return True\n",
    "\n",
    "# Esegui test\n",
    "print(\"Testing the new Dynamic Decoder...\")\n",
    "if test_dynamic_decoder():\n",
    "    print(\"✅ Dynamic Decoder pronto per il training!\")\n",
    "else:\n",
    "    print(\"❌ Dynamic Decoder ha problemi, controlla gli errori.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1c6ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug utilities per il training dinamico\n",
    "def debug_shapes(x, content_emb, style_emb, class_emb, step=\"forward\"):\n",
    "    \"\"\"Debug delle dimensioni durante il training\"\"\"\n",
    "    print(f\"\\n=== DEBUG SHAPES - {step} ===\")\n",
    "    print(f\"Input x shape: {x.shape}\")\n",
    "    print(f\"Content embedding shape: {content_emb.shape}\")\n",
    "    print(f\"Style embedding shape: {style_emb.shape}\")\n",
    "    print(f\"Class embedding shape: {class_emb.shape}\")\n",
    "    \n",
    "    # Verifica dimensioni attese\n",
    "    B, S, C, H, W = x.shape\n",
    "    B_c, S_c, D_c = content_emb.shape\n",
    "    B_s, D_s = style_emb.shape\n",
    "    B_cl, D_cl = class_emb.shape\n",
    "    \n",
    "    print(f\"Batch size: {B}\")\n",
    "    print(f\"Sequence length: {S}\")\n",
    "    print(f\"STFT dims: {C}x{H}x{W}\")\n",
    "    \n",
    "    # Verifica coerenza\n",
    "    assert B == B_c == B_s == B_cl, f\"Batch size mismatch: {B}, {B_c}, {B_s}, {B_cl}\"\n",
    "    assert S == S_c, f\"Sequence length mismatch: {S}, {S_c}\"\n",
    "    assert D_c == D_s == D_cl, f\"Embedding dimension mismatch: {D_c}, {D_s}, {D_cl}\"\n",
    "    \n",
    "    print(\"✅ All dimensions are consistent!\")\n",
    "    return True\n",
    "\n",
    "# Versione safe del training con debug del decoder dinamico\n",
    "def safe_debug_train():\n",
    "    \"\"\"Training con debug delle dimensioni per decoder dinamico\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # ✅ Inizializza modelli con decoder dinamico\n",
    "    style_encoder = StyleEncoder(transformer_dim=TRANSFORMER_DIM).to(device)\n",
    "    content_encoder = ContentEncoder(transformer_dim=TRANSFORMER_DIM).to(device)\n",
    "    decoder = DynamicDecoder(d_model=TRANSFORMER_DIM, nhead=4, num_layers=2).to(device)  # Senza S!\n",
    "    \n",
    "    # Test con un batch\n",
    "    dataloader = get_dummy_dataloader(batch_size=2)  # Batch piccolo per debug\n",
    "    \n",
    "    print(\"\\n🔍 Testing forward pass with Dynamic Decoder...\")\n",
    "    \n",
    "    for i, (x, labels) in enumerate(dataloader):\n",
    "        x, labels = x.to(device), labels.to(device)\n",
    "        \n",
    "        print(f\"\\nBatch {i+1} shapes:\")\n",
    "        print(f\"  x: {x.shape}\")\n",
    "        print(f\"  labels: {labels.shape}\")\n",
    "        \n",
    "        # Forward pass encoders\n",
    "        style_emb, class_emb = style_encoder(x, labels)\n",
    "        content_emb = content_encoder(x)\n",
    "        \n",
    "        # Debug shapes\n",
    "        debug_shapes(x, content_emb, style_emb, class_emb)\n",
    "        \n",
    "        # Test decoder dinamico\n",
    "        print(f\"\\n🧪 Testing Dynamic Decoder...\")\n",
    "        try:\n",
    "            # Training mode con teacher forcing\n",
    "            decoder.train()\n",
    "            reconstructed = decoder(content_emb, style_emb, y=x)\n",
    "            print(f\"✅ Training mode output shape: {reconstructed.shape}\")\n",
    "            print(f\"Expected shape: {x.shape}\")\n",
    "            assert reconstructed.shape == x.shape, f\"Shape mismatch: {reconstructed.shape} vs {x.shape}\"\n",
    "            \n",
    "            # Inference mode\n",
    "            decoder.eval()\n",
    "            with torch.no_grad():\n",
    "                reconstructed_inf = decoder(content_emb, style_emb)\n",
    "                print(f\"✅ Inference mode output shape: {reconstructed_inf.shape}\")\n",
    "            \n",
    "            # Test loss\n",
    "            loss_dict = compute_comprehensive_loss(reconstructed, x)\n",
    "            print(f\"✅ Loss computation: {loss_dict['total_loss'].item():.4f}\")\n",
    "            \n",
    "            print(\"✅ Dynamic Decoder test successful!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Dynamic Decoder error: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return False\n",
    "        \n",
    "        # Test solo il primo batch\n",
    "        break\n",
    "    \n",
    "    print(\"\\n🎉 Debug test completed successfully!\")\n",
    "    print(\"🚀 Dynamic Decoder is ready for full training!\")\n",
    "    return True\n",
    "\n",
    "# Test delle dimensioni del modello dinamico\n",
    "def check_dynamic_model_size():\n",
    "    \"\"\"Controlla le dimensioni del modello dinamico\"\"\"\n",
    "    print(\"\\n📊 Checking Dynamic Model Sizes...\")\n",
    "    \n",
    "    # Crea modelli\n",
    "    style_encoder = StyleEncoder(transformer_dim=TRANSFORMER_DIM)\n",
    "    content_encoder = ContentEncoder(transformer_dim=TRANSFORMER_DIM)\n",
    "    decoder = DynamicDecoder(d_model=TRANSFORMER_DIM, nhead=4, num_layers=3)\n",
    "    discriminator = Discriminator(input_dim=TRANSFORMER_DIM)\n",
    "    \n",
    "    models = [\n",
    "        (\"Style Encoder\", style_encoder),\n",
    "        (\"Content Encoder\", content_encoder), \n",
    "        (\"Dynamic Decoder\", decoder),\n",
    "        (\"Discriminator\", discriminator)\n",
    "    ]\n",
    "    \n",
    "    total_params = 0\n",
    "    total_size = 0\n",
    "    \n",
    "    for name, model in models:\n",
    "        param_count, size_mb = get_model_size(model)\n",
    "        total_params += param_count\n",
    "        total_size += size_mb\n",
    "        print(f\"{name}: {param_count:,} parametri ({size_mb:.1f} MB)\")\n",
    "    \n",
    "    print(f\"\\n📈 TOTALE: {total_params:,} parametri ({total_size:.1f} MB)\")\n",
    "    \n",
    "    # Stima memoria per batch_size=8\n",
    "    estimated_memory = total_size * 3 + (8 * 4 * 2 * 287 * 513 * 4) / (1024**2)  # modello + gradients + data\n",
    "    print(f\"💾 Memoria stimata (batch_size=8): ~{estimated_memory:.0f} MB\")\n",
    "    \n",
    "    if estimated_memory < 10000:  # <10GB\n",
    "        print(\"✅ Dovrebbe funzionare su Colab Free!\")\n",
    "    else:\n",
    "        print(\"⚠️ Potrebbe essere troppo per Colab Free\")\n",
    "\n",
    "# Esegui controlli\n",
    "check_dynamic_model_size()\n",
    "\n",
    "# Uncomment to run debug test\n",
    "# safe_debug_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d6b367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training con monitoraggio delle risorse\n",
    "def safe_train():\n",
    "    \"\"\"Training con monitoraggio delle risorse per Colab\"\"\"\n",
    "    try:\n",
    "        print(\"🚀 Avvio training con monitoraggio risorse...\")\n",
    "        monitor_resources()\n",
    "        \n",
    "        # Avvia il training\n",
    "        train()\n",
    "        \n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e):\n",
    "            print(\"💥 ERRORE: Memoria GPU insufficiente!\")\n",
    "            print(\"Prova a ridurre:\")\n",
    "            print(\"- BATCH_SIZE (attualmente {})\".format(BATCH_SIZE))\n",
    "            print(\"- NUM_FRAMES (attualmente {})\".format(NUM_FRAMES))\n",
    "            print(\"- TRANSFORMER_DIM (attualmente {})\".format(TRANSFORMER_DIM))\n",
    "            clear_memory()\n",
    "        else:\n",
    "            print(f\"❌ Errore durante il training: {e}\")\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"⏹️ Training interrotto dall'utente\")\n",
    "        clear_memory()\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Errore imprevisto: {e}\")\n",
    "        clear_memory()\n",
    "    finally:\n",
    "        print(\"🏁 Training terminato\")\n",
    "        monitor_resources()\n",
    "\n",
    "# Uncomment to run training\n",
    "# safe_train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
