{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "205dcdf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# models\n",
    "from style_encoder import StyleEncoder, initialize_weights\n",
    "from content_encoder import ContentEncoder\n",
    "from discriminator import Discriminator\n",
    "from new_decoder import Decoder, compute_comprehensive_loss  # Nuovo decoder dinamico\n",
    "from losses import infoNCE_loss, margin_loss, adversarial_loss, disentanglement_loss\n",
    "from dataloader import get_dataloader\n",
    "\n",
    "# device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "# hyperparameters\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 8\n",
    "LR_GEN = 5e-5  # Learning rate per Encoders + Decoder\n",
    "LR_DISC = 1e-5  # Learning rate per Discriminator\n",
    "TRANSFORMER_DIM = 256\n",
    "NUM_FRAMES = 4\n",
    "STFT_T, STFT_F = 287, 513\n",
    "CQT_T, CQT_F = 287, 84\n",
    "\n",
    "# loss weights\n",
    "LAMBDA_RECON = 1.0\n",
    "LAMBDA_INFO_NCE = 0.5\n",
    "LAMBDA_MARGIN = 0.5\n",
    "LAMBDA_DISENTANGLE = 0.5\n",
    "LAMBDA_ADV_GEN = 0.01 # Peso per la loss avversaria del generatore\n",
    "\n",
    "# Pesi per le loss comprehensive del decoder\n",
    "LAMBDA_TEMPORAL = 0.3\n",
    "LAMBDA_PHASE = 0.2\n",
    "LAMBDA_SPECTRAL = 0.1\n",
    "LAMBDA_CONSISTENCY = 0.1\n",
    "\n",
    "MODEL_SAVE_PATH = \"./saved_models\"\n",
    "os.makedirs(MODEL_SAVE_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b43ed26",
   "metadata": {},
   "source": [
    "aggiungere data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3042828e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights_conservative(m):\n",
    "    \"\"\"\n",
    "    Inizializzazione conservativa dei pesi per prevenire NaN\n",
    "    \"\"\"\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        # Xavier uniforme con gain ridotto\n",
    "        nn.init.xavier_uniform_(m.weight, gain=0.2)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        # Xavier uniforme con gain ridotto\n",
    "        nn.init.xavier_uniform_(m.weight, gain=0.2)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        nn.init.ones_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.LayerNorm):\n",
    "        nn.init.ones_(m.weight)\n",
    "        nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e7c204",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    print(f\"Training on device: {device}\")\n",
    "\n",
    "    # load models\n",
    "    style_encoder = StyleEncoder(transformer_dim=TRANSFORMER_DIM).to(device)\n",
    "    content_encoder = ContentEncoder(transformer_dim=TRANSFORMER_DIM).to(device)\n",
    "\n",
    "    # decoder\n",
    "    decoder = Decoder(\n",
    "        d_model=TRANSFORMER_DIM,\n",
    "        nhead=4,\n",
    "        num_layers=4,  # Ridotto per Colab\n",
    "        dim_feedforward=TRANSFORMER_DIM * 2,\n",
    "        dropout=0.1\n",
    "    ).to(device)\n",
    "    \n",
    "    discriminator = Discriminator(input_dim=TRANSFORMER_DIM).to(device)\n",
    "\n",
    "    # initialize weights\n",
    "    # initialize_weights(style_encoder)\n",
    "    # initialize_weights(content_encoder)\n",
    "    # initialize_weights(discriminator)\n",
    "    # decoder già ha la sua inizializzazione\n",
    "    models = [style_encoder, content_encoder, discriminator, decoder]\n",
    "    model_names = [\"style_encoder\", \"content_encoder\", \"discriminator\", \"decoder\"]\n",
    "\n",
    "    for model, name in zip(models, model_names):\n",
    "        model.apply(init_weights_conservative)\n",
    "        print(f\"✅ {name} initialized\")\n",
    "\n",
    "    # optimizer for generators (style encoder, content encoder, decoder)\n",
    "    optimizer_G = optim.Adam(\n",
    "        list(style_encoder.parameters()) + list(content_encoder.parameters()) + list(decoder.parameters()),\n",
    "        lr=LR_GEN, betas=(0.5, 0.999)\n",
    "    )\n",
    "    \n",
    "    # optimizer for discriminator\n",
    "    optimizer_D = optim.Adam(discriminator.parameters(), lr=LR_DISC, betas=(0.5, 0.999))\n",
    "\n",
    "    # Create train and validation dataloaders\n",
    "    train_dataloader = get_dataloader(\n",
    "        piano_dir=\"dataset/train/piano\",\n",
    "        violin_dir=\"dataset/train/violin\",\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        stats_path=\"stats_stft_cqt.npz\"\n",
    "    )\n",
    "    \n",
    "    val_dataloader = get_dataloader(\n",
    "        piano_dir=\"dataset/val/piano\", \n",
    "        violin_dir=\"dataset/val/violin\",\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,  # No shuffle for validation\n",
    "        stats_path=\"stats_stft_cqt.npz\"\n",
    "    )\n",
    "    \n",
    "    print(f\"Training batches: {len(train_dataloader)}\")\n",
    "    print(f\"Validation batches: {len(val_dataloader)}\")\n",
    "    \n",
    "    # loss function for reconstruction\n",
    "    def recon_loss_fn(output, target):\n",
    "        loss_dict = compute_comprehensive_loss(\n",
    "            output, target, \n",
    "            lambda_temporal=LAMBDA_TEMPORAL,\n",
    "            lambda_phase=LAMBDA_PHASE,\n",
    "            lambda_spectral=LAMBDA_SPECTRAL,\n",
    "            lambda_consistency=LAMBDA_CONSISTENCY\n",
    "        )\n",
    "        return loss_dict['total_loss'], loss_dict\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    # train loop with epoch progress bar\n",
    "    epoch_pbar = tqdm(range(EPOCHS), desc=\"Training\", unit=\"epoch\")\n",
    "    \n",
    "    for epoch in epoch_pbar:\n",
    "        epoch_pbar.set_description(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "        \n",
    "        # ================================================================== #\n",
    "        #                             TRAINING                               #\n",
    "        # ================================================================== #\n",
    "        style_encoder.train()\n",
    "        content_encoder.train()\n",
    "        decoder.train()\n",
    "        discriminator.train()\n",
    "        \n",
    "        train_loss_epoch = 0\n",
    "        train_recon_loss_epoch = 0\n",
    "        train_batches = 0\n",
    "        \n",
    "        for i, (x, labels) in enumerate(train_dataloader):\n",
    "            x, labels = x.to(device), labels.to(device) # x: (B, S, 2, T, F)\n",
    "            stft_part = x[:, :, :, :, :STFT_F]  # STFT part\n",
    "\n",
    "            # ================================================================== #\n",
    "            #                             Discriminator                          #\n",
    "            # ================================================================== #\n",
    "            optimizer_D.zero_grad()\n",
    "            \n",
    "            # with torch.no_grad() to avoid computing gradients for the encoders    <----------------\n",
    "            style_emb, class_emb = style_encoder(x, labels)\n",
    "            content_emb = content_encoder(x)\n",
    "            \n",
    "            # adversarial loss for the discriminator\n",
    "            discriminator_loss, _ = adversarial_loss(style_emb.detach(), class_emb.detach(), \n",
    "                                                     content_emb.detach(), discriminator, labels, \n",
    "                                                     compute_for_discriminator=True)\n",
    "            # discriminator_loss = torch.tensor(0.0, device=device)\n",
    "            \n",
    "            discriminator_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            # ================================================================== #\n",
    "            #               Generators (Style Encoder, Content Encoder)          #\n",
    "            # ================================================================== #\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            # forward pass\n",
    "            style_emb, class_emb = style_encoder(x, labels)\n",
    "            content_emb = content_encoder(x)\n",
    "\n",
    "            # adversarial loss for the generator\n",
    "            _, adv_generator_loss = adversarial_loss(style_emb, class_emb, content_emb, discriminator, labels,\n",
    "                                                 compute_for_discriminator=False)\n",
    "\n",
    "            # adv_generator_loss = torch.tensor(0.0, device=device)\n",
    "\n",
    "            # disentanglement loss\n",
    "            disent_loss = disentanglement_loss(style_emb, content_emb.mean(dim=1), use_hsic=True)\n",
    "\n",
    "            if len(torch.unique(labels)) > 1:\n",
    "                # contrastive losses\n",
    "                loss_infonce = infoNCE_loss(style_emb, labels)\n",
    "                loss_margin = margin_loss(class_emb)\n",
    "            else:\n",
    "                # Fallback se tutti i label sono uguali in questo batch\n",
    "                loss_infonce = torch.tensor(0.0, device=device)\n",
    "                loss_margin = torch.tensor(0.0, device=device)\n",
    "\n",
    "            # reconstruction loss\n",
    "            B = content_emb.size(0)\n",
    "            class_emb = class_emb.repeat_interleave(repeats=B//2, dim=0)\n",
    "\n",
    "            reconstructed_spec = decoder(content_emb, class_emb, y=stft_part)  # y=x per teacher forcing\n",
    "            loss_recon, loss_dict = recon_loss_fn(reconstructed_spec, stft_part)\n",
    "\n",
    "            if torch.isnan(loss_recon):\n",
    "                print(f\"⚠️ NaN detected in reconstruction loss at batch {i+1}\")\n",
    "                print(f\"   Reconstructed spec stats: min={reconstructed_spec.min():.4f}, max={reconstructed_spec.max():.4f}\")\n",
    "                print(f\"   Target spec stats: min={stft_part.min():.4f}, max={stft_part.max():.4f}\")\n",
    "                continue  # Skip this batch\n",
    "            \n",
    "            # total generator loss\n",
    "            total_gen_loss = (\n",
    "                LAMBDA_RECON * loss_recon +\n",
    "                LAMBDA_INFO_NCE * loss_infonce +\n",
    "                LAMBDA_MARGIN * loss_margin +\n",
    "                LAMBDA_DISENTANGLE * disent_loss \n",
    "                # LAMBDA_ADV_GEN * adv_generator_loss\n",
    "            )\n",
    "\n",
    "            total_gen_loss.backward()\n",
    "\n",
    "            # gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                list(style_encoder.parameters()) + \n",
    "                list(content_encoder.parameters()) + \n",
    "                list(decoder.parameters()), \n",
    "                max_norm=0.5\n",
    "            )\n",
    "\n",
    "            # AGGIUNGI CONTROLLO NaN sui gradienti\n",
    "            has_nan_grad = False\n",
    "            for name, param in decoder.named_parameters():\n",
    "                if param.grad is not None and torch.isnan(param.grad).any():\n",
    "                    print(f\"⚠️ NaN gradient detected in {name}\")\n",
    "                    has_nan_grad = True\n",
    "            \n",
    "            if has_nan_grad:\n",
    "                print(\"⚠️ Skipping optimizer step due to NaN gradients\")\n",
    "                continue\n",
    "\n",
    "            optimizer_G.step()\n",
    "\n",
    "            # Accumulate losses\n",
    "            train_loss_epoch += total_gen_loss.item()\n",
    "            train_recon_loss_epoch += loss_recon.item()\n",
    "            train_batches += 1\n",
    "\n",
    "            # Print batch metrics every batch\n",
    "            print(f\"Epoch {epoch+1}/{EPOCHS} - Batch {i+1}/{len(train_dataloader)} | \"\n",
    "                  f\"D_loss: {discriminator_loss.item():.4f} | \"\n",
    "                  f\"G_loss: {total_gen_loss.item():.4f} | \"\n",
    "                  f\"Recon: {loss_recon.item():.4f} | \"\n",
    "                  f\"InfoNCE: {loss_infonce.item():.4f} | \"\n",
    "                  f\"Margin: {loss_margin.item():.4f} | \"\n",
    "                  f\"Disentangle: {disent_loss.item():.4f}\")\n",
    "        \n",
    "        # Average training losses\n",
    "        avg_train_loss = train_loss_epoch / train_batches\n",
    "        avg_train_recon_loss = train_recon_loss_epoch / train_batches\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # ================================================================== #\n",
    "        #                            VALIDATION                              #\n",
    "        # ================================================================== #\n",
    "        style_encoder.eval()\n",
    "        content_encoder.eval()\n",
    "        decoder.eval()\n",
    "        discriminator.eval()\n",
    "        \n",
    "        val_loss_epoch = 0\n",
    "        val_recon_loss_epoch = 0\n",
    "        val_batches = 0\n",
    "        \n",
    "        print(f\"\\n🔍 Running validation for epoch {epoch+1}...\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i, (x, labels) in enumerate(val_dataloader):\n",
    "                x, labels = x.to(device), labels.to(device)\n",
    "                stft_part = x[:, :, :, :, :STFT_F]\n",
    "\n",
    "                # Forward pass\n",
    "                style_emb, class_emb = style_encoder(x, labels)\n",
    "                content_emb = content_encoder(x)\n",
    "\n",
    "                # Validation losses (only the main ones)\n",
    "                if len(torch.unique(labels)) > 1:\n",
    "                    loss_infonce = infoNCE_loss(style_emb, labels)\n",
    "                    loss_margin = margin_loss(class_emb)\n",
    "                else:\n",
    "                    loss_infonce = torch.tensor(0.0, device=device)\n",
    "                    loss_margin = torch.tensor(0.0, device=device)\n",
    "\n",
    "                disent_loss = disentanglement_loss(style_emb, content_emb.mean(dim=1), use_hsic=True)\n",
    "\n",
    "                # reconstruction loss\n",
    "                # duplicate class embedding from (2,d) to (B,d)\n",
    "                B = content_emb.size(0)\n",
    "                class_emb = class_emb.repeat_interleave(repeats=B//2, dim=0)\n",
    "                \n",
    "                # Reconstruction loss\n",
    "                reconstructed_spec = decoder(content_emb, class_emb, y=stft_part)\n",
    "                loss_recon, _ = recon_loss_fn(reconstructed_spec, stft_part)\n",
    "                \n",
    "                # Total validation loss\n",
    "                total_val_loss = (\n",
    "                    LAMBDA_RECON * loss_recon +\n",
    "                    LAMBDA_INFO_NCE * loss_infonce +\n",
    "                    LAMBDA_MARGIN * loss_margin +\n",
    "                    LAMBDA_DISENTANGLE * disent_loss\n",
    "                )\n",
    "\n",
    "                val_loss_epoch += total_val_loss.item()\n",
    "                val_recon_loss_epoch += loss_recon.item()\n",
    "                val_batches += 1\n",
    "\n",
    "        # Average validation losses\n",
    "        avg_val_loss = val_loss_epoch / val_batches\n",
    "        avg_val_recon_loss = val_recon_loss_epoch / val_batches\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f\"\\nEpoch {epoch+1}/{EPOCHS} Summary:\")\n",
    "        print(f\"    Train Loss: {avg_train_loss:.4f} | Train Recon: {avg_train_recon_loss:.4f}\")\n",
    "        print(f\"    Val Loss:   {avg_val_loss:.4f} | Val Recon:   {avg_val_recon_loss:.4f}\")\n",
    "        \n",
    "        # Update progress bar with current losses\n",
    "        epoch_pbar.set_postfix({\n",
    "            'Train_Loss': f'{avg_train_loss:.4f}',\n",
    "            'Val_Loss': f'{avg_val_loss:.4f}',\n",
    "            'Best_Val': f'{best_val_loss:.4f}'\n",
    "        })\n",
    "        \n",
    "        # Save best model based on validation loss\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            print(f\"New best validation loss: {best_val_loss:.4f}. Saving model...\")\n",
    "            \n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'style_encoder_state_dict': style_encoder.state_dict(),\n",
    "                'content_encoder_state_dict': content_encoder.state_dict(),\n",
    "                'decoder_state_dict': decoder.state_dict(),\n",
    "                'discriminator_state_dict': discriminator.state_dict(),\n",
    "                'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
    "                'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
    "                'best_val_loss': best_val_loss,\n",
    "                'train_losses': train_losses,\n",
    "                'val_losses': val_losses,\n",
    "            }, os.path.join(MODEL_SAVE_PATH, 'best_model.pth'))\n",
    "        \n",
    "        # Early stopping check (opzionale)\n",
    "        if epoch > 10 and avg_val_loss > max(val_losses[-5:]):\n",
    "            print(\"⚠️ Validation loss not improving. Consider early stopping.\")\n",
    "    \n",
    "    print(f\"\\nTraining completed!\")\n",
    "    print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df6100b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device: cuda\n",
      "✅ style_encoder initialized\n",
      "✅ content_encoder initialized\n",
      "✅ discriminator initialized\n",
      "✅ decoder initialized\n",
      "✅ Loaded separate statistics:\n",
      "  Piano: train_set_stats/stats_stft_cqt_piano.npz\n",
      "  Violin: train_set_stats/stats_stft_cqt_violin.npz\n",
      "✅ Loaded separate statistics:\n",
      "  Piano: train_set_stats/stats_stft_cqt_piano.npz\n",
      "  Violin: train_set_stats/stats_stft_cqt_violin.npz\n",
      "Training batches: 77\n",
      "Validation batches: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   0%|          | 0/50 [00:00<?, ?epoch/s]c:\\Users\\Francesco\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:5560: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Batch 1/77 | D_loss: 1.7328 | G_loss: 2.7599 | Recon: 1.7856 | InfoNCE: 1.9420 | Margin: 0.0000 | Disentangle: 0.0068\n",
      "Epoch 1/50 - Batch 2/77 | D_loss: 1.7327 | G_loss: 3.0206 | Recon: 2.0549 | InfoNCE: 1.9300 | Margin: 0.0000 | Disentangle: 0.0014\n",
      "Epoch 1/50 - Batch 3/77 | D_loss: 1.7323 | G_loss: 3.2280 | Recon: 2.3276 | InfoNCE: 1.8000 | Margin: 0.0000 | Disentangle: 0.0008\n",
      "Epoch 1/50 - Batch 4/77 | D_loss: 1.7322 | G_loss: 3.5174 | Recon: 2.5669 | InfoNCE: 1.9008 | Margin: 0.0000 | Disentangle: 0.0002\n",
      "Epoch 1/50 - Batch 5/77 | D_loss: 1.7324 | G_loss: 3.4291 | Recon: 2.5281 | InfoNCE: 1.8018 | Margin: 0.0000 | Disentangle: 0.0002\n",
      "Epoch 1/50 - Batch 6/77 | D_loss: 1.7316 | G_loss: 3.2836 | Recon: 2.3829 | InfoNCE: 1.8009 | Margin: 0.0000 | Disentangle: 0.0005\n",
      "Epoch 1/50 - Batch 7/77 | D_loss: 1.7306 | G_loss: 3.1263 | Recon: 2.3476 | InfoNCE: 1.5571 | Margin: 0.0000 | Disentangle: 0.0003\n",
      "Epoch 1/50 - Batch 8/77 | D_loss: 1.7307 | G_loss: 4.0444 | Recon: 3.2086 | InfoNCE: 1.6715 | Margin: 0.0000 | Disentangle: 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   0%|          | 0/50 [00:07<?, ?epoch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Batch 9/77 | D_loss: 1.7297 | G_loss: 3.4827 | Recon: 2.7267 | InfoNCE: 1.5118 | Margin: 0.0000 | Disentangle: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-b398ef5f7850>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Avvia il training e cattura le loss curves\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_losses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_losses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-baf174d9782a>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[0mtrain_batches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# x: (B, S, 2, T, F)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m             \u001b[0mstft_part\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mSTFT_F\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# STFT part\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Francesco\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m                 \u001b[1;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 630\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    632\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Francesco\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 673\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    674\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    675\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Francesco\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Francesco\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Francesco\\Desktop\\Audio-Style-Transfer\\dataloader.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[0mcqt_p\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_CQT\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maudio_p\u001b[0m\u001b[1;33m)\u001b[0m            \u001b[1;31m# shape: (2, T, F)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[0mstft_v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_STFT\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maudio_v\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[0mcqt_v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_CQT\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maudio_v\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[1;31m# Normalize using separate statistics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Francesco\\Desktop\\Audio-Style-Transfer\\utilityFunctions.py\u001b[0m in \u001b[0;36mget_CQT\u001b[1;34m(waveform, sample_rate, n_bins, hop_length)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mwaveform\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwaveform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# remove channel dim if present\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     \u001b[0mcqt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcqt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwaveform\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_bins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_bins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhop_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhop_length\u001b[0m\u001b[1;33m)\u001b[0m    \u001b[1;31m# (freq, time), complex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;31m# convert to real and imaginary parts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Francesco\\anaconda3\\lib\\site-packages\\librosa\\core\\constantq.py\u001b[0m in \u001b[0;36mcqt\u001b[1;34m(y, sr, hop_length, fmin, n_bins, bins_per_octave, tuning, filter_scale, norm, sparsity, window, scale, pad_mode, res_type, dtype)\u001b[0m\n\u001b[0;32m    169\u001b[0m     \"\"\"\n\u001b[0;32m    170\u001b[0m     \u001b[1;31m# CQT is the special case of VQT with gamma=0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m     return vqt(\n\u001b[0m\u001b[0;32m    172\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[0msr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Francesco\\anaconda3\\lib\\site-packages\\librosa\\core\\constantq.py\u001b[0m in \u001b[0;36mvqt\u001b[1;34m(y, sr, hop_length, fmin, n_bins, intervals, gamma, bins_per_octave, tuning, filter_scale, norm, sparsity, window, scale, pad_mode, res_type, dtype)\u001b[0m\n\u001b[0;32m   1015\u001b[0m         \u001b[1;31m# Compute the vqt filter response and append to the stack\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m         vqt_resp.append(\n\u001b[1;32m-> 1017\u001b[1;33m             \u001b[0m__cqt_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmy_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_fft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmy_hop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfft_basis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpad_mode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1018\u001b[0m         )\n\u001b[0;32m   1019\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Francesco\\anaconda3\\lib\\site-packages\\librosa\\core\\constantq.py\u001b[0m in \u001b[0;36m__cqt_response\u001b[1;34m(y, n_fft, hop_length, fft_basis, mode, window, phase, dtype)\u001b[0m\n\u001b[0;32m   1123\u001b[0m     \u001b[1;34m\"\"\"Compute the filter response with a target STFT hop.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1124\u001b[0m     \u001b[1;31m# Compute the STFT matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1125\u001b[1;33m     D = stft(\n\u001b[0m\u001b[0;32m   1126\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_fft\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_fft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhop_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhop_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpad_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1127\u001b[0m     )\n",
      "\u001b[1;32mc:\\Users\\Francesco\\anaconda3\\lib\\site-packages\\librosa\\core\\spectrum.py\u001b[0m in \u001b[0;36mstft\u001b[1;34m(y, n_fft, hop_length, win_length, window, center, dtype, pad_mode, out)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m         \u001b[0mstft_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"F\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m         raise ParameterError(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Avvia il training e cattura le loss curves\n",
    "train_losses, val_losses = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db7c5d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "📊 ANALISI PARAMETRI ALLENABILI DEL MODELLO\n",
      "============================================================\n",
      "  Style Encoder: 13,169,504 parametri (13,169,504 allenabili) |   50.2 MB\n",
      "Content Encoder: 13,169,248 parametri (13,169,248 allenabili) |   50.2 MB\n",
      "        Decoder:  3,682,027 parametri ( 3,682,027 allenabili) |   14.0 MB\n",
      "  Discriminator:     49,666 parametri (    49,666 allenabili) |    0.2 MB\n",
      "------------------------------------------------------------\n",
      "         TOTALE: 30,070,445 parametri (30,070,445 allenabili)\n",
      "     DIMENSIONE:                                           114.7 MB\n",
      "   MEM. STIMATA:                                          459 MB (solo modello)\n",
      "\n",
      "============================================================\n",
      "💾 ANALISI MEMORIA PER BATCH SIZE\n",
      "============================================================\n",
      "Batch size  4:    501 MB ✅\n",
      "Batch size  8:    543 MB ✅\n",
      "Batch size 16:    626 MB ✅\n",
      "Batch size 32:    793 MB ✅\n",
      "\n",
      "============================================================\n",
      "🎯 CONFIGURAZIONE RACCOMANDATA\n",
      "============================================================\n",
      "✅ Modello di dimensioni ragionevoli per il training\n",
      "\n",
      "🚀 Per GPU con 12GB: Batch size raccomandato = 8\n",
      "🔧 Per GPU con 8GB:  Batch size raccomandato = 4\n"
     ]
    }
   ],
   "source": [
    "def count_trainable_parameters():\n",
    "    \"\"\"Conta i parametri allenabili di tutto il modello\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"📊 ANALISI PARAMETRI ALLENABILI DEL MODELLO\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Crea tutti i modelli\n",
    "    style_encoder = StyleEncoder(transformer_dim=TRANSFORMER_DIM)\n",
    "    content_encoder = ContentEncoder(transformer_dim=TRANSFORMER_DIM)\n",
    "    decoder = Decoder(\n",
    "        d_model=TRANSFORMER_DIM,\n",
    "        nhead=4,\n",
    "        num_layers=4,\n",
    "        dim_feedforward=TRANSFORMER_DIM * 2,\n",
    "        dropout=0.1\n",
    "    )\n",
    "    discriminator = Discriminator(input_dim=TRANSFORMER_DIM)\n",
    "    \n",
    "    models = [\n",
    "        (\"Style Encoder\", style_encoder),\n",
    "        (\"Content Encoder\", content_encoder),\n",
    "        (\"Decoder\", decoder),\n",
    "        (\"Discriminator\", discriminator)\n",
    "    ]\n",
    "    \n",
    "    total_params = 0\n",
    "    total_trainable_params = 0\n",
    "    \n",
    "    for name, model in models:\n",
    "        # Conta parametri totali\n",
    "        model_params = sum(p.numel() for p in model.parameters())\n",
    "        # Conta parametri allenabili\n",
    "        model_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        \n",
    "        total_params += model_params\n",
    "        total_trainable_params += model_trainable_params\n",
    "        \n",
    "        # Dimensione in MB\n",
    "        param_size_mb = sum(p.numel() * p.element_size() for p in model.parameters()) / (1024**2)\n",
    "        \n",
    "        print(f\"{name:>15}: {model_params:>10,} parametri ({model_trainable_params:>10,} allenabili) | {param_size_mb:>6.1f} MB\")\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'TOTALE':>15}: {total_params:>10,} parametri ({total_trainable_params:>10,} allenabili)\")\n",
    "    \n",
    "    # Calcola dimensioni totali\n",
    "    total_size_mb = 0\n",
    "    for name, model in models:\n",
    "        total_size_mb += sum(p.numel() * p.element_size() for p in model.parameters()) / (1024**2)\n",
    "    \n",
    "    print(f\"{'DIMENSIONE':>15}: {total_size_mb:>47.1f} MB\")\n",
    "    \n",
    "    # Stima memoria GPU per training\n",
    "    # Modello + gradients + optimizer states (Adam ~ 2x params) + attivazioni\n",
    "    estimated_gpu_memory = total_size_mb * 4  # rough estimate\n",
    "    print(f\"{'MEM. STIMATA':>15}: {estimated_gpu_memory:>44.0f} MB (solo modello)\")\n",
    "    \n",
    "    # Analisi per batch size\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"💾 ANALISI MEMORIA PER BATCH SIZE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    batch_sizes = [4, 8, 16, 32]\n",
    "    for bs in batch_sizes:\n",
    "        # Stima memoria per batch (approssimativa)\n",
    "        # Input: (B, S, 2, T, F) * 4 bytes per float32\n",
    "        input_size_mb = bs * NUM_FRAMES * 2 * STFT_T * (STFT_F + CQT_F) * 4 / (1024**2)\n",
    "        \n",
    "        # Memoria totale stimata\n",
    "        total_mem_mb = estimated_gpu_memory + input_size_mb * 2  # input + gradients\n",
    "        \n",
    "        status = \"✅\" if total_mem_mb < 8000 else \"⚠️\" if total_mem_mb < 12000 else \"❌\"\n",
    "        print(f\"Batch size {bs:>2}: {total_mem_mb:>6.0f} MB {status}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"🎯 CONFIGURAZIONE RACCOMANDATA\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if total_trainable_params < 50_000_000:  # 50M\n",
    "        print(\"✅ Modello di dimensioni ragionevoli per il training\")\n",
    "    elif total_trainable_params < 100_000_000:  # 100M\n",
    "        print(\"⚠️ Modello grande - considerare mixed precision training\")\n",
    "    else:\n",
    "        print(\"❌ Modello molto grande - ridurre dimensioni o usare tecniche avanzate\")\n",
    "    \n",
    "    # Raccomandazioni finali\n",
    "    print(f\"\\n🚀 Per GPU con 12GB: Batch size raccomandato = {8 if estimated_gpu_memory < 4000 else 4}\")\n",
    "    print(f\"🔧 Per GPU con 8GB:  Batch size raccomandato = {4 if estimated_gpu_memory < 3000 else 2}\")\n",
    "    \n",
    "    return total_trainable_params, total_size_mb\n",
    "\n",
    "# Esegui l'analisi\n",
    "total_params, model_size = count_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7b19a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
