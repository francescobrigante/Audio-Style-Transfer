{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9588c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3080\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "from style_encoder import StyleEncoder\n",
    "from content_encoder import ContentEncoder\n",
    "from discriminator import Discriminator\n",
    "from new_decoder import Decoder, compute_comprehensive_loss\n",
    "from losses import (infoNCE_loss, margin_loss, adversarial_loss, \n",
    "                   disentanglement_loss)\n",
    "from Dataloader import get_dataloader\n",
    "\n",
    "# ================================================================\n",
    "# CONFIGURAZIONE DISPOSITIVO E MEMORIA\n",
    "# ================================================================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "608babe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ================================================================\n",
    "# CONFIGURAZIONE TRAINING\n",
    "# ================================================================\n",
    "config = {\n",
    "    # Architettura - parametri originali richiesti\n",
    "    \"style_dim\": 256,\n",
    "    \"content_dim\": 256,\n",
    "    \"transformer_heads\": 4,\n",
    "    \"transformer_layers\": 4,\n",
    "    \"cnn_channels\": [16, 32, 64, 128, 256],\n",
    "    \n",
    "    # Training - parametri conservativi per stabilità\n",
    "    \"epochs\": 100,\n",
    "    \"batch_size\": 8,          # Ridotto per evitare OOM\n",
    "    \"lr\": 3e-5,              # Learning rate conservativo\n",
    "    \"beta1\": 0.5,\n",
    "    \"beta2\": 0.999,\n",
    "    \"weight_decay\": 1e-4,    # Weight decay per prevenire overfitting\n",
    "    \n",
    "    # Pesi delle loss - bilanciati per stabilità\n",
    "    \"lambda_adv_disc\": 0.8,\n",
    "    \"lambda_adv_gen\": 0.1,\n",
    "    \"lambda_disent\": 0.3,\n",
    "    \"lambda_cont\": 0.2,\n",
    "    \"lambda_margin\": 0.1,\n",
    "    \"lambda_recon\": 2.0,\n",
    "    \n",
    "    # Controlli di stabilità\n",
    "    \"grad_clip_value\": 0.5,      # Gradient clipping conservativo\n",
    "    \"warmup_epochs\": 5,          # Epochs di warmup\n",
    "    \"nan_threshold\": 5,          # Max NaN consecutivi prima di fermarsi\n",
    "    \n",
    "    # Percorsi dati\n",
    "    \"piano_dir\": \"dataset/train/piano\",\n",
    "    \"violin_dir\": \"dataset/train/violin\",\n",
    "    \"stats_path\": \"stats_stft_cqt.npz\",\n",
    "    \n",
    "    # Salvataggio\n",
    "    \"save_dir\": \"checkpoints\",\n",
    "    \"save_interval\": 10,\n",
    "    \n",
    "    # Strategia di training\n",
    "    \"discriminator_steps\": 2,\n",
    "    \"generator_steps\": 1,\n",
    "}\n",
    "\n",
    "# Crea directory di salvataggio\n",
    "os.makedirs(config[\"save_dir\"], exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5389e3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ================================================================\n",
    "# INIZIALIZZAZIONE PESI CONSERVATIVA\n",
    "# ================================================================\n",
    "def init_weights_conservative(m):\n",
    "    \"\"\"\n",
    "    Inizializzazione conservativa dei pesi per prevenire NaN\n",
    "    \"\"\"\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        # Xavier uniforme con gain ridotto\n",
    "        nn.init.xavier_uniform_(m.weight, gain=0.2)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        # Xavier uniforme con gain ridotto\n",
    "        nn.init.xavier_uniform_(m.weight, gain=0.2)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        nn.init.ones_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.LayerNorm):\n",
    "        nn.init.ones_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "# ================================================================\n",
    "# FUNZIONI DI UTILITÀ\n",
    "# ================================================================\n",
    "def check_for_nan(*tensors, names=None):\n",
    "    \"\"\"\n",
    "    Controlla se ci sono NaN o Inf nei tensori\n",
    "    \n",
    "    Args:\n",
    "        *tensors: Tensori da controllare\n",
    "        names: Nomi dei tensori per il debug\n",
    "    \n",
    "    Returns:\n",
    "        bool: True se trovati NaN/Inf\n",
    "    \"\"\"\n",
    "    if names is None:\n",
    "        names = [f\"tensor_{i}\" for i in range(len(tensors))]\n",
    "    \n",
    "    for tensor, name in zip(tensors, names):\n",
    "        if torch.isnan(tensor).any():\n",
    "            print(f\"🚨 NaN detected in {name}\")\n",
    "            return True\n",
    "        if torch.isinf(tensor).any():\n",
    "            print(f\"🚨 Inf detected in {name}\")\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def set_requires_grad(models, requires_grad):\n",
    "    \"\"\"\n",
    "    Abilita/disabilita i gradienti per i modelli\n",
    "    \n",
    "    Args:\n",
    "        models: Modello singolo o lista di modelli\n",
    "        requires_grad: True per abilitare, False per disabilitare\n",
    "    \"\"\"\n",
    "    if not isinstance(models, list):\n",
    "        models = [models]\n",
    "    for model in models:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = requires_grad\n",
    "\n",
    "def get_learning_rate_multiplier(epoch, warmup_epochs):\n",
    "    \"\"\"\n",
    "    Calcola il moltiplicatore del learning rate per il warmup\n",
    "    \n",
    "    Args:\n",
    "        epoch: Epoca corrente\n",
    "        warmup_epochs: Numero di epoche di warmup\n",
    "    \n",
    "    Returns:\n",
    "        float: Moltiplicatore del learning rate\n",
    "    \"\"\"\n",
    "    if epoch < warmup_epochs:\n",
    "        return (epoch + 1) / warmup_epochs\n",
    "    return 1.0\n",
    "\n",
    "def save_checkpoint(epoch, models_dict, optimizers_dict, schedulers_dict, config):\n",
    "    \"\"\"\n",
    "    Salva checkpoint completo\n",
    "    \n",
    "    Args:\n",
    "        epoch: Epoca corrente\n",
    "        models_dict: Dizionario dei modelli\n",
    "        optimizers_dict: Dizionario degli ottimizzatori\n",
    "        schedulers_dict: Dizionario degli schedulers\n",
    "        config: Configurazione\n",
    "    \"\"\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'config': config\n",
    "    }\n",
    "    \n",
    "    # Salva stati dei modelli\n",
    "    for name, model in models_dict.items():\n",
    "        checkpoint[name] = model.state_dict()\n",
    "    \n",
    "    # Salva stati degli ottimizzatori\n",
    "    for name, optimizer in optimizers_dict.items():\n",
    "        checkpoint[name] = optimizer.state_dict()\n",
    "    \n",
    "    # Salva stati degli schedulers\n",
    "    for name, scheduler in schedulers_dict.items():\n",
    "        checkpoint[name] = scheduler.state_dict()\n",
    "    \n",
    "    checkpoint_path = os.path.join(config[\"save_dir\"], f\"checkpoint_epoch_{epoch}.pth\")\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    print(f\"💾 Checkpoint saved: {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6491e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Initializing models...\n",
      "✅ style_encoder initialized\n",
      "✅ content_encoder initialized\n",
      "✅ discriminator initialized\n",
      "✅ decoder initialized\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ================================================================\n",
    "# INIZIALIZZAZIONE MODELLI\n",
    "# ================================================================\n",
    "print(\"🔧 Initializing models...\")\n",
    "\n",
    "style_encoder = StyleEncoder(\n",
    "    cnn_out_dim=config[\"style_dim\"],\n",
    "    transformer_dim=config[\"style_dim\"],\n",
    "    num_heads=config[\"transformer_heads\"],\n",
    "    num_layers=config[\"transformer_layers\"]\n",
    ").to(device)\n",
    "\n",
    "content_encoder = ContentEncoder(\n",
    "    cnn_out_dim=config[\"content_dim\"],\n",
    "    transformer_dim=config[\"content_dim\"],\n",
    "    num_heads=config[\"transformer_heads\"],\n",
    "    num_layers=config[\"transformer_layers\"],\n",
    "    channels_list=config[\"cnn_channels\"]\n",
    ").to(device)\n",
    "\n",
    "discriminator = Discriminator(\n",
    "    input_dim=config[\"style_dim\"],\n",
    "    hidden_dim=128\n",
    ").to(device)\n",
    "\n",
    "decoder = Decoder(\n",
    "    d_model=config[\"style_dim\"],\n",
    "    nhead=config[\"transformer_heads\"],\n",
    "    num_layers=config[\"transformer_layers\"]\n",
    ").to(device)\n",
    "\n",
    "# Applica inizializzazione conservativa\n",
    "models = [style_encoder, content_encoder, discriminator, decoder]\n",
    "model_names = [\"style_encoder\", \"content_encoder\", \"discriminator\", \"decoder\"]\n",
    "\n",
    "for model, name in zip(models, model_names):\n",
    "    model.apply(init_weights_conservative)\n",
    "    print(f\"✅ {name} initialized\")\n",
    "\n",
    "\n",
    "# ottimizzazione regolare\n",
    "# from style_encoder import initialize_weights\n",
    "# initialize_weights(style_encoder)\n",
    "# initialize_weights(content_encoder)\n",
    "# initialize_weights(decoder)\n",
    "# initialize_weights(discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64392f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Setting up optimizers and schedulers...\n",
      "🔧 Setting up dataloader...\n",
      "✅ DataLoader created successfully with batch_size=8\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# OTTIMIZZATORI E SCHEDULERS\n",
    "# ================================================================\n",
    "print(\"🔧 Setting up optimizers and schedulers...\")\n",
    "\n",
    "# Ottimizzatori con AdamW per maggiore stabilità\n",
    "optimizer_G = optim.AdamW(\n",
    "    list(style_encoder.parameters()) + \n",
    "    list(content_encoder.parameters()) + \n",
    "    list(decoder.parameters()),\n",
    "    lr=config[\"lr\"],\n",
    "    betas=(config[\"beta1\"], config[\"beta2\"]),\n",
    "    weight_decay=config[\"weight_decay\"]\n",
    ")\n",
    "\n",
    "optimizer_D = optim.AdamW(\n",
    "    discriminator.parameters(),\n",
    "    lr=config[\"lr\"],\n",
    "    betas=(config[\"beta1\"], config[\"beta2\"]),\n",
    "    weight_decay=config[\"weight_decay\"]\n",
    ")\n",
    "\n",
    "# Schedulers per learning rate adattivo\n",
    "scheduler_G = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer_G, mode='min', factor=0.7, patience=5\n",
    ")\n",
    "\n",
    "scheduler_D = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer_D, mode='min', factor=0.7, patience=5\n",
    ")\n",
    "\n",
    "# ================================================================\n",
    "# DATALOADER\n",
    "# ================================================================\n",
    "print(\"🔧 Setting up dataloader...\")\n",
    "\n",
    "try:\n",
    "    train_loader = get_dataloader(\n",
    "        piano_dir=config[\"piano_dir\"],\n",
    "        violin_dir=config[\"violin_dir\"],\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        stats_path=config[\"stats_path\"]\n",
    "    )\n",
    "    print(f\"✅ DataLoader created successfully with batch_size={config['batch_size']}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error creating DataLoader: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10a50400",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ================================================================\n",
    "# FUNZIONI DI TRAINING\n",
    "# ================================================================\n",
    "def discriminator_training_step(x, labels, epoch):\n",
    "    \"\"\"\n",
    "    Step di training per il discriminatore\n",
    "    \n",
    "    Args:\n",
    "        x: Input batch\n",
    "        labels: Labels del batch\n",
    "        epoch: Epoca corrente\n",
    "    \n",
    "    Returns:\n",
    "        float: Loss del discriminatore\n",
    "    \"\"\"\n",
    "    # Abilita gradienti solo per discriminatore\n",
    "    set_requires_grad(discriminator, True)\n",
    "    set_requires_grad([style_encoder, content_encoder, decoder], False)\n",
    "    \n",
    "    try:\n",
    "        # Forward pass senza gradienti per i generatori\n",
    "        with torch.no_grad():\n",
    "            style_emb, class_emb = style_encoder(x, labels)\n",
    "            content_emb = content_encoder(x)\n",
    "        \n",
    "        # Controllo NaN negli embeddings\n",
    "        if check_for_nan(style_emb, class_emb, content_emb, \n",
    "                         names=[\"style_emb\", \"class_emb\", \"content_emb\"]):\n",
    "            return float('nan')\n",
    "        \n",
    "        # Calcola loss del discriminatore\n",
    "        disc_loss, _ = adversarial_loss(\n",
    "            style_emb.detach(),\n",
    "            class_emb.detach(),\n",
    "            content_emb.detach(),\n",
    "            discriminator,\n",
    "            labels,\n",
    "            compute_for_discriminator=True,\n",
    "            lambda_content=config[\"lambda_adv_disc\"]\n",
    "        )\n",
    "        \n",
    "        # Controllo NaN nella loss\n",
    "        if check_for_nan(disc_loss, names=[\"disc_loss\"]):\n",
    "            return float('nan')\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer_D.zero_grad()\n",
    "        disc_loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(discriminator.parameters(), config[\"grad_clip_value\"])\n",
    "        \n",
    "        # Controllo gradienti per NaN\n",
    "        for name, param in discriminator.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                if check_for_nan(param.grad, names=[f\"discriminator.{name}.grad\"]):\n",
    "                    return float('nan')\n",
    "        \n",
    "        optimizer_D.step()\n",
    "        return disc_loss.item()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in discriminator training step: {e}\")\n",
    "        return float('nan')\n",
    "\n",
    "def generator_training_step(x, labels, epoch):\n",
    "    \"\"\"\n",
    "    Step di training per i generatori\n",
    "    \n",
    "    Args:\n",
    "        x: Input batch\n",
    "        labels: Labels del batch\n",
    "        epoch: Epoca corrente\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dizionario con tutte le loss\n",
    "    \"\"\"\n",
    "    # Abilita gradienti per i generatori\n",
    "    set_requires_grad([style_encoder, content_encoder, decoder], True)\n",
    "    set_requires_grad(discriminator, False)\n",
    "    \n",
    "    try:\n",
    "        # Forward pass\n",
    "        style_emb, class_emb = style_encoder(x, labels)\n",
    "        content_emb = content_encoder(x)\n",
    "        \n",
    "        # Controllo NaN negli embeddings\n",
    "        if check_for_nan(style_emb, class_emb, content_emb,\n",
    "                         names=[\"style_emb\", \"class_emb\", \"content_emb\"]):\n",
    "            return {key: float('nan') for key in ['total_G', 'adv_gen', 'disent', 'cont', 'margin', 'recon']}\n",
    "        \n",
    "        # Calcola loss avversariale per il generatore\n",
    "        _, adv_gen_loss = adversarial_loss(\n",
    "            style_emb,\n",
    "            class_emb,\n",
    "            content_emb,\n",
    "            discriminator,\n",
    "            labels,\n",
    "            compute_for_discriminator=False,\n",
    "            lambda_content=config[\"lambda_adv_gen\"]\n",
    "        )\n",
    "        \n",
    "        # Calcola loss di disentanglement\n",
    "        disent_loss = disentanglement_loss(\n",
    "            style_emb,\n",
    "            content_emb.mean(dim=1),\n",
    "            use_hsic=True\n",
    "        )\n",
    "        \n",
    "        # Calcola loss contrastiva\n",
    "        cont_loss = infoNCE_loss(style_emb, labels)\n",
    "        \n",
    "        # Calcola margin loss\n",
    "        margin_loss_val = margin_loss(class_emb)\n",
    "        \n",
    "        # Calcola loss di ricostruzione\n",
    "        stft_part = x[:, :, :, :, :513]\n",
    "        recon_x = decoder(content_emb, style_emb, y=stft_part)\n",
    "        recon_losses = compute_comprehensive_loss(recon_x, stft_part)\n",
    "        recon_loss = recon_losses['total_loss']\n",
    "        \n",
    "        # Controllo NaN in tutte le loss\n",
    "        losses = [adv_gen_loss, disent_loss, cont_loss, margin_loss_val, recon_loss]\n",
    "        loss_names = ['adv_gen_loss', 'disent_loss', 'cont_loss', 'margin_loss', 'recon_loss']\n",
    "        \n",
    "        if check_for_nan(*losses, names=loss_names):\n",
    "            return {key: float('nan') for key in ['total_G', 'adv_gen', 'disent', 'cont', 'margin', 'recon']}\n",
    "        \n",
    "        # Calcola loss totale con warmup\n",
    "        lr_multiplier = get_learning_rate_multiplier(epoch, config[\"warmup_epochs\"])\n",
    "        warmup_factor = lr_multiplier if epoch < config[\"warmup_epochs\"] else 1.0\n",
    "        \n",
    "        total_G_loss = (\n",
    "            config[\"lambda_adv_gen\"] * adv_gen_loss * warmup_factor +\n",
    "            config[\"lambda_disent\"] * disent_loss * warmup_factor +\n",
    "            config[\"lambda_cont\"] * cont_loss * warmup_factor +\n",
    "            config[\"lambda_margin\"] * margin_loss_val * warmup_factor +\n",
    "            config[\"lambda_recon\"] * recon_loss\n",
    "        )\n",
    "        \n",
    "        # Controllo NaN nella loss totale\n",
    "        if check_for_nan(total_G_loss, names=[\"total_G_loss\"]):\n",
    "            return {key: float('nan') for key in ['total_G', 'adv_gen', 'disent', 'cont', 'margin', 'recon']}\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer_G.zero_grad()\n",
    "        total_G_loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        generator_params = (\n",
    "            list(style_encoder.parameters()) + \n",
    "            list(content_encoder.parameters()) +\n",
    "            list(decoder.parameters())\n",
    "        )\n",
    "        torch.nn.utils.clip_grad_norm_(generator_params, config[\"grad_clip_value\"])\n",
    "        \n",
    "        # Controllo gradienti per NaN\n",
    "        for model, model_name in [(style_encoder, \"style_encoder\"), \n",
    "                                 (content_encoder, \"content_encoder\"), \n",
    "                                 (decoder, \"decoder\")]:\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    if check_for_nan(param.grad, names=[f\"{model_name}.{name}.grad\"]):\n",
    "                        return {key: float('nan') for key in ['total_G', 'adv_gen', 'disent', 'cont', 'margin', 'recon']}\n",
    "        \n",
    "        optimizer_G.step()\n",
    "        \n",
    "        return {\n",
    "            'total_G': total_G_loss.item(),\n",
    "            'adv_gen': adv_gen_loss.item(),\n",
    "            'disent': disent_loss.item(),\n",
    "            'cont': cont_loss.item(),\n",
    "            'margin': margin_loss_val.item(),\n",
    "            'recon': recon_loss.item()\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in generator training step: {e}\")\n",
    "        return {key: float('nan') for key in ['total_G', 'adv_gen', 'disent', 'cont', 'margin', 'recon']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cef9ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting training with enhanced NaN protection...\n",
      "📊 Configuration: {'style_dim': 256, 'content_dim': 256, 'transformer_heads': 4, 'transformer_layers': 4, 'cnn_channels': [16, 32, 64, 128, 256], 'epochs': 100, 'batch_size': 8, 'lr': 3e-05, 'beta1': 0.5, 'beta2': 0.999, 'weight_decay': 0.0001, 'lambda_adv_disc': 0.8, 'lambda_adv_gen': 0.1, 'lambda_disent': 0.3, 'lambda_cont': 0.2, 'lambda_margin': 0.1, 'lambda_recon': 2.0, 'grad_clip_value': 0.5, 'warmup_epochs': 5, 'nan_threshold': 5, 'piano_dir': 'dataset/train/piano', 'violin_dir': 'dataset/train/violin', 'stats_path': 'stats_stft_cqt.npz', 'save_dir': 'checkpoints', 'save_interval': 10, 'discriminator_steps': 2, 'generator_steps': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔄 Epoch 1/100\n",
      "------------------------------------------------------------\n",
      "\n",
      "📊 Batch 1:\n",
      "   Discriminator Loss: 1.594134\n",
      "   Generator Total Loss: 7.922220\n",
      "     Adv Gen: -0.693144\n",
      "     Disent: 0.004668\n",
      "     Cont: 1.930429\n",
      "     Margin: 0.000000\n",
      "     Recon: 3.929293\n",
      "   Consecutive NaN count: 0\n",
      "   GPU Memory: 392.8 MB\n",
      "\n",
      "📊 Batch 2:\n",
      "   Discriminator Loss: 1.593978\n",
      "   Generator Total Loss: 7.215709\n",
      "     Adv Gen: -0.693145\n",
      "     Disent: 0.001488\n",
      "     Cont: 1.920036\n",
      "     Margin: 0.000000\n",
      "     Recon: 3.576340\n",
      "   Consecutive NaN count: 0\n",
      "   GPU Memory: 392.4 MB\n",
      "\n",
      "📊 Batch 3:\n",
      "   Discriminator Loss: 1.593538\n",
      "   Generator Total Loss: 6.371064\n",
      "     Adv Gen: -0.693146\n",
      "     Disent: 0.000555\n",
      "     Cont: 1.915881\n",
      "     Margin: 0.000000\n",
      "     Recon: 3.154129\n",
      "   Consecutive NaN count: 0\n",
      "   GPU Memory: 392.4 MB\n",
      "\n",
      "📊 Batch 4:\n",
      "   Discriminator Loss: 1.593152\n",
      "   Generator Total Loss: 7.179996\n",
      "     Adv Gen: -0.693146\n",
      "     Disent: 0.000518\n",
      "     Cont: 1.886470\n",
      "     Margin: 0.000000\n",
      "     Recon: 3.559184\n",
      "   Consecutive NaN count: 0\n",
      "   GPU Memory: 392.4 MB\n",
      "\n",
      "📊 Batch 5:\n",
      "   Discriminator Loss: 1.592819\n",
      "   Generator Total Loss: 6.607413\n",
      "     Adv Gen: -0.693146\n",
      "     Disent: 0.000671\n",
      "     Cont: 1.905966\n",
      "     Margin: 0.000000\n",
      "     Recon: 3.272498\n",
      "   Consecutive NaN count: 0\n",
      "   GPU Memory: 392.4 MB\n",
      "\n",
      "📊 Batch 6:\n",
      "   Discriminator Loss: 1.591052\n",
      "   Generator Total Loss: 5.415570\n",
      "     Adv Gen: -0.693145\n",
      "     Disent: 0.000510\n",
      "     Cont: 1.768983\n",
      "     Margin: 0.000000\n",
      "     Recon: 2.679322\n",
      "   Consecutive NaN count: 0\n",
      "   GPU Memory: 392.4 MB\n",
      "\n",
      "📊 Batch 7:\n",
      "   Discriminator Loss: 1.588414\n",
      "   Generator Total Loss: 8.770458\n",
      "     Adv Gen: -0.693145\n",
      "     Disent: 0.000408\n",
      "     Cont: 1.547709\n",
      "     Margin: 0.000000\n",
      "     Recon: 4.361194\n",
      "   Consecutive NaN count: 0\n",
      "   GPU Memory: 392.4 MB\n",
      "\n",
      "📊 Batch 8:\n",
      "   Discriminator Loss: 1.586682\n",
      "   Generator Total Loss: 7.449286\n",
      "     Adv Gen: -0.693146\n",
      "     Disent: 0.000183\n",
      "     Cont: 1.496971\n",
      "     Margin: 0.000000\n",
      "     Recon: 3.701630\n",
      "   Consecutive NaN count: 0\n",
      "   GPU Memory: 392.4 MB\n",
      "\n",
      "📊 Batch 9:\n",
      "   Discriminator Loss: 1.583987\n",
      "   Generator Total Loss: 6.477983\n",
      "     Adv Gen: -0.693145\n",
      "     Disent: 0.000182\n",
      "     Cont: 1.389642\n",
      "     Margin: 0.000000\n",
      "     Recon: 3.218125\n",
      "   Consecutive NaN count: 0\n",
      "   GPU Memory: 392.4 MB\n",
      "\n",
      "📊 Batch 10:\n",
      "   Discriminator Loss: 1.580377\n",
      "   Generator Total Loss: 6.757029\n",
      "     Adv Gen: -0.693146\n",
      "     Disent: 0.000172\n",
      "     Cont: 1.308427\n",
      "     Margin: 0.000000\n",
      "     Recon: 3.359272\n",
      "   Consecutive NaN count: 0\n",
      "   GPU Memory: 392.4 MB\n",
      "\n",
      "📊 Batch 11:\n",
      "   Discriminator Loss: 1.587811\n",
      "   Generator Total Loss: 19.341301\n",
      "     Adv Gen: -0.693146\n",
      "     Disent: 0.000103\n",
      "     Cont: 2.346543\n",
      "     Margin: 0.000000\n",
      "     Recon: 9.630648\n",
      "   Consecutive NaN count: 0\n",
      "   GPU Memory: 392.4 MB\n",
      "\n",
      "📊 Batch 12:\n",
      "   Discriminator Loss: 1.578271\n",
      "   Generator Total Loss: 14.136860\n",
      "     Adv Gen: -0.693146\n",
      "     Disent: 0.000189\n",
      "     Cont: 1.550425\n",
      "     Margin: 0.000000\n",
      "     Recon: 7.044347\n",
      "   Consecutive NaN count: 0\n",
      "   GPU Memory: 392.4 MB\n",
      "\n",
      "📊 Batch 13:\n",
      "   Discriminator Loss: 1.582467\n",
      "   Generator Total Loss: 8.131932\n",
      "     Adv Gen: -0.693146\n",
      "     Disent: 0.000252\n",
      "     Cont: 1.696745\n",
      "     Margin: 0.000000\n",
      "     Recon: 4.038955\n",
      "   Consecutive NaN count: 0\n",
      "   GPU Memory: 392.4 MB\n",
      "\n",
      "📊 Batch 14:\n",
      "   Discriminator Loss: 1.578697\n",
      "   Generator Total Loss: 7.899668\n",
      "     Adv Gen: -0.693145\n",
      "     Disent: 0.000253\n",
      "     Cont: 1.642721\n",
      "     Margin: 0.000000\n",
      "     Recon: 3.923903\n",
      "   Consecutive NaN count: 0\n",
      "   GPU Memory: 392.4 MB\n",
      "\n",
      "📊 Batch 15:\n",
      "   Discriminator Loss: 1.575907\n",
      "   Generator Total Loss: 7.856606\n",
      "     Adv Gen: -0.693145\n",
      "     Disent: 0.000187\n",
      "     Cont: 1.473144\n",
      "     Margin: 0.000000\n",
      "     Recon: 3.905766\n",
      "   Consecutive NaN count: 0\n",
      "   GPU Memory: 392.4 MB\n",
      "\n",
      "📊 Batch 16:\n",
      "   Discriminator Loss: 1.572331\n",
      "   Generator Total Loss: 8.509715\n",
      "     Adv Gen: -0.693145\n",
      "     Disent: 0.000248\n",
      "     Cont: 1.417494\n",
      "     Margin: 0.000000\n",
      "     Recon: 4.233432\n",
      "   Consecutive NaN count: 0\n",
      "   GPU Memory: 392.4 MB\n",
      "\n",
      "📊 Batch 17:\n",
      "   Discriminator Loss: 1.565153\n",
      "   Generator Total Loss: 4.851913\n",
      "     Adv Gen: -0.693143\n",
      "     Disent: 0.000204\n",
      "     Cont: 1.152672\n",
      "     Margin: 0.000000\n",
      "     Recon: 2.409829\n",
      "   Consecutive NaN count: 0\n",
      "   GPU Memory: 392.4 MB\n",
      "\n",
      "📊 Batch 18:\n",
      "   Discriminator Loss: 1.562813\n",
      "   Generator Total Loss: 5.951996\n",
      "     Adv Gen: -0.693143\n",
      "     Disent: 0.000189\n",
      "     Cont: 1.214792\n",
      "     Margin: 0.000000\n",
      "     Recon: 2.958628\n",
      "   Consecutive NaN count: 0\n",
      "   GPU Memory: 392.4 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 0/100 [00:14<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Batch 19:\n",
      "   Discriminator Loss: 1.557698\n",
      "   Generator Total Loss: 10.662372\n",
      "     Adv Gen: -0.693141\n",
      "     Disent: 0.000123\n",
      "     Cont: 1.171685\n",
      "     Margin: 0.000000\n",
      "     Recon: 5.314680\n",
      "   Consecutive NaN count: 0\n",
      "   GPU Memory: 392.4 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⏹️  Training interrupted by user\n",
      "\n",
      "💾 Saving final checkpoint...\n",
      "💾 Checkpoint saved: checkpoints\\checkpoint_epoch_1.pth\n",
      "\n",
      "🎯 Training completed!\n",
      "   Total epochs processed: 1\n",
      "   Final consecutive NaN count: 0\n",
      "   Checkpoints saved in: checkpoints\n",
      "   Final GPU memory: 392.4 MB\n",
      "🎉 Training session finished successfully!\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# CICLO DI TRAINING PRINCIPALE\n",
    "# ================================================================\n",
    "print(\"🚀 Starting training with enhanced NaN protection...\")\n",
    "print(f\"📊 Configuration: {config}\")\n",
    "\n",
    "# Inizializza strutture per il tracking delle loss\n",
    "loss_history = {\n",
    "    'total_G': [],\n",
    "    'disc': [],\n",
    "    'disent': [],\n",
    "    'cont': [],\n",
    "    'margin': [],\n",
    "    'recon': [],\n",
    "    'adv_gen': []\n",
    "}\n",
    "\n",
    "# Contatore per NaN consecutivi\n",
    "consecutive_nan_count = 0\n",
    "max_consecutive_nans = config[\"nan_threshold\"]\n",
    "\n",
    "# Dizionari per salvataggio\n",
    "models_dict = {\n",
    "    'style_encoder': style_encoder,\n",
    "    'content_encoder': content_encoder,\n",
    "    'discriminator': discriminator,\n",
    "    'decoder': decoder\n",
    "}\n",
    "\n",
    "optimizers_dict = {\n",
    "    'optimizer_G': optimizer_G,\n",
    "    'optimizer_D': optimizer_D\n",
    "}\n",
    "\n",
    "schedulers_dict = {\n",
    "    'scheduler_G': scheduler_G,\n",
    "    'scheduler_D': scheduler_D\n",
    "}\n",
    "\n",
    "try:\n",
    "    for epoch in tqdm(range(config[\"epochs\"]), desc=\"Training Progress\"):\n",
    "        # Controllo early stopping per NaN\n",
    "        if consecutive_nan_count >= max_consecutive_nans:\n",
    "            print(f\"🛑 Early stopping: {consecutive_nan_count} consecutive NaN occurrences\")\n",
    "            break\n",
    "        \n",
    "        # Imposta tutti i modelli in modalità training\n",
    "        for model in models:\n",
    "            model.train()\n",
    "        \n",
    "        # Tracking delle loss per l'epoca corrente\n",
    "        epoch_losses = {key: [] for key in loss_history.keys()}\n",
    "        \n",
    "        print(f\"\\n🔄 Epoch {epoch+1}/{config['epochs']}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for batch_idx, (x, labels) in enumerate(train_loader):\n",
    "            # Trasferisce dati su GPU\n",
    "            x = x.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Controllo NaN nei dati di input\n",
    "            if check_for_nan(x, labels, names=[\"input_x\", \"input_labels\"]):\n",
    "                print(f\"⚠️  Skipping batch {batch_idx} due to NaN in input data\")\n",
    "                continue\n",
    "            \n",
    "            # ============================================\n",
    "            # TRAINING DISCRIMINATORE\n",
    "            # ============================================\n",
    "            for _ in range(config[\"discriminator_steps\"]):\n",
    "                disc_loss = discriminator_training_step(x, labels, epoch)\n",
    "                \n",
    "                if not np.isnan(disc_loss):\n",
    "                    epoch_losses['disc'].append(disc_loss)\n",
    "                    consecutive_nan_count = 0  # Reset counter su successo\n",
    "                else:\n",
    "                    consecutive_nan_count += 1\n",
    "                    print(f\"⚠️  NaN in discriminator loss (consecutive: {consecutive_nan_count})\")\n",
    "            \n",
    "            # ============================================\n",
    "            # TRAINING GENERATORE\n",
    "            # ============================================\n",
    "            for _ in range(config[\"generator_steps\"]):\n",
    "                gen_losses = generator_training_step(x, labels, epoch)\n",
    "                \n",
    "                # Controlla se ci sono NaN nelle loss del generatore\n",
    "                nan_in_gen = any(np.isnan(v) for v in gen_losses.values())\n",
    "                \n",
    "                if not nan_in_gen:\n",
    "                    # Aggiungi loss valide alla storia\n",
    "                    for key, value in gen_losses.items():\n",
    "                        if key in epoch_losses:\n",
    "                            epoch_losses[key].append(value)\n",
    "                    consecutive_nan_count = 0  # Reset counter su successo\n",
    "                else:\n",
    "                    consecutive_nan_count += 1\n",
    "                    print(f\"⚠️  NaN in generator losses (consecutive: {consecutive_nan_count})\")\n",
    "            \n",
    "            # ============================================\n",
    "            # LOGGING PER BATCH\n",
    "            # ============================================\n",
    "            # Stampa loss dettagliate per ogni batch\n",
    "            if batch_idx % 25 == 0:\n",
    "                print(f\"\\n📊 Batch {batch_idx + 1}:\")\n",
    "                \n",
    "                # Loss del discriminatore\n",
    "                if epoch_losses['disc']:\n",
    "                    recent_disc = epoch_losses['disc'][-config[\"discriminator_steps\"]:]\n",
    "                    avg_disc = np.mean(recent_disc)\n",
    "                    print(f\"   Discriminator Loss: {avg_disc:.6f}\")\n",
    "                \n",
    "                # Loss del generatore\n",
    "                if epoch_losses['total_G']:\n",
    "                    recent_gen = epoch_losses['total_G'][-config[\"generator_steps\"]:]\n",
    "                    avg_total_G = np.mean(recent_gen)\n",
    "                    print(f\"   Generator Total Loss: {avg_total_G:.6f}\")\n",
    "                \n",
    "                # Loss individuali del generatore\n",
    "                gen_loss_names = ['adv_gen', 'disent', 'cont', 'margin', 'recon']\n",
    "                for loss_name in gen_loss_names:\n",
    "                    if epoch_losses[loss_name]:\n",
    "                        recent_loss = epoch_losses[loss_name][-config[\"generator_steps\"]:]\n",
    "                        avg_loss = np.mean(recent_loss)\n",
    "                        print(f\"     {loss_name.replace('_', ' ').title()}: {avg_loss:.6f}\")\n",
    "                \n",
    "                # Informazioni aggiuntive\n",
    "                print(f\"   Consecutive NaN count: {consecutive_nan_count}\")\n",
    "                if torch.cuda.is_available():\n",
    "                    print(f\"   GPU Memory: {torch.cuda.memory_allocated()/1024**2:.1f} MB\")\n",
    "            \n",
    "            # Controllo early stopping durante il batch\n",
    "            if consecutive_nan_count >= max_consecutive_nans:\n",
    "                print(f\"🛑 Stopping epoch {epoch+1} due to consecutive NaN issues\")\n",
    "                break\n",
    "        \n",
    "        # ============================================\n",
    "        # FINE EPOCA: AGGIORNAMENTI E SALVATAGGIO\n",
    "        # ============================================\n",
    "        if consecutive_nan_count < max_consecutive_nans:\n",
    "            # Aggiorna schedulers\n",
    "            if epoch_losses['disc']:\n",
    "                scheduler_D.step(np.mean(epoch_losses['disc']))\n",
    "            if epoch_losses['total_G']:\n",
    "                scheduler_G.step(np.mean(epoch_losses['total_G']))\n",
    "            \n",
    "            # Aggiungi loss dell'epoca alla storia globale\n",
    "            for key in loss_history.keys():\n",
    "                if epoch_losses[key]:\n",
    "                    loss_history[key].extend(epoch_losses[key])\n",
    "            \n",
    "            # Salva checkpoint periodicamente\n",
    "            if (epoch + 1) % config[\"save_interval\"] == 0:\n",
    "                save_checkpoint(epoch + 1, models_dict, optimizers_dict, schedulers_dict, config)\n",
    "            \n",
    "            # Riepilogo dell'epoca\n",
    "            print(f\"\\n✅ Epoch {epoch+1} Summary:\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            for key in ['disc', 'total_G', 'adv_gen', 'disent', 'cont', 'margin', 'recon']:\n",
    "                if epoch_losses[key]:\n",
    "                    avg_loss = np.mean(epoch_losses[key])\n",
    "                    print(f\"   {key.replace('_', ' ').title()}: {avg_loss:.6f}\")\n",
    "            \n",
    "            # Informazioni sui learning rates\n",
    "            current_lr_G = optimizer_G.param_groups[0]['lr']\n",
    "            current_lr_D = optimizer_D.param_groups[0]['lr']\n",
    "            print(f\"   LR Generator: {current_lr_G:.2e}\")\n",
    "            print(f\"   LR Discriminator: {current_lr_D:.2e}\")\n",
    "            \n",
    "            print(\"-\" * 60)\n",
    "        \n",
    "        # Pulizia memoria periodica\n",
    "        if torch.cuda.is_available() and epoch % 10 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n⏹️  Training interrupted by user\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Training error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "finally:\n",
    "    # Salvataggio finale\n",
    "    print(\"\\n💾 Saving final checkpoint...\")\n",
    "    save_checkpoint(epoch + 1, models_dict, optimizers_dict, schedulers_dict, config)\n",
    "    \n",
    "    # Visualizzazione delle loss\n",
    "    if any(loss_history[key] for key in loss_history.keys()):\n",
    "        print(\"📊 Generating loss plots...\")\n",
    "        \n",
    "        # Configura plot\n",
    "        plt.style.use('default')\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        # Definisci le loss da plottare\n",
    "        loss_configs = [\n",
    "            ('disc', 'Discriminator Loss', 'red'),\n",
    "            ('total_G', 'Generator Total Loss', 'blue'),\n",
    "            ('recon', 'Reconstruction Loss', 'green'),\n",
    "            ('adv_gen', 'Adversarial Generator Loss', 'orange'),\n",
    "            ('disent', 'Disentanglement Loss', 'purple'),\n",
    "            ('cont', 'Contrastive Loss', 'brown')\n",
    "        ]\n",
    "        \n",
    "        for i, (loss_name, title, color) in enumerate(loss_configs):\n",
    "            if loss_history[loss_name]:\n",
    "                values = loss_history[loss_name]\n",
    "                \n",
    "                # Plot raw values\n",
    "                axes[i].plot(values, alpha=0.4, color=color, linewidth=0.5, label='Raw')\n",
    "                \n",
    "                # Plot smoothed values se ci sono abbastanza punti\n",
    "                if len(values) > 50:\n",
    "                    window = max(1, len(values) // 50)\n",
    "                    smoothed = np.convolve(values, np.ones(window)/window, mode='valid')\n",
    "                    axes[i].plot(smoothed, color=color, linewidth=2, label='Smoothed')\n",
    "                \n",
    "                axes[i].set_title(title, fontsize=12, fontweight='bold')\n",
    "                axes[i].set_xlabel('Iteration')\n",
    "                axes[i].set_ylabel('Loss')\n",
    "                axes[i].legend()\n",
    "                axes[i].grid(True, alpha=0.3)\n",
    "                axes[i].set_xlim(0, len(values))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plot_path = os.path.join(config[\"save_dir\"], \"loss_curves.png\")\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(f\"📈 Loss curves saved to: {plot_path}\")\n",
    "    \n",
    "    # Riepilogo finale\n",
    "    print(f\"\\n🎯 Training completed!\")\n",
    "    print(f\"   Total epochs processed: {epoch + 1}\")\n",
    "    print(f\"   Final consecutive NaN count: {consecutive_nan_count}\")\n",
    "    print(f\"   Checkpoints saved in: {config['save_dir']}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"   Final GPU memory: {torch.cuda.memory_allocated()/1024**2:.1f} MB\")\n",
    "    \n",
    "    print(\"🎉 Training session finished successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80a7039",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
