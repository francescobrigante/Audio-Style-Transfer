{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9588c62",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] Impossibile trovare il percorso specificato: 'C:\\\\Users\\\\Lucia\\\\Desktop\\\\Uni\\\\DL\\\\Dataset\\\\DATASET_partitioned\\\\test\\\\PianoMotion10M_ready'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-96223b14db36>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m from losses import (infoNCE_loss, margin_loss, adversarial_loss, \n\u001b[0;32m     16\u001b[0m                    disentanglement_loss)\n\u001b[1;32m---> 17\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mdataloader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_dataloader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# ================================================================\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Francesco\\Desktop\\Audio-Style-Transfer\\dataloader.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m dataset = DualInstrumentDataset(\n\u001b[0m\u001b[0;32m     91\u001b[0m     \u001b[0mpiano_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mr\"C:\\Users\\Lucia\\Desktop\\Uni\\DL\\Dataset\\DATASET_partitioned\\test\\PianoMotion10M_ready\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[0mviolin_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mr\"C:\\Users\\Lucia\\Desktop\\Uni\\DL\\Dataset\\DATASET_partitioned\\test\\Bach+ViolinEtudes_44khz\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Francesco\\Desktop\\Audio-Style-Transfer\\dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, piano_dir, violin_dir)\u001b[0m\n\u001b[0;32m     18\u001b[0m         self.piano_files = sorted([\n\u001b[0;32m     19\u001b[0m             \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpiano_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpiano_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".mp3\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".wav\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         ])\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] Impossibile trovare il percorso specificato: 'C:\\\\Users\\\\Lucia\\\\Desktop\\\\Uni\\\\DL\\\\Dataset\\\\DATASET_partitioned\\\\test\\\\PianoMotion10M_ready'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "from style_encoder import StyleEncoder\n",
    "from content_encoder import ContentEncoder\n",
    "from discriminator import Discriminator\n",
    "from new_decoder import Decoder, compute_comprehensive_loss\n",
    "from losses import (infoNCE_loss, margin_loss, adversarial_loss, \n",
    "                   disentanglement_loss)\n",
    "from dataloader import get_dataloader\n",
    "\n",
    "# ================================================================\n",
    "# CONFIGURAZIONE DISPOSITIVO E MEMORIA\n",
    "# ================================================================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608babe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ================================================================\n",
    "# CONFIGURAZIONE TRAINING\n",
    "# ================================================================\n",
    "config = {\n",
    "    # Architettura - parametri originali richiesti\n",
    "    \"style_dim\": 256,\n",
    "    \"content_dim\": 256,\n",
    "    \"transformer_heads\": 4,\n",
    "    \"transformer_layers\": 4,\n",
    "    \"cnn_channels\": [16, 32, 64, 128, 256],\n",
    "    \n",
    "    # Training - parametri conservativi per stabilità\n",
    "    \"epochs\": 100,\n",
    "    \"batch_size\": 8,          # Ridotto per evitare OOM\n",
    "    \"lr_gen\": 3e-5,              # Learning rate conservativo\n",
    "    \"lr_disc\": 1e-5,              # Learning rate conservativo\n",
    "    \"beta1\": 0.5,\n",
    "    \"beta2\": 0.999,\n",
    "    \"weight_decay\": 1e-4,    # Weight decay per prevenire overfitting\n",
    "    \n",
    "    # Pesi delle loss - bilanciati per stabilità\n",
    "    \"lambda_adv_disc\": 0.5,\n",
    "    \"lambda_adv_gen\": 0.3,\n",
    "    \"lambda_disent\": 0.8,\n",
    "    \"lambda_cont\": 0.1,\n",
    "    \"lambda_margin\": 0.05,\n",
    "    \"lambda_recon\": 5.0,\n",
    "    \n",
    "    # Controlli di stabilità\n",
    "    \"grad_clip_value\": 0.5,      # Gradient clipping conservativo\n",
    "    \"warmup_epochs\": 5,          # Epochs di warmup\n",
    "    \"nan_threshold\": 5,          # Max NaN consecutivi prima di fermarsi\n",
    "    \n",
    "    # Percorsi dati\n",
    "    \"piano_dir\": \"dataset/train/piano\",\n",
    "    \"violin_dir\": \"dataset/train/violin\",\n",
    "    \"stats_path\": \"stats_stft_cqt.npz\",\n",
    "    \n",
    "    # Salvataggio\n",
    "    \"save_dir\": \"checkpoints\",\n",
    "    \"save_interval\": 10,\n",
    "    \n",
    "    # Strategia di training\n",
    "    \"discriminator_steps\": 1,\n",
    "    \"generator_steps\": 2,\n",
    "}\n",
    "\n",
    "# Crea directory di salvataggio\n",
    "os.makedirs(config[\"save_dir\"], exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5389e3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ================================================================\n",
    "# INIZIALIZZAZIONE PESI CONSERVATIVA\n",
    "# ================================================================\n",
    "def init_weights_conservative(m):\n",
    "    \"\"\"\n",
    "    Inizializzazione conservativa dei pesi per prevenire NaN\n",
    "    \"\"\"\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        # Xavier uniforme con gain ridotto\n",
    "        nn.init.xavier_uniform_(m.weight, gain=0.2)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        # Xavier uniforme con gain ridotto\n",
    "        nn.init.xavier_uniform_(m.weight, gain=0.2)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        nn.init.ones_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.LayerNorm):\n",
    "        nn.init.ones_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "# ================================================================\n",
    "# FUNZIONI DI UTILITÀ\n",
    "# ================================================================\n",
    "def check_for_nan(*tensors, names=None):\n",
    "    \"\"\"\n",
    "    Controlla se ci sono NaN o Inf nei tensori\n",
    "    \n",
    "    Args:\n",
    "        *tensors: Tensori da controllare\n",
    "        names: Nomi dei tensori per il debug\n",
    "    \n",
    "    Returns:\n",
    "        bool: True se trovati NaN/Inf\n",
    "    \"\"\"\n",
    "    if names is None:\n",
    "        names = [f\"tensor_{i}\" for i in range(len(tensors))]\n",
    "    \n",
    "    for tensor, name in zip(tensors, names):\n",
    "        if torch.isnan(tensor).any():\n",
    "            print(f\"🚨 NaN detected in {name}\")\n",
    "            return True\n",
    "        if torch.isinf(tensor).any():\n",
    "            print(f\"🚨 Inf detected in {name}\")\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def set_requires_grad(models, requires_grad):\n",
    "    \"\"\"\n",
    "    Abilita/disabilita i gradienti per i modelli\n",
    "    \n",
    "    Args:\n",
    "        models: Modello singolo o lista di modelli\n",
    "        requires_grad: True per abilitare, False per disabilitare\n",
    "    \"\"\"\n",
    "    if not isinstance(models, list):\n",
    "        models = [models]\n",
    "    for model in models:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = requires_grad\n",
    "\n",
    "def get_learning_rate_multiplier(epoch, warmup_epochs):\n",
    "    \"\"\"\n",
    "    Calcola il moltiplicatore del learning rate per il warmup\n",
    "    \n",
    "    Args:\n",
    "        epoch: Epoca corrente\n",
    "        warmup_epochs: Numero di epoche di warmup\n",
    "    \n",
    "    Returns:\n",
    "        float: Moltiplicatore del learning rate\n",
    "    \"\"\"\n",
    "    if epoch < warmup_epochs:\n",
    "        return (epoch + 1) / warmup_epochs\n",
    "    return 1.0\n",
    "\n",
    "def save_checkpoint(epoch, models_dict, optimizers_dict, schedulers_dict, config):\n",
    "    \"\"\"\n",
    "    Salva checkpoint completo\n",
    "    \n",
    "    Args:\n",
    "        epoch: Epoca corrente\n",
    "        models_dict: Dizionario dei modelli\n",
    "        optimizers_dict: Dizionario degli ottimizzatori\n",
    "        schedulers_dict: Dizionario degli schedulers\n",
    "        config: Configurazione\n",
    "    \"\"\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'config': config\n",
    "    }\n",
    "    \n",
    "    # Salva stati dei modelli\n",
    "    for name, model in models_dict.items():\n",
    "        checkpoint[name] = model.state_dict()\n",
    "    \n",
    "    # Salva stati degli ottimizzatori\n",
    "    for name, optimizer in optimizers_dict.items():\n",
    "        checkpoint[name] = optimizer.state_dict()\n",
    "    \n",
    "    # Salva stati degli schedulers\n",
    "    for name, scheduler in schedulers_dict.items():\n",
    "        checkpoint[name] = scheduler.state_dict()\n",
    "    \n",
    "    checkpoint_path = os.path.join(config[\"save_dir\"], f\"checkpoint_epoch_{epoch}.pth\")\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    print(f\"💾 Checkpoint saved: {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6491e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ================================================================\n",
    "# INIZIALIZZAZIONE MODELLI\n",
    "# ================================================================\n",
    "print(\"🔧 Initializing models...\")\n",
    "\n",
    "style_encoder = StyleEncoder(\n",
    "    cnn_out_dim=config[\"style_dim\"],\n",
    "    transformer_dim=config[\"style_dim\"],\n",
    "    num_heads=config[\"transformer_heads\"],\n",
    "    num_layers=config[\"transformer_layers\"]\n",
    ").to(device)\n",
    "\n",
    "content_encoder = ContentEncoder(\n",
    "    cnn_out_dim=config[\"content_dim\"],\n",
    "    transformer_dim=config[\"content_dim\"],\n",
    "    num_heads=config[\"transformer_heads\"],\n",
    "    num_layers=config[\"transformer_layers\"],\n",
    "    channels_list=config[\"cnn_channels\"]\n",
    ").to(device)\n",
    "\n",
    "discriminator = Discriminator(\n",
    "    input_dim=config[\"style_dim\"],\n",
    "    hidden_dim=128\n",
    ").to(device)\n",
    "\n",
    "decoder = Decoder(\n",
    "    d_model=config[\"style_dim\"],\n",
    "    nhead=config[\"transformer_heads\"],\n",
    "    num_layers=config[\"transformer_layers\"]\n",
    ").to(device)\n",
    "\n",
    "# Applica inizializzazione conservativa\n",
    "models = [style_encoder, content_encoder, discriminator, decoder]\n",
    "model_names = [\"style_encoder\", \"content_encoder\", \"discriminator\", \"decoder\"]\n",
    "\n",
    "for model, name in zip(models, model_names):\n",
    "    model.apply(init_weights_conservative)\n",
    "    print(f\"✅ {name} initialized\")\n",
    "\n",
    "\n",
    "# ottimizzazione regolare\n",
    "# from style_encoder import initialize_weights\n",
    "# initialize_weights(style_encoder)\n",
    "# initialize_weights(content_encoder)\n",
    "# initialize_weights(decoder)\n",
    "# initialize_weights(discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64392f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# OTTIMIZZATORI E SCHEDULERS\n",
    "# ================================================================\n",
    "print(\"🔧 Setting up optimizers and schedulers...\")\n",
    "\n",
    "# Ottimizzatori con AdamW per maggiore stabilità\n",
    "optimizer_G = optim.AdamW(\n",
    "    list(style_encoder.parameters()) + \n",
    "    list(content_encoder.parameters()) + \n",
    "    list(decoder.parameters()),\n",
    "    lr=config[\"lr_gen\"],\n",
    "    betas=(config[\"beta1\"], config[\"beta2\"]),\n",
    "    weight_decay=config[\"weight_decay\"]\n",
    ")\n",
    "\n",
    "optimizer_D = optim.AdamW(\n",
    "    discriminator.parameters(),\n",
    "    lr=config[\"lr_disc\"],\n",
    "    betas=(config[\"beta1\"], config[\"beta2\"]),\n",
    "    weight_decay=config[\"weight_decay\"]\n",
    ")\n",
    "\n",
    "# Schedulers per learning rate adattivo\n",
    "scheduler_G = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer_G, mode='min', factor=0.7, patience=5\n",
    ")\n",
    "\n",
    "scheduler_D = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer_D, mode='min', factor=0.7, patience=5\n",
    ")\n",
    "\n",
    "# ================================================================\n",
    "# DATALOADER\n",
    "# ================================================================\n",
    "print(\"🔧 Setting up dataloader...\")\n",
    "\n",
    "try:\n",
    "    train_loader = get_dataloader(\n",
    "        piano_dir=config[\"piano_dir\"],\n",
    "        violin_dir=config[\"violin_dir\"],\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        stats_path=config[\"stats_path\"]\n",
    "    )\n",
    "    print(f\"✅ DataLoader created successfully with batch_size={config['batch_size']}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error creating DataLoader: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a50400",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ================================================================\n",
    "# FUNZIONI DI TRAINING\n",
    "# ================================================================\n",
    "def discriminator_training_step(x, labels, epoch):\n",
    "    \"\"\"\n",
    "    Step di training per il discriminatore\n",
    "    \n",
    "    Args:\n",
    "        x: Input batch\n",
    "        labels: Labels del batch\n",
    "        epoch: Epoca corrente\n",
    "    \n",
    "    Returns:\n",
    "        float: Loss del discriminatore\n",
    "    \"\"\"\n",
    "    # Abilita gradienti solo per discriminatore\n",
    "    set_requires_grad(discriminator, True)\n",
    "    set_requires_grad([style_encoder, content_encoder, decoder], False)\n",
    "    \n",
    "    try:\n",
    "        # Forward pass senza gradienti per i generatori\n",
    "        with torch.no_grad():\n",
    "            style_emb, class_emb = style_encoder(x, labels)\n",
    "            content_emb = content_encoder(x)\n",
    "        \n",
    "        # Controllo NaN negli embeddings\n",
    "        if check_for_nan(style_emb, class_emb, content_emb, \n",
    "                         names=[\"style_emb\", \"class_emb\", \"content_emb\"]):\n",
    "            return float('nan')\n",
    "        \n",
    "        # Calcola loss del discriminatore\n",
    "        disc_loss, _ = adversarial_loss(\n",
    "            style_emb.detach(),\n",
    "            class_emb.detach(),\n",
    "            content_emb.detach(),\n",
    "            discriminator,\n",
    "            labels,\n",
    "            compute_for_discriminator=True,\n",
    "            lambda_content=config[\"lambda_adv_disc\"]\n",
    "        )\n",
    "        \n",
    "        # Controllo NaN nella loss\n",
    "        if check_for_nan(disc_loss, names=[\"disc_loss\"]):\n",
    "            return float('nan')\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer_D.zero_grad()\n",
    "        disc_loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(discriminator.parameters(), config[\"grad_clip_value\"])\n",
    "        \n",
    "        # Controllo gradienti per NaN\n",
    "        for name, param in discriminator.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                if check_for_nan(param.grad, names=[f\"discriminator.{name}.grad\"]):\n",
    "                    return float('nan')\n",
    "        \n",
    "        optimizer_D.step()\n",
    "        return disc_loss.item()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in discriminator training step: {e}\")\n",
    "        return float('nan')\n",
    "\n",
    "def generator_training_step(x, labels, epoch):\n",
    "    \"\"\"\n",
    "    Step di training per i generatori\n",
    "    \n",
    "    Args:\n",
    "        x: Input batch\n",
    "        labels: Labels del batch\n",
    "        epoch: Epoca corrente\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dizionario con tutte le loss\n",
    "    \"\"\"\n",
    "    # Abilita gradienti per i generatori\n",
    "    set_requires_grad([style_encoder, content_encoder, decoder], True)\n",
    "    set_requires_grad(discriminator, False)\n",
    "    \n",
    "    try:\n",
    "        # Forward pass\n",
    "        style_emb, class_emb = style_encoder(x, labels)\n",
    "        content_emb = content_encoder(x)\n",
    "        \n",
    "        # Controllo NaN negli embeddings\n",
    "        if check_for_nan(style_emb, class_emb, content_emb,\n",
    "                         names=[\"style_emb\", \"class_emb\", \"content_emb\"]):\n",
    "            return {key: float('nan') for key in ['total_G', 'adv_gen', 'disent', 'cont', 'margin', 'recon']}\n",
    "        \n",
    "        # Calcola loss avversariale per il generatore\n",
    "        _, adv_gen_loss = adversarial_loss(\n",
    "            style_emb,\n",
    "            class_emb,\n",
    "            content_emb,\n",
    "            discriminator,\n",
    "            labels,\n",
    "            compute_for_discriminator=False,\n",
    "            lambda_content=config[\"lambda_adv_gen\"]\n",
    "        )\n",
    "        \n",
    "        # Calcola loss di disentanglement\n",
    "        disent_loss = disentanglement_loss(\n",
    "            style_emb,\n",
    "            content_emb.mean(dim=1),\n",
    "            use_hsic=True\n",
    "        )\n",
    "        \n",
    "        # Calcola loss contrastiva\n",
    "        cont_loss = infoNCE_loss(style_emb, labels)\n",
    "        \n",
    "        # Calcola margin loss\n",
    "        margin_loss_val = margin_loss(class_emb)\n",
    "        \n",
    "        # Calcola loss di ricostruzione\n",
    "        stft_part = x[:, :, :, :, :513]\n",
    "        recon_x = decoder(content_emb, style_emb, y=stft_part)\n",
    "        recon_losses = compute_comprehensive_loss(recon_x, stft_part)\n",
    "        recon_loss = recon_losses['total_loss']\n",
    "        \n",
    "        # Controllo NaN in tutte le loss\n",
    "        losses = [adv_gen_loss, disent_loss, cont_loss, margin_loss_val, recon_loss]\n",
    "        loss_names = ['adv_gen_loss', 'disent_loss', 'cont_loss', 'margin_loss', 'recon_loss']\n",
    "        \n",
    "        if check_for_nan(*losses, names=loss_names):\n",
    "            return {key: float('nan') for key in ['total_G', 'adv_gen', 'disent', 'cont', 'margin', 'recon']}\n",
    "        \n",
    "        # Calcola loss totale con warmup\n",
    "        lr_multiplier = get_learning_rate_multiplier(epoch, config[\"warmup_epochs\"])\n",
    "        warmup_factor = lr_multiplier if epoch < config[\"warmup_epochs\"] else 1.0\n",
    "        \n",
    "        total_G_loss = (\n",
    "            config[\"lambda_adv_gen\"] * adv_gen_loss * warmup_factor +\n",
    "            config[\"lambda_disent\"] * disent_loss * warmup_factor +\n",
    "            config[\"lambda_cont\"] * cont_loss * warmup_factor +\n",
    "            config[\"lambda_margin\"] * margin_loss_val * warmup_factor +\n",
    "            config[\"lambda_recon\"] * recon_loss\n",
    "        )\n",
    "        \n",
    "        # Controllo NaN nella loss totale\n",
    "        if check_for_nan(total_G_loss, names=[\"total_G_loss\"]):\n",
    "            return {key: float('nan') for key in ['total_G', 'adv_gen', 'disent', 'cont', 'margin', 'recon']}\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer_G.zero_grad()\n",
    "        total_G_loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        generator_params = (\n",
    "            list(style_encoder.parameters()) + \n",
    "            list(content_encoder.parameters()) +\n",
    "            list(decoder.parameters())\n",
    "        )\n",
    "        torch.nn.utils.clip_grad_norm_(generator_params, config[\"grad_clip_value\"])\n",
    "        \n",
    "        # Controllo gradienti per NaN\n",
    "        for model, model_name in [(style_encoder, \"style_encoder\"), \n",
    "                                 (content_encoder, \"content_encoder\"), \n",
    "                                 (decoder, \"decoder\")]:\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    if check_for_nan(param.grad, names=[f\"{model_name}.{name}.grad\"]):\n",
    "                        return {key: float('nan') for key in ['total_G', 'adv_gen', 'disent', 'cont', 'margin', 'recon']}\n",
    "        \n",
    "        optimizer_G.step()\n",
    "        \n",
    "        return {\n",
    "            'total_G': total_G_loss.item(),\n",
    "            'adv_gen': adv_gen_loss.item(),\n",
    "            'disent': disent_loss.item(),\n",
    "            'cont': cont_loss.item(),\n",
    "            'margin': margin_loss_val.item(),\n",
    "            'recon': recon_loss.item()\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in generator training step: {e}\")\n",
    "        return {key: float('nan') for key in ['total_G', 'adv_gen', 'disent', 'cont', 'margin', 'recon']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cef9ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting training with enhanced NaN protection...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-dabe2d01139b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# ================================================================\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"🚀 Starting training with enhanced NaN protection...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"📊 Configuration: {config}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Inizializza strutture per il tracking delle loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'config' is not defined"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# CICLO DI TRAINING PRINCIPALE\n",
    "# ================================================================\n",
    "print(\"🚀 Starting training with enhanced NaN protection...\")\n",
    "print(f\"📊 Configuration: {config}\")\n",
    "\n",
    "# Inizializza strutture per il tracking delle loss\n",
    "loss_history = {\n",
    "    'total_G': [],\n",
    "    'disc': [],\n",
    "    'disent': [],\n",
    "    'cont': [],\n",
    "    'margin': [],\n",
    "    'recon': [],\n",
    "    'adv_gen': []\n",
    "}\n",
    "\n",
    "# Contatore per NaN consecutivi\n",
    "consecutive_nan_count = 0\n",
    "max_consecutive_nans = config[\"nan_threshold\"]\n",
    "\n",
    "# Dizionari per salvataggio\n",
    "models_dict = {\n",
    "    'style_encoder': style_encoder,\n",
    "    'content_encoder': content_encoder,\n",
    "    'discriminator': discriminator,\n",
    "    'decoder': decoder\n",
    "}\n",
    "\n",
    "optimizers_dict = {\n",
    "    'optimizer_G': optimizer_G,\n",
    "    'optimizer_D': optimizer_D\n",
    "}\n",
    "\n",
    "schedulers_dict = {\n",
    "    'scheduler_G': scheduler_G,\n",
    "    'scheduler_D': scheduler_D\n",
    "}\n",
    "\n",
    "try:\n",
    "    for epoch in tqdm(range(config[\"epochs\"]), desc=\"Training Progress\"):\n",
    "        # Controllo early stopping per NaN\n",
    "        if consecutive_nan_count >= max_consecutive_nans:\n",
    "            print(f\"🛑 Early stopping: {consecutive_nan_count} consecutive NaN occurrences\")\n",
    "            break\n",
    "        \n",
    "        # Imposta tutti i modelli in modalità training\n",
    "        for model in models:\n",
    "            model.train()\n",
    "        \n",
    "        # Tracking delle loss per l'epoca corrente\n",
    "        epoch_losses = {key: [] for key in loss_history.keys()}\n",
    "        \n",
    "        print(f\"\\n🔄 Epoch {epoch+1}/{config['epochs']}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for batch_idx, (x, labels) in enumerate(train_loader):\n",
    "            # Trasferisce dati su GPU\n",
    "            x = x.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Controllo NaN nei dati di input\n",
    "            if check_for_nan(x, labels, names=[\"input_x\", \"input_labels\"]):\n",
    "                print(f\"⚠️  Skipping batch {batch_idx} due to NaN in input data\")\n",
    "                continue\n",
    "            \n",
    "            # ============================================\n",
    "            # TRAINING DISCRIMINATORE\n",
    "            # ============================================\n",
    "            for _ in range(config[\"discriminator_steps\"]):\n",
    "                disc_loss = discriminator_training_step(x, labels, epoch)\n",
    "                \n",
    "                if not np.isnan(disc_loss):\n",
    "                    epoch_losses['disc'].append(disc_loss)\n",
    "                    consecutive_nan_count = 0  # Reset counter su successo\n",
    "                else:\n",
    "                    consecutive_nan_count += 1\n",
    "                    print(f\"⚠️  NaN in discriminator loss (consecutive: {consecutive_nan_count})\")\n",
    "            \n",
    "            # ============================================\n",
    "            # TRAINING GENERATORE\n",
    "            # ============================================\n",
    "            for _ in range(config[\"generator_steps\"]):\n",
    "                gen_losses = generator_training_step(x, labels, epoch)\n",
    "                \n",
    "                # Controlla se ci sono NaN nelle loss del generatore\n",
    "                nan_in_gen = any(np.isnan(v) for v in gen_losses.values())\n",
    "                \n",
    "                if not nan_in_gen:\n",
    "                    # Aggiungi loss valide alla storia\n",
    "                    for key, value in gen_losses.items():\n",
    "                        if key in epoch_losses:\n",
    "                            epoch_losses[key].append(value)\n",
    "                    consecutive_nan_count = 0  # Reset counter su successo\n",
    "                else:\n",
    "                    consecutive_nan_count += 1\n",
    "                    print(f\"⚠️  NaN in generator losses (consecutive: {consecutive_nan_count})\")\n",
    "            \n",
    "            # ============================================\n",
    "            # LOGGING PER BATCH\n",
    "            # ============================================\n",
    "            # Stampa loss dettagliate per ogni batch\n",
    "            if batch_idx % 25 == 0:\n",
    "                print(f\"\\n📊 Batch {batch_idx + 1}:\")\n",
    "                \n",
    "                # Loss del discriminatore\n",
    "                if epoch_losses['disc']:\n",
    "                    recent_disc = epoch_losses['disc'][-config[\"discriminator_steps\"]:]\n",
    "                    avg_disc = np.mean(recent_disc)\n",
    "                    print(f\"   Discriminator Loss: {avg_disc:.6f}\")\n",
    "                \n",
    "                # Loss del generatore\n",
    "                if epoch_losses['total_G']:\n",
    "                    recent_gen = epoch_losses['total_G'][-config[\"generator_steps\"]:]\n",
    "                    avg_total_G = np.mean(recent_gen)\n",
    "                    print(f\"   Generator Total Loss: {avg_total_G:.6f}\")\n",
    "                \n",
    "                # Loss individuali del generatore\n",
    "                gen_loss_names = ['adv_gen', 'disent', 'cont', 'margin', 'recon']\n",
    "                for loss_name in gen_loss_names:\n",
    "                    if epoch_losses[loss_name]:\n",
    "                        recent_loss = epoch_losses[loss_name][-config[\"generator_steps\"]:]\n",
    "                        avg_loss = np.mean(recent_loss)\n",
    "                        print(f\"     {loss_name.replace('_', ' ').title()}: {avg_loss:.6f}\")\n",
    "                \n",
    "                # Informazioni aggiuntive\n",
    "                print(f\"   Consecutive NaN count: {consecutive_nan_count}\")\n",
    "                if torch.cuda.is_available():\n",
    "                    print(f\"   GPU Memory: {torch.cuda.memory_allocated()/1024**2:.1f} MB\")\n",
    "            \n",
    "            # Controllo early stopping durante il batch\n",
    "            if consecutive_nan_count >= max_consecutive_nans:\n",
    "                print(f\"🛑 Stopping epoch {epoch+1} due to consecutive NaN issues\")\n",
    "                break\n",
    "        \n",
    "        # ============================================\n",
    "        # FINE EPOCA: AGGIORNAMENTI E SALVATAGGIO\n",
    "        # ============================================\n",
    "        if consecutive_nan_count < max_consecutive_nans:\n",
    "            # Aggiorna schedulers\n",
    "            if epoch_losses['disc']:\n",
    "                scheduler_D.step(np.mean(epoch_losses['disc']))\n",
    "            if epoch_losses['total_G']:\n",
    "                scheduler_G.step(np.mean(epoch_losses['total_G']))\n",
    "            \n",
    "            # Aggiungi loss dell'epoca alla storia globale\n",
    "            for key in loss_history.keys():\n",
    "                if epoch_losses[key]:\n",
    "                    loss_history[key].extend(epoch_losses[key])\n",
    "            \n",
    "            # Salva checkpoint periodicamente\n",
    "            if (epoch + 1) % config[\"save_interval\"] == 0:\n",
    "                save_checkpoint(epoch + 1, models_dict, optimizers_dict, schedulers_dict, config)\n",
    "            \n",
    "            # Riepilogo dell'epoca\n",
    "            print(f\"\\n✅ Epoch {epoch+1} Summary:\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            for key in ['disc', 'total_G', 'adv_gen', 'disent', 'cont', 'margin', 'recon']:\n",
    "                if epoch_losses[key]:\n",
    "                    avg_loss = np.mean(epoch_losses[key])\n",
    "                    print(f\"   {key.replace('_', ' ').title()}: {avg_loss:.6f}\")\n",
    "            \n",
    "            # Informazioni sui learning rates\n",
    "            current_lr_G = optimizer_G.param_groups[0]['lr']\n",
    "            current_lr_D = optimizer_D.param_groups[0]['lr']\n",
    "            print(f\"   LR Generator: {current_lr_G:.2e}\")\n",
    "            print(f\"   LR Discriminator: {current_lr_D:.2e}\")\n",
    "            \n",
    "            print(\"-\" * 60)\n",
    "        \n",
    "        # Pulizia memoria periodica\n",
    "        if torch.cuda.is_available() and epoch % 10 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n⏹️  Training interrupted by user\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Training error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "finally:\n",
    "    # Salvataggio finale\n",
    "    print(\"\\n💾 Saving final checkpoint...\")\n",
    "    save_checkpoint(epoch + 1, models_dict, optimizers_dict, schedulers_dict, config)\n",
    "    \n",
    "    # Visualizzazione delle loss\n",
    "    if any(loss_history[key] for key in loss_history.keys()):\n",
    "        print(\"📊 Generating loss plots...\")\n",
    "        \n",
    "        # Configura plot\n",
    "        plt.style.use('default')\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        # Definisci le loss da plottare\n",
    "        loss_configs = [\n",
    "            ('disc', 'Discriminator Loss', 'red'),\n",
    "            ('total_G', 'Generator Total Loss', 'blue'),\n",
    "            ('recon', 'Reconstruction Loss', 'green'),\n",
    "            ('adv_gen', 'Adversarial Generator Loss', 'orange'),\n",
    "            ('disent', 'Disentanglement Loss', 'purple'),\n",
    "            ('cont', 'Contrastive Loss', 'brown')\n",
    "        ]\n",
    "        \n",
    "        for i, (loss_name, title, color) in enumerate(loss_configs):\n",
    "            if loss_history[loss_name]:\n",
    "                values = loss_history[loss_name]\n",
    "                \n",
    "                # Plot raw values\n",
    "                axes[i].plot(values, alpha=0.4, color=color, linewidth=0.5, label='Raw')\n",
    "                \n",
    "                # Plot smoothed values se ci sono abbastanza punti\n",
    "                if len(values) > 50:\n",
    "                    window = max(1, len(values) // 50)\n",
    "                    smoothed = np.convolve(values, np.ones(window)/window, mode='valid')\n",
    "                    axes[i].plot(smoothed, color=color, linewidth=2, label='Smoothed')\n",
    "                \n",
    "                axes[i].set_title(title, fontsize=12, fontweight='bold')\n",
    "                axes[i].set_xlabel('Iteration')\n",
    "                axes[i].set_ylabel('Loss')\n",
    "                axes[i].legend()\n",
    "                axes[i].grid(True, alpha=0.3)\n",
    "                axes[i].set_xlim(0, len(values))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plot_path = os.path.join(config[\"save_dir\"], \"loss_curves.png\")\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(f\"📈 Loss curves saved to: {plot_path}\")\n",
    "    \n",
    "    # Riepilogo finale\n",
    "    print(f\"\\n🎯 Training completed!\")\n",
    "    print(f\"   Total epochs processed: {epoch + 1}\")\n",
    "    print(f\"   Final consecutive NaN count: {consecutive_nan_count}\")\n",
    "    print(f\"   Checkpoints saved in: {config['save_dir']}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"   Final GPU memory: {torch.cuda.memory_allocated()/1024**2:.1f} MB\")\n",
    "    \n",
    "    print(\"🎉 Training session finished successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80a7039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO\n",
    "# CURRICULUM LEARNING\n",
    "def get_curriculum_weights(epoch, total_epochs):\n",
    "    \"\"\"\n",
    "    Curriculum learning progressivo - migliore del pre-training separato\n",
    "    \"\"\"\n",
    "    progress = epoch / total_epochs\n",
    "    \n",
    "    # Fase 1: Solo ricostruzione (0-20%)\n",
    "    if progress < 0.2:\n",
    "        return {\n",
    "            \"lambda_recon\": 10.0,\n",
    "            \"lambda_adv_gen\": 0.0,\n",
    "            \"lambda_adv_disc\": 0.0,\n",
    "            \"lambda_disent\": 0.0,\n",
    "            \"lambda_cont\": 0.0,\n",
    "            \"lambda_margin\": 0.0\n",
    "        }\n",
    "    \n",
    "    # Fase 2: Aggiungi disentanglement (20-40%)\n",
    "    elif progress < 0.4:\n",
    "        return {\n",
    "            \"lambda_recon\": 8.0,\n",
    "            \"lambda_adv_gen\": 0.0,\n",
    "            \"lambda_adv_disc\": 0.0,\n",
    "            \"lambda_disent\": 0.5,\n",
    "            \"lambda_cont\": 0.0,\n",
    "            \"lambda_margin\": 0.0\n",
    "        }\n",
    "    \n",
    "    # Fase 3: Aggiungi contrastive learning (40-60%)\n",
    "    elif progress < 0.6:\n",
    "        return {\n",
    "            \"lambda_recon\": 5.0,\n",
    "            \"lambda_adv_gen\": 0.0,\n",
    "            \"lambda_adv_disc\": 0.0,\n",
    "            \"lambda_disent\": 0.8,\n",
    "            \"lambda_cont\": 0.2,\n",
    "            \"lambda_margin\": 0.1\n",
    "        }\n",
    "    \n",
    "    # Fase 4: Introduci adversarial training gradualmente (60-100%)\n",
    "    else:\n",
    "        # Crescita graduale dell'adversarial loss\n",
    "        adv_strength = min(1.0, (progress - 0.6) / 0.4)\n",
    "        return {\n",
    "            \"lambda_recon\": 3.0,\n",
    "            \"lambda_adv_gen\": 0.3 * adv_strength,\n",
    "            \"lambda_adv_disc\": 0.5 * adv_strength,\n",
    "            \"lambda_disent\": 0.8,\n",
    "            \"lambda_cont\": 0.1,\n",
    "            \"lambda_margin\": 0.05\n",
    "        }\n",
    "    \n",
    "\n",
    "\n",
    "import torch.nn.utils.spectral_norm as spectral_norm\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            spectral_norm(nn.Linear(input_dim, hidden_dim)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            spectral_norm(nn.Linear(hidden_dim, hidden_dim//2)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            spectral_norm(nn.Linear(hidden_dim//2, 1))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c59454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_training_step_curriculum(x, labels, epoch, total_epochs):\n",
    "    \"\"\"\n",
    "    Step di training con curriculum learning integrato\n",
    "    \"\"\"\n",
    "    # Ottieni pesi curriculum per l'epoca corrente\n",
    "    curriculum_weights = get_curriculum_weights(epoch, total_epochs)\n",
    "    \n",
    "    # Abilita gradienti per i generatori\n",
    "    set_requires_grad([style_encoder, content_encoder, decoder], True)\n",
    "    set_requires_grad(discriminator, False)\n",
    "    \n",
    "    try:\n",
    "        # Forward pass\n",
    "        style_emb, class_emb = style_encoder(x, labels)\n",
    "        content_emb = content_encoder(x)\n",
    "        \n",
    "        # Controllo NaN\n",
    "        if check_for_nan(style_emb, class_emb, content_emb,\n",
    "                         names=[\"style_emb\", \"class_emb\", \"content_emb\"]):\n",
    "            return {key: float('nan') for key in curriculum_weights.keys()}\n",
    "        \n",
    "        # Calcola solo le loss attive nel curriculum\n",
    "        losses = {}\n",
    "        \n",
    "        # Loss di ricostruzione (sempre attiva)\n",
    "        if curriculum_weights[\"lambda_recon\"] > 0:\n",
    "            stft_part = x[:, :, :, :, :513]\n",
    "            recon_x = decoder(content_emb, style_emb, y=stft_part)\n",
    "            recon_losses = compute_comprehensive_loss(recon_x, stft_part)\n",
    "            losses[\"recon\"] = recon_losses['total_loss']\n",
    "        \n",
    "        # Loss di disentanglement\n",
    "        if curriculum_weights[\"lambda_disent\"] > 0:\n",
    "            losses[\"disent\"] = disentanglement_loss(\n",
    "                style_emb, content_emb.mean(dim=1), use_hsic=True\n",
    "            )\n",
    "        \n",
    "        # Loss contrastiva\n",
    "        if curriculum_weights[\"lambda_cont\"] > 0:\n",
    "            losses[\"cont\"] = infoNCE_loss(style_emb, labels)\n",
    "        \n",
    "        # Margin loss\n",
    "        if curriculum_weights[\"lambda_margin\"] > 0:\n",
    "            losses[\"margin\"] = margin_loss(class_emb)\n",
    "        \n",
    "        # Loss avversariale (solo quando attivata)\n",
    "        if curriculum_weights[\"lambda_adv_gen\"] > 0:\n",
    "            _, losses[\"adv_gen\"] = adversarial_loss(\n",
    "                style_emb, class_emb, content_emb, discriminator, labels,\n",
    "                compute_for_discriminator=False,\n",
    "                lambda_content=curriculum_weights[\"lambda_adv_gen\"]\n",
    "            )\n",
    "        \n",
    "        # Controlla NaN in tutte le loss calcolate\n",
    "        if check_for_nan(*losses.values(), names=list(losses.keys())):\n",
    "            return {key: float('nan') for key in curriculum_weights.keys()}\n",
    "        \n",
    "        # Calcola loss totale usando i pesi del curriculum\n",
    "        total_G_loss = sum(\n",
    "            curriculum_weights[f\"lambda_{key}\"] * loss_val\n",
    "            for key, loss_val in losses.items()\n",
    "            if f\"lambda_{key}\" in curriculum_weights\n",
    "        )\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer_G.zero_grad()\n",
    "        total_G_loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        generator_params = (\n",
    "            list(style_encoder.parameters()) + \n",
    "            list(content_encoder.parameters()) +\n",
    "            list(decoder.parameters())\n",
    "        )\n",
    "        torch.nn.utils.clip_grad_norm_(generator_params, config[\"grad_clip_value\"])\n",
    "        \n",
    "        optimizer_G.step()\n",
    "        \n",
    "        # Prepara output con tutte le loss (0 se non attive)\n",
    "        output_losses = {\n",
    "            'total_G': total_G_loss.item(),\n",
    "            'recon': losses.get(\"recon\", torch.tensor(0.0)).item(),\n",
    "            'disent': losses.get(\"disent\", torch.tensor(0.0)).item(),\n",
    "            'cont': losses.get(\"cont\", torch.tensor(0.0)).item(),\n",
    "            'margin': losses.get(\"margin\", torch.tensor(0.0)).item(),\n",
    "            'adv_gen': losses.get(\"adv_gen\", torch.tensor(0.0)).item()\n",
    "        }\n",
    "        \n",
    "        return output_losses\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in generator training step: {e}\")\n",
    "        return {key: float('nan') for key in ['total_G', 'adv_gen', 'disent', 'cont', 'margin', 'recon']}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
